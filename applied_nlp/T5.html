<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml">

<head>

<meta charset="utf-8" />
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="generator" content="pandoc" />

<meta name="viewport" content="width=device-width, initial-scale=1">

<meta name="author" content="Andrew Fogarty" />


<title>Summarization: T5</title>



<style type="text/css">code{white-space: pre;}</style>
<style type="text/css" data-origin="pandoc">
a.sourceLine { display: inline-block; line-height: 1.25; }
a.sourceLine { pointer-events: none; color: inherit; text-decoration: inherit; }
a.sourceLine:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
a.sourceLine { text-indent: -1em; padding-left: 1em; }
}
pre.numberSource a.sourceLine
  { position: relative; left: -4em; }
pre.numberSource a.sourceLine::before
  { content: attr(title);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; pointer-events: all; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {  }
@media screen {
a.sourceLine::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */

</style>
<script>
// apply pandoc div.sourceCode style to pre.sourceCode instead
(function() {
  var sheets = document.styleSheets;
  for (var i = 0; i < sheets.length; i++) {
    if (sheets[i].ownerNode.dataset["origin"] !== "pandoc") continue;
    try { var rules = sheets[i].cssRules; } catch (e) { continue; }
    for (var j = 0; j < rules.length; j++) {
      var rule = rules[j];
      // check if there is a div.sourceCode rule
      if (rule.type !== rule.STYLE_RULE || rule.selectorText !== "div.sourceCode") continue;
      var style = rule.style.cssText;
      // check if color or background-color is set
      if (rule.style.color === '' || rule.style.backgroundColor === '') continue;
      // replace div.sourceCode by a pre.sourceCode rule
      sheets[i].deleteRule(j);
      sheets[i].insertRule('pre.sourceCode{' + style + '}', j);
    }
  }
})();
</script>



<style type="text/css">body {
background-color: #fff;
margin: 1em auto;
max-width: 700px;
overflow: visible;
padding-left: 2em;
padding-right: 2em;
font-family: "Open Sans", "Helvetica Neue", Helvetica, Arial, sans-serif;
font-size: 14px;
line-height: 1.35;
}
#header {
text-align: center;
}
#TOC {
clear: both;
margin: 0 0 10px 10px;
padding: 4px;
width: 400px;
border: 1px solid #CCCCCC;
border-radius: 5px;
background-color: #f6f6f6;
font-size: 13px;
line-height: 1.3;
}
#TOC .toctitle {
font-weight: bold;
font-size: 15px;
margin-left: 5px;
}
#TOC ul {
padding-left: 40px;
margin-left: -1.5em;
margin-top: 5px;
margin-bottom: 5px;
}
#TOC ul ul {
margin-left: -2em;
}
#TOC li {
line-height: 16px;
}
table {
margin: 1em auto;
border-width: 1px;
border-color: #DDDDDD;
border-style: outset;
border-collapse: collapse;
}
table th {
border-width: 2px;
padding: 5px;
border-style: inset;
}
table td {
border-width: 1px;
border-style: inset;
line-height: 18px;
padding: 5px 5px;
}
table, table th, table td {
border-left-style: none;
border-right-style: none;
}
table thead, table tr.even {
background-color: #f7f7f7;
}
p {
margin: 0.5em 0;
}
blockquote {
background-color: #f6f6f6;
padding: 0.25em 0.75em;
}
hr {
border-style: solid;
border: none;
border-top: 1px solid #777;
margin: 28px 0;
}
dl {
margin-left: 0;
}
dl dd {
margin-bottom: 13px;
margin-left: 13px;
}
dl dt {
font-weight: bold;
}
ul {
margin-top: 0;
}
ul li {
list-style: circle outside;
}
ul ul {
margin-bottom: 0;
}
pre, code {
background-color: #f7f7f7;
border-radius: 3px;
color: #333;
white-space: pre-wrap; 
}
pre {
border-radius: 3px;
margin: 5px 0px 10px 0px;
padding: 10px;
}
pre:not([class]) {
background-color: #f7f7f7;
}
code {
font-family: Consolas, Monaco, 'Courier New', monospace;
font-size: 85%;
}
p > code, li > code {
padding: 2px 0px;
}
div.figure {
text-align: center;
}
img {
background-color: #FFFFFF;
padding: 2px;
border: 1px solid #DDDDDD;
border-radius: 3px;
border: 1px solid #CCCCCC;
margin: 0 5px;
}
h1 {
margin-top: 0;
font-size: 35px;
line-height: 40px;
}
h2 {
border-bottom: 4px solid #f7f7f7;
padding-top: 10px;
padding-bottom: 2px;
font-size: 145%;
}
h3 {
border-bottom: 2px solid #f7f7f7;
padding-top: 10px;
font-size: 120%;
}
h4 {
border-bottom: 1px solid #f7f7f7;
margin-left: 8px;
font-size: 105%;
}
h5, h6 {
border-bottom: 1px solid #ccc;
font-size: 105%;
}
a {
color: #0033dd;
text-decoration: none;
}
a:hover {
color: #6666ff; }
a:visited {
color: #800080; }
a:visited:hover {
color: #BB00BB; }
a[href^="http:"] {
text-decoration: underline; }
a[href^="https:"] {
text-decoration: underline; }

code > span.kw { color: #555; font-weight: bold; } 
code > span.dt { color: #902000; } 
code > span.dv { color: #40a070; } 
code > span.bn { color: #d14; } 
code > span.fl { color: #d14; } 
code > span.ch { color: #d14; } 
code > span.st { color: #d14; } 
code > span.co { color: #888888; font-style: italic; } 
code > span.ot { color: #007020; } 
code > span.al { color: #ff0000; font-weight: bold; } 
code > span.fu { color: #900; font-weight: bold; }  code > span.er { color: #a61717; background-color: #e3d2d2; } 
</style>




</head>

<body>




<h1 class="title toc-ignore">Summarization: T5</h1>
<h4 class="author">Andrew Fogarty</h4>
<h4 class="date">7/18/2020</h4>


<div id="TOC">
<ul>
<li><a href="#introduction"><span class="toc-section-number">1</span> Introduction</a><ul>
<li><a href="#data-preparation"><span class="toc-section-number">1.1</span> Data Preparation</a></li>
<li><a href="#instantiate-tokenizer"><span class="toc-section-number">1.2</span> Instantiate Tokenizer</a></li>
<li><a href="#tokenize-the-corpus"><span class="toc-section-number">1.3</span> Tokenize the Corpus</a></li>
<li><a href="#prepare-and-split-data"><span class="toc-section-number">1.4</span> Prepare and Split Data</a></li>
<li><a href="#instantiate-training-models"><span class="toc-section-number">1.5</span> Instantiate Training Models</a></li>
<li><a href="#prepare-for-training"><span class="toc-section-number">1.6</span> Prepare for Training</a></li>
<li><a href="#train-and-validate"><span class="toc-section-number">1.7</span> Train and Validate</a></li>
<li><a href="#test-and-generate-summaries"><span class="toc-section-number">1.8</span> Test and Generate Summaries</a></li>
<li><a href="#rouge-metrics"><span class="toc-section-number">1.9</span> ROUGE Metrics</a></li>
</ul></li>
<li><a href="#sources"><span class="toc-section-number">2</span> Sources</a></li>
</ul>
</div>

<div class="sourceCode" id="cb1"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1-1" title="1"><span class="co"># load python</span></a>
<a class="sourceLine" id="cb1-2" title="2"><span class="kw">library</span>(reticulate)</a>
<a class="sourceLine" id="cb1-3" title="3"><span class="kw">use_condaenv</span>(<span class="st">&quot;my_ml&quot;</span>)</a></code></pre></div>
<div class="sourceCode" id="cb2"><pre class="sourceCode python"><code class="sourceCode python"><a class="sourceLine" id="cb2-1" title="1"><span class="co"># load packages</span></a>
<a class="sourceLine" id="cb2-2" title="2"><span class="im">import</span> numpy <span class="im">as</span> np</a>
<a class="sourceLine" id="cb2-3" title="3"><span class="im">import</span> pandas <span class="im">as</span> pd</a>
<a class="sourceLine" id="cb2-4" title="4"><span class="im">import</span> torch</a>
<a class="sourceLine" id="cb2-5" title="5"><span class="im">import</span> torch.nn.functional <span class="im">as</span> F</a>
<a class="sourceLine" id="cb2-6" title="6"><span class="im">from</span> torch.utils.data <span class="im">import</span> Dataset, DataLoader, RandomSampler, SequentialSampler</a>
<a class="sourceLine" id="cb2-7" title="7"><span class="im">from</span> collections <span class="im">import</span> Counter</a>
<a class="sourceLine" id="cb2-8" title="8"><span class="im">from</span> transformers <span class="im">import</span> T5Tokenizer, T5ForConditionalGeneration, AdamW</a>
<a class="sourceLine" id="cb2-9" title="9"><span class="im">from</span> torch.utils.data <span class="im">import</span> TensorDataset, random_split, DataLoader, RandomSampler, SequentialSampler</a>
<a class="sourceLine" id="cb2-10" title="10"><span class="im">import</span> time, os, datetime, random, re</a>
<a class="sourceLine" id="cb2-11" title="11"><span class="im">from</span> transformers <span class="im">import</span> get_linear_schedule_with_warmup</a>
<a class="sourceLine" id="cb2-12" title="12"><span class="im">from</span> torch.cuda.amp <span class="im">import</span> autocast, GradScaler</a>
<a class="sourceLine" id="cb2-13" title="13"><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</a>
<a class="sourceLine" id="cb2-14" title="14"><span class="im">import</span> seaborn <span class="im">as</span> sns</a>
<a class="sourceLine" id="cb2-15" title="15"><span class="im">import</span> torch.nn <span class="im">as</span> nn</a>
<a class="sourceLine" id="cb2-16" title="16"><span class="im">import</span> nlp</a>
<a class="sourceLine" id="cb2-17" title="17"></a>
<a class="sourceLine" id="cb2-18" title="18">torch.cuda.amp.autocast(enabled<span class="op">=</span><span class="va">True</span>)</a></code></pre></div>
<pre><code>## &lt;torch.cuda.amp.autocast_mode.autocast object at 0x0000000032834FC8&gt;</code></pre>
<div class="sourceCode" id="cb4"><pre class="sourceCode python"><code class="sourceCode python"><a class="sourceLine" id="cb4-1" title="1">SEED <span class="op">=</span> <span class="dv">15</span></a>
<a class="sourceLine" id="cb4-2" title="2">random.seed(SEED)</a>
<a class="sourceLine" id="cb4-3" title="3">np.random.seed(SEED)</a>
<a class="sourceLine" id="cb4-4" title="4">torch.manual_seed(SEED)</a></code></pre></div>
<pre><code>## &lt;torch._C.Generator object at 0x000000002050A850&gt;</code></pre>
<div class="sourceCode" id="cb6"><pre class="sourceCode python"><code class="sourceCode python"><a class="sourceLine" id="cb6-1" title="1">torch.backends.cudnn.deterministic <span class="op">=</span> <span class="va">True</span></a>
<a class="sourceLine" id="cb6-2" title="2"></a>
<a class="sourceLine" id="cb6-3" title="3"><span class="co"># tell pytorch to use cuda</span></a>
<a class="sourceLine" id="cb6-4" title="4">device <span class="op">=</span> torch.device(<span class="st">&quot;cuda&quot;</span>)</a></code></pre></div>
<div id="introduction" class="section level1">
<h1><span class="header-section-number">1</span> Introduction</h1>
<p>There are two types of summaries: (1) abstractive, or explaining in your own words, and (2) extractive, or building a summary from existing text. Humans are mostly abstractive while NLP systems are mostly extractive. In this guide we use T5, a pre-trained and very large (e.g., roughly twice the size of BERT-base) encoder-decoder Transformer model. T5, a model devised by Google, is an important advancement in the field of Transformers because it achieves near human-level performance on a variety of benchmarks like GLUE and SQuAD.</p>
<p>The guide proceeds by (1) preparing the data for text summarization with T5 small – a small version of T5 base, and (2) training the data in PyTorch.</p>
<div id="data-preparation" class="section level2">
<h2><span class="header-section-number">1.1</span> Data Preparation</h2>
<p>Some unique pre-processing is required when using T5 for summarization. Specifically, we need to add “summarize:” to the beginning of all of our text that needs to be summarized and we need shift our summaries rightward by one token which we do by adding “&lt;pad” to the beginning of each summary. T5’s tokenizer in the <code>transformers</code> library will handle the details from there.</p>
<div class="sourceCode" id="cb7"><pre class="sourceCode python"><code class="sourceCode python"><a class="sourceLine" id="cb7-1" title="1"><span class="co"># prepare and load data</span></a>
<a class="sourceLine" id="cb7-2" title="2"><span class="kw">def</span> prepare_df(pkl_location):</a>
<a class="sourceLine" id="cb7-3" title="3">    <span class="co"># read pkl as pandas</span></a>
<a class="sourceLine" id="cb7-4" title="4">    df <span class="op">=</span> pd.read_pickle(pkl_location)</a>
<a class="sourceLine" id="cb7-5" title="5">    <span class="co"># just keep us/kabul labels</span></a>
<a class="sourceLine" id="cb7-6" title="6">    df <span class="op">=</span> df.loc[(df[<span class="st">&#39;target&#39;</span>] <span class="op">==</span> <span class="st">&#39;US&#39;</span>) <span class="op">|</span> (df[<span class="st">&#39;target&#39;</span>] <span class="op">==</span> <span class="st">&#39;Kabul&#39;</span>)]</a>
<a class="sourceLine" id="cb7-7" title="7">    <span class="co"># mask DV to recode</span></a>
<a class="sourceLine" id="cb7-8" title="8">    us <span class="op">=</span> df[<span class="st">&#39;target&#39;</span>] <span class="op">==</span> <span class="st">&#39;US&#39;</span></a>
<a class="sourceLine" id="cb7-9" title="9">    kabul <span class="op">=</span> df[<span class="st">&#39;target&#39;</span>] <span class="op">==</span> <span class="st">&#39;Kabul&#39;</span></a>
<a class="sourceLine" id="cb7-10" title="10">    <span class="co"># apply mask</span></a>
<a class="sourceLine" id="cb7-11" title="11">    df.loc[us, <span class="st">&#39;target&#39;</span>] <span class="op">=</span> <span class="dv">1</span></a>
<a class="sourceLine" id="cb7-12" title="12">    df.loc[kabul, <span class="st">&#39;target&#39;</span>] <span class="op">=</span> <span class="dv">0</span></a>
<a class="sourceLine" id="cb7-13" title="13">    <span class="co"># reset index</span></a>
<a class="sourceLine" id="cb7-14" title="14">    df <span class="op">=</span> df.reset_index(drop<span class="op">=</span><span class="va">True</span>)</a>
<a class="sourceLine" id="cb7-15" title="15">    <span class="cf">return</span> df</a>
<a class="sourceLine" id="cb7-16" title="16"></a>
<a class="sourceLine" id="cb7-17" title="17"></a>
<a class="sourceLine" id="cb7-18" title="18"><span class="co"># load df</span></a>
<a class="sourceLine" id="cb7-19" title="19">df <span class="op">=</span> prepare_df(<span class="st">&#39;C:</span><span class="ch">\\</span><span class="st">Users</span><span class="ch">\\</span><span class="st">Andrew</span><span class="ch">\\</span><span class="st">Desktop</span><span class="ch">\\</span><span class="st">df.pkl&#39;</span>)</a>
<a class="sourceLine" id="cb7-20" title="20"></a>
<a class="sourceLine" id="cb7-21" title="21"></a>
<a class="sourceLine" id="cb7-22" title="22"><span class="co"># prepare data</span></a>
<a class="sourceLine" id="cb7-23" title="23"><span class="kw">def</span> clean_df(df):</a>
<a class="sourceLine" id="cb7-24" title="24">    <span class="co"># strip dash but keep a space</span></a>
<a class="sourceLine" id="cb7-25" title="25">    df[<span class="st">&#39;body&#39;</span>] <span class="op">=</span> df[<span class="st">&#39;body&#39;</span>].<span class="bu">str</span>.replace(<span class="st">&#39;-&#39;</span>, <span class="st">&#39; &#39;</span>)</a>
<a class="sourceLine" id="cb7-26" title="26">    <span class="co"># lower case the data</span></a>
<a class="sourceLine" id="cb7-27" title="27">    df[<span class="st">&#39;body&#39;</span>] <span class="op">=</span> df[<span class="st">&#39;body&#39;</span>].<span class="bu">apply</span>(<span class="kw">lambda</span> x: x.lower())</a>
<a class="sourceLine" id="cb7-28" title="28">    <span class="co"># remove excess spaces near punctuation</span></a>
<a class="sourceLine" id="cb7-29" title="29">    df[<span class="st">&#39;body&#39;</span>] <span class="op">=</span> df[<span class="st">&#39;body&#39;</span>].<span class="bu">apply</span>(<span class="kw">lambda</span> x: re.sub(<span class="vs">r&#39;\s([?.!&quot;](?:\s|$))&#39;</span>, <span class="vs">r&#39;\1&#39;</span>, x))</a>
<a class="sourceLine" id="cb7-30" title="30">    <span class="co"># generate a word count for body</span></a>
<a class="sourceLine" id="cb7-31" title="31">    df[<span class="st">&#39;word_count&#39;</span>] <span class="op">=</span> df[<span class="st">&#39;body&#39;</span>].<span class="bu">apply</span>(<span class="kw">lambda</span> x: <span class="bu">len</span>(x.split()))</a>
<a class="sourceLine" id="cb7-32" title="32">    <span class="co"># generate a word count for summary</span></a>
<a class="sourceLine" id="cb7-33" title="33">    df[<span class="st">&#39;word_count_summary&#39;</span>] <span class="op">=</span> df[<span class="st">&#39;title_osc&#39;</span>].<span class="bu">apply</span>(<span class="kw">lambda</span> x: <span class="bu">len</span>(x.split()))</a>
<a class="sourceLine" id="cb7-34" title="34">    <span class="co"># remove excess white spaces</span></a>
<a class="sourceLine" id="cb7-35" title="35">    df[<span class="st">&#39;body&#39;</span>] <span class="op">=</span> df[<span class="st">&#39;body&#39;</span>].<span class="bu">apply</span>(<span class="kw">lambda</span> x: <span class="st">&quot; &quot;</span>.join(x.split()))</a>
<a class="sourceLine" id="cb7-36" title="36">    <span class="co"># lower case to body</span></a>
<a class="sourceLine" id="cb7-37" title="37">    df[<span class="st">&#39;body&#39;</span>] <span class="op">=</span> df[<span class="st">&#39;body&#39;</span>].<span class="bu">apply</span>(<span class="kw">lambda</span> x: x.lower())</a>
<a class="sourceLine" id="cb7-38" title="38">    <span class="co"># lower case to summary</span></a>
<a class="sourceLine" id="cb7-39" title="39">    df[<span class="st">&#39;title_osc&#39;</span>] <span class="op">=</span> df[<span class="st">&#39;title_osc&#39;</span>].<span class="bu">apply</span>(<span class="kw">lambda</span> x: x.lower())</a>
<a class="sourceLine" id="cb7-40" title="40">    <span class="co"># add summarize akin to T5 setup</span></a>
<a class="sourceLine" id="cb7-41" title="41">    df[<span class="st">&#39;body&#39;</span>] <span class="op">=</span> <span class="st">&#39;summarize: &#39;</span> <span class="op">+</span> df[<span class="st">&#39;body&#39;</span>]</a>
<a class="sourceLine" id="cb7-42" title="42">    <span class="co"># add pad token to summaries</span></a>
<a class="sourceLine" id="cb7-43" title="43">    df[<span class="st">&#39;title_osc&#39;</span>] <span class="op">=</span> <span class="st">&#39;&lt;pad&gt;&#39;</span> <span class="op">+</span> df[<span class="st">&#39;title_osc&#39;</span>]</a>
<a class="sourceLine" id="cb7-44" title="44">    <span class="co"># add &quot; &lt;/s&gt;&quot; to end of body</span></a>
<a class="sourceLine" id="cb7-45" title="45">    df[<span class="st">&#39;body&#39;</span>] <span class="op">=</span> df[<span class="st">&#39;body&#39;</span>] <span class="op">+</span> <span class="st">&quot; &lt;/s&gt;&quot;</span></a>
<a class="sourceLine" id="cb7-46" title="46">    <span class="co"># add &quot; &lt;/s&gt;&quot; to end of review</span></a>
<a class="sourceLine" id="cb7-47" title="47">    df[<span class="st">&#39;title_osc&#39;</span>] <span class="op">=</span> df[<span class="st">&#39;title_osc&#39;</span>] <span class="op">+</span> <span class="st">&quot; &lt;/s&gt;&quot;</span></a>
<a class="sourceLine" id="cb7-48" title="48">    <span class="cf">return</span> df</a>
<a class="sourceLine" id="cb7-49" title="49"></a>
<a class="sourceLine" id="cb7-50" title="50"></a>
<a class="sourceLine" id="cb7-51" title="51"><span class="co"># clean df</span></a>
<a class="sourceLine" id="cb7-52" title="52">df <span class="op">=</span> clean_df(df)</a></code></pre></div>
</div>
<div id="instantiate-tokenizer" class="section level2">
<h2><span class="header-section-number">1.2</span> Instantiate Tokenizer</h2>
<p>Next, we instantiate the T5 tokenizer from <code>transformers</code> and check some special token IDs.</p>
<div class="sourceCode" id="cb8"><pre class="sourceCode python"><code class="sourceCode python"><a class="sourceLine" id="cb8-1" title="1"><span class="co"># instantiate T5 tokenizer</span></a>
<a class="sourceLine" id="cb8-2" title="2">tokenizer <span class="op">=</span> T5Tokenizer.from_pretrained(<span class="st">&#39;t5-small&#39;</span>)</a>
<a class="sourceLine" id="cb8-3" title="3"></a>
<a class="sourceLine" id="cb8-4" title="4"><span class="co"># check token ids</span></a>
<a class="sourceLine" id="cb8-5" title="5">tokenizer.eos_token_id</a></code></pre></div>
<pre><code>## 1</code></pre>
<div class="sourceCode" id="cb10"><pre class="sourceCode python"><code class="sourceCode python"><a class="sourceLine" id="cb10-1" title="1">tokenizer.bos_token_id</a>
<a class="sourceLine" id="cb10-2" title="2">tokenizer.unk_token_id</a></code></pre></div>
<pre><code>## 2</code></pre>
<div class="sourceCode" id="cb12"><pre class="sourceCode python"><code class="sourceCode python"><a class="sourceLine" id="cb12-1" title="1">tokenizer.pad_token_id</a></code></pre></div>
<pre><code>## 0</code></pre>
</div>
<div id="tokenize-the-corpus" class="section level2">
<h2><span class="header-section-number">1.3</span> Tokenize the Corpus</h2>
<p>Then, we proceed to tokenize our corpus like usual. Notice that we effectively do this process twice as we tokenize our corpus and also tokenize our summaries.</p>
<div class="sourceCode" id="cb14"><pre class="sourceCode python"><code class="sourceCode python"><a class="sourceLine" id="cb14-1" title="1"><span class="co"># tokenize the main text</span></a>
<a class="sourceLine" id="cb14-2" title="2"><span class="kw">def</span> tokenize_corpus(df, tokenizer, max_len):</a>
<a class="sourceLine" id="cb14-3" title="3">    <span class="co"># token ID storage</span></a>
<a class="sourceLine" id="cb14-4" title="4">    input_ids <span class="op">=</span> []</a>
<a class="sourceLine" id="cb14-5" title="5">    <span class="co"># attension mask storage</span></a>
<a class="sourceLine" id="cb14-6" title="6">    attention_masks <span class="op">=</span> []</a>
<a class="sourceLine" id="cb14-7" title="7">    <span class="co"># max len -- 512 is max</span></a>
<a class="sourceLine" id="cb14-8" title="8">    max_len <span class="op">=</span> max_len</a>
<a class="sourceLine" id="cb14-9" title="9">    <span class="co"># for every document:</span></a>
<a class="sourceLine" id="cb14-10" title="10">    <span class="cf">for</span> doc <span class="kw">in</span> df:</a>
<a class="sourceLine" id="cb14-11" title="11">        <span class="co"># `encode_plus` will:</span></a>
<a class="sourceLine" id="cb14-12" title="12">        <span class="co">#   (1) Tokenize the sentence.</span></a>
<a class="sourceLine" id="cb14-13" title="13">        <span class="co">#   (2) Prepend the `[CLS]` token to the start.</span></a>
<a class="sourceLine" id="cb14-14" title="14">        <span class="co">#   (3) Append the `[SEP]` token to the end.</span></a>
<a class="sourceLine" id="cb14-15" title="15">        <span class="co">#   (4) Map tokens to their IDs.</span></a>
<a class="sourceLine" id="cb14-16" title="16">        <span class="co">#   (5) Pad or truncate the sentence to `max_length`</span></a>
<a class="sourceLine" id="cb14-17" title="17">        <span class="co">#   (6) Create attention masks for [PAD] tokens.</span></a>
<a class="sourceLine" id="cb14-18" title="18">        encoded_dict <span class="op">=</span> tokenizer.encode_plus(</a>
<a class="sourceLine" id="cb14-19" title="19">                            doc,  <span class="co"># document to encode.</span></a>
<a class="sourceLine" id="cb14-20" title="20">                            add_special_tokens<span class="op">=</span><span class="va">True</span>,  <span class="co"># add tokens relative to model</span></a>
<a class="sourceLine" id="cb14-21" title="21">                            max_length<span class="op">=</span>max_len,  <span class="co"># set max length</span></a>
<a class="sourceLine" id="cb14-22" title="22">                            truncation<span class="op">=</span><span class="va">True</span>,  <span class="co"># truncate longer messages</span></a>
<a class="sourceLine" id="cb14-23" title="23">                            pad_to_max_length<span class="op">=</span><span class="va">True</span>,  <span class="co"># add padding</span></a>
<a class="sourceLine" id="cb14-24" title="24">                            return_attention_mask<span class="op">=</span><span class="va">True</span>,  <span class="co"># create attn. masks</span></a>
<a class="sourceLine" id="cb14-25" title="25">                            return_tensors<span class="op">=</span><span class="st">&#39;pt&#39;</span>  <span class="co"># return pytorch tensors</span></a>
<a class="sourceLine" id="cb14-26" title="26">                       )</a>
<a class="sourceLine" id="cb14-27" title="27"></a>
<a class="sourceLine" id="cb14-28" title="28">        <span class="co"># add the tokenized sentence to the list</span></a>
<a class="sourceLine" id="cb14-29" title="29">        input_ids.append(encoded_dict[<span class="st">&#39;input_ids&#39;</span>])</a>
<a class="sourceLine" id="cb14-30" title="30"></a>
<a class="sourceLine" id="cb14-31" title="31">        <span class="co"># and its attention mask (differentiates padding from non-padding)</span></a>
<a class="sourceLine" id="cb14-32" title="32">        attention_masks.append(encoded_dict[<span class="st">&#39;attention_mask&#39;</span>])</a>
<a class="sourceLine" id="cb14-33" title="33"></a>
<a class="sourceLine" id="cb14-34" title="34">    <span class="cf">return</span> torch.cat(input_ids, dim<span class="op">=</span><span class="dv">0</span>), torch.cat(attention_masks, dim<span class="op">=</span><span class="dv">0</span>)</a>
<a class="sourceLine" id="cb14-35" title="35"></a>
<a class="sourceLine" id="cb14-36" title="36"></a>
<a class="sourceLine" id="cb14-37" title="37"><span class="co"># create tokenized data</span></a>
<a class="sourceLine" id="cb14-38" title="38">body_input_ids, body_attention_masks <span class="op">=</span> tokenize_corpus(df[<span class="st">&#39;body&#39;</span>].values, tokenizer, <span class="dv">512</span>)</a>
<a class="sourceLine" id="cb14-39" title="39"></a>
<a class="sourceLine" id="cb14-40" title="40"></a>
<a class="sourceLine" id="cb14-41" title="41"></a>
<a class="sourceLine" id="cb14-42" title="42"><span class="co"># how long are tokenized summaries</span></a>
<a class="sourceLine" id="cb14-43" title="43">ls <span class="op">=</span> []</a>
<a class="sourceLine" id="cb14-44" title="44"><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(df.shape[<span class="dv">0</span>]):</a>
<a class="sourceLine" id="cb14-45" title="45">    ls.append(<span class="bu">len</span>(tokenizer.tokenize(df.iloc[i][<span class="st">&#39;title_osc&#39;</span>])))</a>
<a class="sourceLine" id="cb14-46" title="46"></a>
<a class="sourceLine" id="cb14-47" title="47">temp_df <span class="op">=</span> pd.DataFrame({<span class="st">&#39;len_tokens&#39;</span>: ls})</a>
<a class="sourceLine" id="cb14-48" title="48">temp_df[<span class="st">&#39;len_tokens&#39;</span>].mean()  <span class="co"># 19</span></a></code></pre></div>
<pre><code>## 19.956993529118964</code></pre>
<div class="sourceCode" id="cb16"><pre class="sourceCode python"><code class="sourceCode python"><a class="sourceLine" id="cb16-1" title="1">temp_df[<span class="st">&#39;len_tokens&#39;</span>].median()  <span class="co"># 20</span></a></code></pre></div>
<pre><code>## 20.0</code></pre>
<div class="sourceCode" id="cb18"><pre class="sourceCode python"><code class="sourceCode python"><a class="sourceLine" id="cb18-1" title="1">temp_df[<span class="st">&#39;len_tokens&#39;</span>].<span class="bu">max</span>()  <span class="co"># 44</span></a></code></pre></div>
<pre><code>## 44</code></pre>
<div class="sourceCode" id="cb20"><pre class="sourceCode python"><code class="sourceCode python"><a class="sourceLine" id="cb20-1" title="1">temp_df.quantile(<span class="fl">0.99</span>)  <span class="co"># 30</span></a>
<a class="sourceLine" id="cb20-2" title="2"></a>
<a class="sourceLine" id="cb20-3" title="3"><span class="co"># create tokenized summaries</span></a></code></pre></div>
<pre><code>## len_tokens    30.0
## Name: 0.99, dtype: float64</code></pre>
<div class="sourceCode" id="cb22"><pre class="sourceCode python"><code class="sourceCode python"><a class="sourceLine" id="cb22-1" title="1">summary_input_ids, summary_attention_masks <span class="op">=</span> tokenize_corpus(df[<span class="st">&#39;title_osc&#39;</span>].values, tokenizer, <span class="dv">44</span>)</a></code></pre></div>
</div>
<div id="prepare-and-split-data" class="section level2">
<h2><span class="header-section-number">1.4</span> Prepare and Split Data</h2>
<p>Next, we split our data into train, validation, and test sets.</p>
<div class="sourceCode" id="cb23"><pre class="sourceCode python"><code class="sourceCode python"><a class="sourceLine" id="cb23-1" title="1"><span class="co"># prepare tensor data sets</span></a>
<a class="sourceLine" id="cb23-2" title="2"><span class="kw">def</span> prepare_dataset(body_tokens, body_masks, summary_token, summary_masks):</a>
<a class="sourceLine" id="cb23-3" title="3">    <span class="co"># create tensor data sets</span></a>
<a class="sourceLine" id="cb23-4" title="4">    tensor_df <span class="op">=</span> TensorDataset(body_tokens, body_masks, summary_token, summary_masks)</a>
<a class="sourceLine" id="cb23-5" title="5">    <span class="co"># 80% of df</span></a>
<a class="sourceLine" id="cb23-6" title="6">    train_size <span class="op">=</span> <span class="bu">int</span>(<span class="fl">0.8</span> <span class="op">*</span> <span class="bu">len</span>(df))</a>
<a class="sourceLine" id="cb23-7" title="7">    <span class="co"># 20% of df</span></a>
<a class="sourceLine" id="cb23-8" title="8">    val_size <span class="op">=</span> <span class="bu">len</span>(df) <span class="op">-</span> train_size</a>
<a class="sourceLine" id="cb23-9" title="9">    <span class="co"># 50% of validation</span></a>
<a class="sourceLine" id="cb23-10" title="10">    test_size <span class="op">=</span> <span class="bu">int</span>(val_size <span class="op">-</span> <span class="fl">0.5</span><span class="op">*</span>val_size)</a>
<a class="sourceLine" id="cb23-11" title="11">    <span class="co"># divide the dataset by randomly selecting samples</span></a>
<a class="sourceLine" id="cb23-12" title="12">    train_dataset, val_dataset <span class="op">=</span> random_split(tensor_df, [train_size, val_size])</a>
<a class="sourceLine" id="cb23-13" title="13">    <span class="co"># divide validation by randomly selecting samples</span></a>
<a class="sourceLine" id="cb23-14" title="14">    val_dataset, test_dataset <span class="op">=</span> random_split(val_dataset, [test_size, test_size<span class="op">+</span><span class="dv">1</span>])</a>
<a class="sourceLine" id="cb23-15" title="15"></a>
<a class="sourceLine" id="cb23-16" title="16">    <span class="cf">return</span> train_dataset, val_dataset, test_dataset</a>
<a class="sourceLine" id="cb23-17" title="17"></a>
<a class="sourceLine" id="cb23-18" title="18"></a>
<a class="sourceLine" id="cb23-19" title="19"><span class="co"># create tensor data sets</span></a>
<a class="sourceLine" id="cb23-20" title="20">train_dataset, val_dataset, test_dataset <span class="op">=</span> prepare_dataset(body_input_ids,</a>
<a class="sourceLine" id="cb23-21" title="21">                                                           body_attention_masks,</a>
<a class="sourceLine" id="cb23-22" title="22">                                                           summary_input_ids,</a>
<a class="sourceLine" id="cb23-23" title="23">                                                           summary_attention_masks</a>
<a class="sourceLine" id="cb23-24" title="24">                                                           )</a></code></pre></div>
</div>
<div id="instantiate-training-models" class="section level2">
<h2><span class="header-section-number">1.5</span> Instantiate Training Models</h2>
<p>Now we are ready to prepare our training scripts which follow the other guides closely. <code>T5ForConditionalGeneration</code> asks that we supply three inputs into the model’s <code>forward</code> function: (1) corpus token ids, (2) corpus attention masks, (3) our summaries, and (4) our summary attention masks.</p>
<div class="sourceCode" id="cb24"><pre class="sourceCode python"><code class="sourceCode python"><a class="sourceLine" id="cb24-1" title="1"><span class="kw">def</span> train(model, dataloader, optimizer):</a>
<a class="sourceLine" id="cb24-2" title="2"></a>
<a class="sourceLine" id="cb24-3" title="3">    <span class="co"># capture time</span></a>
<a class="sourceLine" id="cb24-4" title="4">    total_t0 <span class="op">=</span> time.time()</a>
<a class="sourceLine" id="cb24-5" title="5"></a>
<a class="sourceLine" id="cb24-6" title="6">    <span class="co"># Perform one full pass over the training set.</span></a>
<a class="sourceLine" id="cb24-7" title="7">    <span class="bu">print</span>(<span class="st">&quot;&quot;</span>)</a>
<a class="sourceLine" id="cb24-8" title="8">    <span class="bu">print</span>(<span class="st">&#39;======== Epoch </span><span class="sc">{:}</span><span class="st"> / </span><span class="sc">{:}</span><span class="st"> ========&#39;</span>.<span class="bu">format</span>(epoch <span class="op">+</span> <span class="dv">1</span>, epochs))</a>
<a class="sourceLine" id="cb24-9" title="9">    <span class="bu">print</span>(<span class="st">&#39;Training...&#39;</span>)</a>
<a class="sourceLine" id="cb24-10" title="10"></a>
<a class="sourceLine" id="cb24-11" title="11">    <span class="co"># reset total loss for epoch</span></a>
<a class="sourceLine" id="cb24-12" title="12">    train_total_loss <span class="op">=</span> <span class="dv">0</span></a>
<a class="sourceLine" id="cb24-13" title="13">    total_train_f1 <span class="op">=</span> <span class="dv">0</span></a>
<a class="sourceLine" id="cb24-14" title="14"></a>
<a class="sourceLine" id="cb24-15" title="15">    <span class="co"># put model into traning mode</span></a>
<a class="sourceLine" id="cb24-16" title="16">    model.train()</a>
<a class="sourceLine" id="cb24-17" title="17"></a>
<a class="sourceLine" id="cb24-18" title="18">    <span class="co"># for each batch of training data...</span></a>
<a class="sourceLine" id="cb24-19" title="19">    <span class="cf">for</span> step, batch <span class="kw">in</span> <span class="bu">enumerate</span>(dataloader):</a>
<a class="sourceLine" id="cb24-20" title="20"></a>
<a class="sourceLine" id="cb24-21" title="21">        <span class="co"># progress update every 40 batches.</span></a>
<a class="sourceLine" id="cb24-22" title="22">        <span class="cf">if</span> step <span class="op">%</span> <span class="dv">40</span> <span class="op">==</span> <span class="dv">0</span> <span class="kw">and</span> <span class="kw">not</span> step <span class="op">==</span> <span class="dv">0</span>:</a>
<a class="sourceLine" id="cb24-23" title="23"></a>
<a class="sourceLine" id="cb24-24" title="24">            <span class="co"># Report progress.</span></a>
<a class="sourceLine" id="cb24-25" title="25">            <span class="bu">print</span>(<span class="st">&#39;  Batch {:&gt;5,}  of  {:&gt;5,}.&#39;</span>.<span class="bu">format</span>(step, <span class="bu">len</span>(dataloader)))</a>
<a class="sourceLine" id="cb24-26" title="26"></a>
<a class="sourceLine" id="cb24-27" title="27">        <span class="co"># Unpack this training batch from our dataloader:</span></a>
<a class="sourceLine" id="cb24-28" title="28">        <span class="co">#</span></a>
<a class="sourceLine" id="cb24-29" title="29">        <span class="co"># As we unpack the batch, we&#39;ll also copy each tensor to the GPU using</span></a>
<a class="sourceLine" id="cb24-30" title="30">        <span class="co"># the `to` method.</span></a>
<a class="sourceLine" id="cb24-31" title="31">        <span class="co">#</span></a>
<a class="sourceLine" id="cb24-32" title="32">        <span class="co"># `batch` contains three pytorch tensors:</span></a>
<a class="sourceLine" id="cb24-33" title="33">        <span class="co">#   [0]: input tokens</span></a>
<a class="sourceLine" id="cb24-34" title="34">        <span class="co">#   [1]: attention masks</span></a>
<a class="sourceLine" id="cb24-35" title="35">        <span class="co">#   [2]: summary tokens</span></a>
<a class="sourceLine" id="cb24-36" title="36">        <span class="co">#   [3]: summary masks</span></a>
<a class="sourceLine" id="cb24-37" title="37">        b_input_ids <span class="op">=</span> batch[<span class="dv">0</span>].cuda()</a>
<a class="sourceLine" id="cb24-38" title="38">        b_input_mask <span class="op">=</span> batch[<span class="dv">1</span>].cuda()</a>
<a class="sourceLine" id="cb24-39" title="39">        b_summary_ids <span class="op">=</span> batch[<span class="dv">2</span>].cuda()</a>
<a class="sourceLine" id="cb24-40" title="40">        b_summary_mask <span class="op">=</span> batch[<span class="dv">3</span>].cuda()</a>
<a class="sourceLine" id="cb24-41" title="41"></a>
<a class="sourceLine" id="cb24-42" title="42">        <span class="co"># clear previously calculated gradients</span></a>
<a class="sourceLine" id="cb24-43" title="43">        optimizer.zero_grad()</a>
<a class="sourceLine" id="cb24-44" title="44"></a>
<a class="sourceLine" id="cb24-45" title="45">        <span class="co"># runs the forward pass with autocasting.</span></a>
<a class="sourceLine" id="cb24-46" title="46">        <span class="cf">with</span> autocast():</a>
<a class="sourceLine" id="cb24-47" title="47">            <span class="co"># forward propagation (evaluate model on training batch)</span></a>
<a class="sourceLine" id="cb24-48" title="48">            outputs <span class="op">=</span> model(input_ids<span class="op">=</span>b_input_ids,</a>
<a class="sourceLine" id="cb24-49" title="49">                            attention_mask<span class="op">=</span>b_input_mask,</a>
<a class="sourceLine" id="cb24-50" title="50">                            labels<span class="op">=</span>b_summary_ids,</a>
<a class="sourceLine" id="cb24-51" title="51">                            decoder_attention_mask<span class="op">=</span>b_summary_mask)</a>
<a class="sourceLine" id="cb24-52" title="52"></a>
<a class="sourceLine" id="cb24-53" title="53">            loss, prediction_scores <span class="op">=</span> outputs[:<span class="dv">2</span>]</a>
<a class="sourceLine" id="cb24-54" title="54"></a>
<a class="sourceLine" id="cb24-55" title="55">            <span class="co"># sum the training loss over all batches for average loss at end</span></a>
<a class="sourceLine" id="cb24-56" title="56">            <span class="co"># loss is a tensor containing a single value</span></a>
<a class="sourceLine" id="cb24-57" title="57">            train_total_loss <span class="op">+=</span> loss.item()</a>
<a class="sourceLine" id="cb24-58" title="58"></a>
<a class="sourceLine" id="cb24-59" title="59">        <span class="co"># Scales loss.  Calls backward() on scaled loss to create scaled gradients.</span></a>
<a class="sourceLine" id="cb24-60" title="60">        <span class="co"># Backward passes under autocast are not recommended.</span></a>
<a class="sourceLine" id="cb24-61" title="61">        <span class="co"># Backward ops run in the same dtype autocast chose for corresponding forward ops.</span></a>
<a class="sourceLine" id="cb24-62" title="62">        scaler.scale(loss).backward()</a>
<a class="sourceLine" id="cb24-63" title="63"></a>
<a class="sourceLine" id="cb24-64" title="64">        <span class="co"># scaler.step() first unscales the gradients of the optimizer&#39;s assigned params.</span></a>
<a class="sourceLine" id="cb24-65" title="65">        <span class="co"># If these gradients do not contain infs or NaNs, optimizer.step() is then called,</span></a>
<a class="sourceLine" id="cb24-66" title="66">        <span class="co"># otherwise, optimizer.step() is skipped.</span></a>
<a class="sourceLine" id="cb24-67" title="67">        scaler.step(optimizer)</a>
<a class="sourceLine" id="cb24-68" title="68"></a>
<a class="sourceLine" id="cb24-69" title="69">        <span class="co"># Updates the scale for next iteration.</span></a>
<a class="sourceLine" id="cb24-70" title="70">        scaler.update()</a>
<a class="sourceLine" id="cb24-71" title="71"></a>
<a class="sourceLine" id="cb24-72" title="72">        <span class="co"># update the learning rate</span></a>
<a class="sourceLine" id="cb24-73" title="73">        scheduler.step()</a>
<a class="sourceLine" id="cb24-74" title="74"></a>
<a class="sourceLine" id="cb24-75" title="75">    <span class="co"># calculate the average loss over all of the batches</span></a>
<a class="sourceLine" id="cb24-76" title="76">    avg_train_loss <span class="op">=</span> train_total_loss <span class="op">/</span> <span class="bu">len</span>(dataloader)</a>
<a class="sourceLine" id="cb24-77" title="77"></a>
<a class="sourceLine" id="cb24-78" title="78">    <span class="co"># Record all statistics from this epoch.</span></a>
<a class="sourceLine" id="cb24-79" title="79">    training_stats.append(</a>
<a class="sourceLine" id="cb24-80" title="80">        {</a>
<a class="sourceLine" id="cb24-81" title="81">            <span class="st">&#39;Train Loss&#39;</span>: avg_train_loss</a>
<a class="sourceLine" id="cb24-82" title="82">        }</a>
<a class="sourceLine" id="cb24-83" title="83">    )</a>
<a class="sourceLine" id="cb24-84" title="84"></a>
<a class="sourceLine" id="cb24-85" title="85">    <span class="co"># training time end</span></a>
<a class="sourceLine" id="cb24-86" title="86">    training_time <span class="op">=</span> format_time(time.time() <span class="op">-</span> total_t0)</a>
<a class="sourceLine" id="cb24-87" title="87"></a>
<a class="sourceLine" id="cb24-88" title="88">    <span class="co"># print result summaries</span></a>
<a class="sourceLine" id="cb24-89" title="89">    <span class="bu">print</span>(<span class="st">&quot;&quot;</span>)</a>
<a class="sourceLine" id="cb24-90" title="90">    <span class="bu">print</span>(<span class="st">&quot;summary results&quot;</span>)</a>
<a class="sourceLine" id="cb24-91" title="91">    <span class="bu">print</span>(<span class="st">&quot;epoch | trn loss | trn time &quot;</span>)</a>
<a class="sourceLine" id="cb24-92" title="92">    <span class="bu">print</span>(<span class="ss">f&quot;</span><span class="sc">{</span>epoch<span class="op">+</span><span class="dv">1</span><span class="sc">:5d}</span><span class="ss"> | </span><span class="sc">{</span>avg_train_loss<span class="sc">:.5f}</span><span class="ss"> | </span><span class="sc">{</span>training_time<span class="sc">:}</span><span class="ss">&quot;</span>)</a>
<a class="sourceLine" id="cb24-93" title="93"></a>
<a class="sourceLine" id="cb24-94" title="94">    <span class="cf">return</span> training_stats</a>
<a class="sourceLine" id="cb24-95" title="95"></a>
<a class="sourceLine" id="cb24-96" title="96"></a>
<a class="sourceLine" id="cb24-97" title="97"><span class="kw">def</span> validating(model, dataloader):</a>
<a class="sourceLine" id="cb24-98" title="98"></a>
<a class="sourceLine" id="cb24-99" title="99">    <span class="co"># capture validation time</span></a>
<a class="sourceLine" id="cb24-100" title="100">    total_t0 <span class="op">=</span> time.time()</a>
<a class="sourceLine" id="cb24-101" title="101"></a>
<a class="sourceLine" id="cb24-102" title="102">    <span class="co"># After the completion of each training epoch, measure our performance on</span></a>
<a class="sourceLine" id="cb24-103" title="103">    <span class="co"># our validation set.</span></a>
<a class="sourceLine" id="cb24-104" title="104">    <span class="bu">print</span>(<span class="st">&quot;&quot;</span>)</a>
<a class="sourceLine" id="cb24-105" title="105">    <span class="bu">print</span>(<span class="st">&quot;Running Validation...&quot;</span>)</a>
<a class="sourceLine" id="cb24-106" title="106"></a>
<a class="sourceLine" id="cb24-107" title="107">    <span class="co"># put the model in evaluation mode</span></a>
<a class="sourceLine" id="cb24-108" title="108">    model.<span class="bu">eval</span>()</a>
<a class="sourceLine" id="cb24-109" title="109"></a>
<a class="sourceLine" id="cb24-110" title="110">    <span class="co"># track variables</span></a>
<a class="sourceLine" id="cb24-111" title="111">    total_valid_loss <span class="op">=</span> <span class="dv">0</span></a>
<a class="sourceLine" id="cb24-112" title="112"></a>
<a class="sourceLine" id="cb24-113" title="113">    <span class="co"># evaluate data for one epoch</span></a>
<a class="sourceLine" id="cb24-114" title="114">    <span class="cf">for</span> batch <span class="kw">in</span> dataloader:</a>
<a class="sourceLine" id="cb24-115" title="115"></a>
<a class="sourceLine" id="cb24-116" title="116">        <span class="co"># Unpack this training batch from our dataloader:</span></a>
<a class="sourceLine" id="cb24-117" title="117">        <span class="co"># `batch` contains three pytorch tensors:</span></a>
<a class="sourceLine" id="cb24-118" title="118">        <span class="co">#   [0]: input tokens</span></a>
<a class="sourceLine" id="cb24-119" title="119">        <span class="co">#   [1]: attention masks</span></a>
<a class="sourceLine" id="cb24-120" title="120">        <span class="co">#   [2]: summary tokens</span></a>
<a class="sourceLine" id="cb24-121" title="121">        <span class="co">#   [3]: summary masks</span></a>
<a class="sourceLine" id="cb24-122" title="122">        b_input_ids <span class="op">=</span> batch[<span class="dv">0</span>].cuda()</a>
<a class="sourceLine" id="cb24-123" title="123">        b_input_mask <span class="op">=</span> batch[<span class="dv">1</span>].cuda()</a>
<a class="sourceLine" id="cb24-124" title="124">        b_summary_ids <span class="op">=</span> batch[<span class="dv">2</span>].cuda()</a>
<a class="sourceLine" id="cb24-125" title="125">        b_summary_mask <span class="op">=</span> batch[<span class="dv">3</span>].cuda()</a>
<a class="sourceLine" id="cb24-126" title="126"></a>
<a class="sourceLine" id="cb24-127" title="127">        <span class="co"># tell pytorch not to bother calculating gradients</span></a>
<a class="sourceLine" id="cb24-128" title="128">        <span class="co"># as its only necessary for training</span></a>
<a class="sourceLine" id="cb24-129" title="129">        <span class="cf">with</span> torch.no_grad():</a>
<a class="sourceLine" id="cb24-130" title="130"></a>
<a class="sourceLine" id="cb24-131" title="131">            <span class="co"># forward propagation (evaluate model on training batch)</span></a>
<a class="sourceLine" id="cb24-132" title="132">            outputs <span class="op">=</span> model(input_ids<span class="op">=</span>b_input_ids,</a>
<a class="sourceLine" id="cb24-133" title="133">                            attention_mask<span class="op">=</span>b_input_mask,</a>
<a class="sourceLine" id="cb24-134" title="134">                            labels<span class="op">=</span>b_summary_ids,</a>
<a class="sourceLine" id="cb24-135" title="135">                            decoder_attention_mask<span class="op">=</span>b_summary_mask)</a>
<a class="sourceLine" id="cb24-136" title="136"></a>
<a class="sourceLine" id="cb24-137" title="137">            loss, prediction_scores <span class="op">=</span> outputs[:<span class="dv">2</span>]</a>
<a class="sourceLine" id="cb24-138" title="138"></a>
<a class="sourceLine" id="cb24-139" title="139">            <span class="co"># sum the training loss over all batches for average loss at end</span></a>
<a class="sourceLine" id="cb24-140" title="140">            <span class="co"># loss is a tensor containing a single value</span></a>
<a class="sourceLine" id="cb24-141" title="141">            total_valid_loss <span class="op">+=</span> loss.item()</a>
<a class="sourceLine" id="cb24-142" title="142"></a>
<a class="sourceLine" id="cb24-143" title="143">    <span class="co"># calculate the average loss over all of the batches.</span></a>
<a class="sourceLine" id="cb24-144" title="144">    <span class="kw">global</span> avg_val_loss</a>
<a class="sourceLine" id="cb24-145" title="145">    avg_val_loss <span class="op">=</span> total_valid_loss <span class="op">/</span> <span class="bu">len</span>(dataloader)</a>
<a class="sourceLine" id="cb24-146" title="146"></a>
<a class="sourceLine" id="cb24-147" title="147">    <span class="co"># Record all statistics from this epoch.</span></a>
<a class="sourceLine" id="cb24-148" title="148">    valid_stats.append(</a>
<a class="sourceLine" id="cb24-149" title="149">        {</a>
<a class="sourceLine" id="cb24-150" title="150">            <span class="st">&#39;Val Loss&#39;</span>: avg_val_loss,</a>
<a class="sourceLine" id="cb24-151" title="151">            <span class="st">&#39;Val PPL.&#39;</span>: np.exp(avg_val_loss)</a>
<a class="sourceLine" id="cb24-152" title="152">        }</a>
<a class="sourceLine" id="cb24-153" title="153">    )</a>
<a class="sourceLine" id="cb24-154" title="154"></a>
<a class="sourceLine" id="cb24-155" title="155">    <span class="co"># capture end validation time</span></a>
<a class="sourceLine" id="cb24-156" title="156">    training_time <span class="op">=</span> format_time(time.time() <span class="op">-</span> total_t0)</a>
<a class="sourceLine" id="cb24-157" title="157"></a>
<a class="sourceLine" id="cb24-158" title="158">    <span class="co"># print result summaries</span></a>
<a class="sourceLine" id="cb24-159" title="159">    <span class="bu">print</span>(<span class="st">&quot;&quot;</span>)</a>
<a class="sourceLine" id="cb24-160" title="160">    <span class="bu">print</span>(<span class="st">&quot;summary results&quot;</span>)</a>
<a class="sourceLine" id="cb24-161" title="161">    <span class="bu">print</span>(<span class="st">&quot;epoch | val loss | val ppl | val time&quot;</span>)</a>
<a class="sourceLine" id="cb24-162" title="162">    <span class="bu">print</span>(<span class="ss">f&quot;</span><span class="sc">{</span>epoch<span class="op">+</span><span class="dv">1</span><span class="sc">:5d}</span><span class="ss"> | </span><span class="sc">{</span>avg_val_loss<span class="sc">:.5f}</span><span class="ss"> | </span><span class="sc">{np.</span>exp(avg_val_loss)<span class="sc">:.3f}</span><span class="ss"> | </span><span class="sc">{</span>training_time<span class="sc">:}</span><span class="ss">&quot;</span>)</a>
<a class="sourceLine" id="cb24-163" title="163"></a>
<a class="sourceLine" id="cb24-164" title="164">    <span class="cf">return</span> valid_stats</a>
<a class="sourceLine" id="cb24-165" title="165"></a>
<a class="sourceLine" id="cb24-166" title="166"></a>
<a class="sourceLine" id="cb24-167" title="167"><span class="kw">def</span> testing(model, dataloader):</a>
<a class="sourceLine" id="cb24-168" title="168"></a>
<a class="sourceLine" id="cb24-169" title="169">    <span class="bu">print</span>(<span class="st">&quot;&quot;</span>)</a>
<a class="sourceLine" id="cb24-170" title="170">    <span class="bu">print</span>(<span class="st">&quot;Running Testing...&quot;</span>)</a>
<a class="sourceLine" id="cb24-171" title="171"></a>
<a class="sourceLine" id="cb24-172" title="172">    <span class="co"># measure training time</span></a>
<a class="sourceLine" id="cb24-173" title="173">    t0 <span class="op">=</span> time.time()</a>
<a class="sourceLine" id="cb24-174" title="174"></a>
<a class="sourceLine" id="cb24-175" title="175">    <span class="co"># put the model in evaluation mode</span></a>
<a class="sourceLine" id="cb24-176" title="176">    model.<span class="bu">eval</span>()</a>
<a class="sourceLine" id="cb24-177" title="177"></a>
<a class="sourceLine" id="cb24-178" title="178">    <span class="co"># track variables</span></a>
<a class="sourceLine" id="cb24-179" title="179">    total_test_loss <span class="op">=</span> <span class="dv">0</span></a>
<a class="sourceLine" id="cb24-180" title="180">    predictions <span class="op">=</span> []</a>
<a class="sourceLine" id="cb24-181" title="181">    actuals <span class="op">=</span> []</a>
<a class="sourceLine" id="cb24-182" title="182"></a>
<a class="sourceLine" id="cb24-183" title="183">    <span class="co"># evaluate data for one epoch</span></a>
<a class="sourceLine" id="cb24-184" title="184">    <span class="cf">for</span> step, batch <span class="kw">in</span> <span class="bu">enumerate</span>(dataloader):</a>
<a class="sourceLine" id="cb24-185" title="185">        <span class="co"># progress update every 40 batches.</span></a>
<a class="sourceLine" id="cb24-186" title="186">        <span class="cf">if</span> step <span class="op">%</span> <span class="dv">40</span> <span class="op">==</span> <span class="dv">0</span> <span class="kw">and</span> <span class="kw">not</span> step <span class="op">==</span> <span class="dv">0</span>:</a>
<a class="sourceLine" id="cb24-187" title="187">            <span class="co"># Calculate elapsed time in minutes.</span></a>
<a class="sourceLine" id="cb24-188" title="188">            elapsed <span class="op">=</span> format_time(time.time() <span class="op">-</span> t0)</a>
<a class="sourceLine" id="cb24-189" title="189">            <span class="co"># Report progress.</span></a>
<a class="sourceLine" id="cb24-190" title="190">            <span class="bu">print</span>(<span class="st">&#39;  Batch {:&gt;5,}  of  {:&gt;5,}.    Elapsed: </span><span class="sc">{:}</span><span class="st">.&#39;</span>.<span class="bu">format</span>(step, <span class="bu">len</span>(dataloader), elapsed))</a>
<a class="sourceLine" id="cb24-191" title="191"></a>
<a class="sourceLine" id="cb24-192" title="192">        <span class="co"># Unpack this training batch from our dataloader:</span></a>
<a class="sourceLine" id="cb24-193" title="193">        <span class="co"># `batch` contains three pytorch tensors:</span></a>
<a class="sourceLine" id="cb24-194" title="194">        <span class="co">#   [0]: input tokens</span></a>
<a class="sourceLine" id="cb24-195" title="195">        <span class="co">#   [1]: attention masks</span></a>
<a class="sourceLine" id="cb24-196" title="196">        <span class="co">#   [2]: summary tokens</span></a>
<a class="sourceLine" id="cb24-197" title="197">        <span class="co">#   [3]: summary masks</span></a>
<a class="sourceLine" id="cb24-198" title="198">        b_input_ids <span class="op">=</span> batch[<span class="dv">0</span>].cuda()</a>
<a class="sourceLine" id="cb24-199" title="199">        b_input_mask <span class="op">=</span> batch[<span class="dv">1</span>].cuda()</a>
<a class="sourceLine" id="cb24-200" title="200">        b_summary_ids <span class="op">=</span> batch[<span class="dv">2</span>].cuda()</a>
<a class="sourceLine" id="cb24-201" title="201">        b_summary_mask <span class="op">=</span> batch[<span class="dv">3</span>].cuda()</a>
<a class="sourceLine" id="cb24-202" title="202"></a>
<a class="sourceLine" id="cb24-203" title="203">        <span class="co"># tell pytorch not to bother calculating gradients</span></a>
<a class="sourceLine" id="cb24-204" title="204">        <span class="co"># as its only necessary for training</span></a>
<a class="sourceLine" id="cb24-205" title="205">        <span class="cf">with</span> torch.no_grad():</a>
<a class="sourceLine" id="cb24-206" title="206"></a>
<a class="sourceLine" id="cb24-207" title="207">            <span class="co"># forward propagation (evaluate model on training batch)</span></a>
<a class="sourceLine" id="cb24-208" title="208">            outputs <span class="op">=</span> model(input_ids<span class="op">=</span>b_input_ids,</a>
<a class="sourceLine" id="cb24-209" title="209">                            attention_mask<span class="op">=</span>b_input_mask,</a>
<a class="sourceLine" id="cb24-210" title="210">                            labels<span class="op">=</span>b_summary_ids,</a>
<a class="sourceLine" id="cb24-211" title="211">                            decoder_attention_mask<span class="op">=</span>b_summary_mask)</a>
<a class="sourceLine" id="cb24-212" title="212"></a>
<a class="sourceLine" id="cb24-213" title="213">            loss, prediction_scores <span class="op">=</span> outputs[:<span class="dv">2</span>]</a>
<a class="sourceLine" id="cb24-214" title="214"></a>
<a class="sourceLine" id="cb24-215" title="215">            total_test_loss <span class="op">+=</span> loss.item()</a>
<a class="sourceLine" id="cb24-216" title="216"></a>
<a class="sourceLine" id="cb24-217" title="217">            generated_ids <span class="op">=</span> model.generate(</a>
<a class="sourceLine" id="cb24-218" title="218">                    input_ids<span class="op">=</span>b_input_ids,</a>
<a class="sourceLine" id="cb24-219" title="219">                    attention_mask<span class="op">=</span>b_input_mask,</a>
<a class="sourceLine" id="cb24-220" title="220">                    do_sample<span class="op">=</span><span class="va">True</span>,  <span class="co"># add some random sampling</span></a>
<a class="sourceLine" id="cb24-221" title="221">                    temperature<span class="op">=</span><span class="fl">0.8</span>,  <span class="co"># sharper conditional next distribution</span></a>
<a class="sourceLine" id="cb24-222" title="222">                    top_k<span class="op">=</span><span class="dv">45</span>,  <span class="co"># k most likely next words</span></a>
<a class="sourceLine" id="cb24-223" title="223">                    top_p<span class="op">=</span><span class="fl">0.9</span>,  <span class="co"># smallest set of cum. prob exceed p for k</span></a>
<a class="sourceLine" id="cb24-224" title="224">                    max_length<span class="op">=</span><span class="dv">18</span>,  <span class="co"># slightly longer that word count</span></a>
<a class="sourceLine" id="cb24-225" title="225">                    min_length<span class="op">=</span><span class="dv">14</span>,  <span class="co"># dont eos before reaching</span></a>
<a class="sourceLine" id="cb24-226" title="226">                    num_beams<span class="op">=</span><span class="dv">1</span>,  <span class="co"># not using beam</span></a>
<a class="sourceLine" id="cb24-227" title="227">                    repetition_penalty<span class="op">=</span><span class="fl">2.5</span>,</a>
<a class="sourceLine" id="cb24-228" title="228">                    length_penalty<span class="op">=</span><span class="fl">2.5</span>,</a>
<a class="sourceLine" id="cb24-229" title="229">                    early_stopping<span class="op">=</span><span class="va">False</span>,  <span class="co"># for use with beam</span></a>
<a class="sourceLine" id="cb24-230" title="230">                    use_cache<span class="op">=</span><span class="va">True</span>,</a>
<a class="sourceLine" id="cb24-231" title="231">                    num_return_sequences<span class="op">=</span><span class="dv">1</span></a>
<a class="sourceLine" id="cb24-232" title="232">                    )</a>
<a class="sourceLine" id="cb24-233" title="233"></a>
<a class="sourceLine" id="cb24-234" title="234">            preds <span class="op">=</span> [tokenizer.decode(g, skip_special_tokens<span class="op">=</span><span class="va">True</span>, clean_up_tokenization_spaces<span class="op">=</span><span class="va">True</span>) <span class="cf">for</span> g <span class="kw">in</span> generated_ids]</a>
<a class="sourceLine" id="cb24-235" title="235">            target <span class="op">=</span> [tokenizer.decode(t, skip_special_tokens<span class="op">=</span><span class="va">True</span>, clean_up_tokenization_spaces<span class="op">=</span><span class="va">True</span>)<span class="cf">for</span> t <span class="kw">in</span> b_summary_ids]</a>
<a class="sourceLine" id="cb24-236" title="236">            predictions.extend(preds)</a>
<a class="sourceLine" id="cb24-237" title="237">            actuals.extend(target)</a>
<a class="sourceLine" id="cb24-238" title="238"></a>
<a class="sourceLine" id="cb24-239" title="239">    <span class="co"># calculate the average loss over all of the batches.</span></a>
<a class="sourceLine" id="cb24-240" title="240">    avg_test_loss <span class="op">=</span> total_test_loss <span class="op">/</span> <span class="bu">len</span>(dataloader)</a>
<a class="sourceLine" id="cb24-241" title="241"></a>
<a class="sourceLine" id="cb24-242" title="242">    <span class="co"># Record all statistics from this epoch.</span></a>
<a class="sourceLine" id="cb24-243" title="243">    test_stats.append(</a>
<a class="sourceLine" id="cb24-244" title="244">        {</a>
<a class="sourceLine" id="cb24-245" title="245">            <span class="st">&#39;Test Loss&#39;</span>: avg_test_loss,</a>
<a class="sourceLine" id="cb24-246" title="246">            <span class="st">&#39;Test PPL.&#39;</span>: np.exp(avg_test_loss),</a>
<a class="sourceLine" id="cb24-247" title="247">        }</a>
<a class="sourceLine" id="cb24-248" title="248">    )</a>
<a class="sourceLine" id="cb24-249" title="249">    <span class="kw">global</span> df2</a>
<a class="sourceLine" id="cb24-250" title="250">    temp_data <span class="op">=</span> pd.DataFrame({<span class="st">&#39;predicted&#39;</span>: predictions, <span class="st">&#39;actual&#39;</span>: actuals})</a>
<a class="sourceLine" id="cb24-251" title="251">    df2 <span class="op">=</span> df2.append(temp_data)</a>
<a class="sourceLine" id="cb24-252" title="252"></a>
<a class="sourceLine" id="cb24-253" title="253">    <span class="cf">return</span> test_stats</a>
<a class="sourceLine" id="cb24-254" title="254">    </a>
<a class="sourceLine" id="cb24-255" title="255"></a>
<a class="sourceLine" id="cb24-256" title="256"><span class="co"># time function</span></a>
<a class="sourceLine" id="cb24-257" title="257"><span class="kw">def</span> format_time(elapsed):</a>
<a class="sourceLine" id="cb24-258" title="258">    <span class="co">&#39;&#39;&#39;</span></a>
<a class="sourceLine" id="cb24-259" title="259"><span class="co">    Takes a time in seconds and returns a string hh:mm:ss</span></a>
<a class="sourceLine" id="cb24-260" title="260"><span class="co">    &#39;&#39;&#39;</span></a>
<a class="sourceLine" id="cb24-261" title="261">    <span class="co"># Round to the nearest second.</span></a>
<a class="sourceLine" id="cb24-262" title="262">    elapsed_rounded <span class="op">=</span> <span class="bu">int</span>(<span class="bu">round</span>((elapsed)))</a>
<a class="sourceLine" id="cb24-263" title="263">    <span class="co"># Format as hh:mm:ss</span></a>
<a class="sourceLine" id="cb24-264" title="264">    <span class="cf">return</span> <span class="bu">str</span>(datetime.timedelta(seconds<span class="op">=</span>elapsed_rounded))</a></code></pre></div>
<p>Before training, several prepatory objects are instantiated like the model, data loaders, and the optimizer.</p>
</div>
<div id="prepare-for-training" class="section level2">
<h2><span class="header-section-number">1.6</span> Prepare for Training</h2>
<div class="sourceCode" id="cb25"><pre class="sourceCode python"><code class="sourceCode python"><a class="sourceLine" id="cb25-1" title="1"><span class="co"># instantiate model T5 transformer with a language modeling head on top</span></a>
<a class="sourceLine" id="cb25-2" title="2">model <span class="op">=</span> T5ForConditionalGeneration.from_pretrained(<span class="st">&#39;t5-small&#39;</span>).cuda()  <span class="co"># to GPU</span></a>
<a class="sourceLine" id="cb25-3" title="3"></a>
<a class="sourceLine" id="cb25-4" title="4"></a>
<a class="sourceLine" id="cb25-5" title="5"><span class="co"># create the DataLoaders</span></a></code></pre></div>
<pre><code>## Some weights of T5ForConditionalGeneration were not initialized from the model checkpoint at t5-small and are newly initialized: [&#39;encoder.embed_tokens.weight&#39;, &#39;decoder.embed_tokens.weight&#39;, &#39;lm_head.weight&#39;]
## You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.</code></pre>
<div class="sourceCode" id="cb27"><pre class="sourceCode python"><code class="sourceCode python"><a class="sourceLine" id="cb27-1" title="1">train_dataloader <span class="op">=</span> DataLoader(dataset<span class="op">=</span>train_dataset,</a>
<a class="sourceLine" id="cb27-2" title="2">                              shuffle<span class="op">=</span><span class="va">True</span>,</a>
<a class="sourceLine" id="cb27-3" title="3">                              batch_size<span class="op">=</span><span class="dv">16</span></a>
<a class="sourceLine" id="cb27-4" title="4">                              )</a>
<a class="sourceLine" id="cb27-5" title="5"></a>
<a class="sourceLine" id="cb27-6" title="6">valid_dataloader <span class="op">=</span> DataLoader(dataset<span class="op">=</span>val_dataset,</a>
<a class="sourceLine" id="cb27-7" title="7">                              shuffle<span class="op">=</span><span class="va">False</span>,</a>
<a class="sourceLine" id="cb27-8" title="8">                              batch_size<span class="op">=</span><span class="dv">16</span></a>
<a class="sourceLine" id="cb27-9" title="9">                              )</a>
<a class="sourceLine" id="cb27-10" title="10"></a>
<a class="sourceLine" id="cb27-11" title="11">test_dataloader <span class="op">=</span> DataLoader(dataset<span class="op">=</span>test_dataset,</a>
<a class="sourceLine" id="cb27-12" title="12">                             shuffle<span class="op">=</span><span class="va">False</span>,</a>
<a class="sourceLine" id="cb27-13" title="13">                             batch_size<span class="op">=</span><span class="dv">16</span></a>
<a class="sourceLine" id="cb27-14" title="14">                             )</a>
<a class="sourceLine" id="cb27-15" title="15"></a>
<a class="sourceLine" id="cb27-16" title="16"></a>
<a class="sourceLine" id="cb27-17" title="17"><span class="co"># Adam w/ Weight Decay Fix</span></a>
<a class="sourceLine" id="cb27-18" title="18"><span class="co"># set to optimizer_grouped_parameters or model.parameters()</span></a>
<a class="sourceLine" id="cb27-19" title="19">optimizer <span class="op">=</span> AdamW(model.parameters(),</a>
<a class="sourceLine" id="cb27-20" title="20">                  lr <span class="op">=</span> <span class="fl">3e-5</span></a>
<a class="sourceLine" id="cb27-21" title="21">                  )</a>
<a class="sourceLine" id="cb27-22" title="22"></a>
<a class="sourceLine" id="cb27-23" title="23"><span class="co"># epochs</span></a>
<a class="sourceLine" id="cb27-24" title="24">epochs <span class="op">=</span> <span class="dv">6</span></a>
<a class="sourceLine" id="cb27-25" title="25"></a>
<a class="sourceLine" id="cb27-26" title="26"><span class="co"># lr scheduler</span></a>
<a class="sourceLine" id="cb27-27" title="27">total_steps <span class="op">=</span> <span class="bu">len</span>(train_dataloader) <span class="op">*</span> epochs</a>
<a class="sourceLine" id="cb27-28" title="28">scheduler <span class="op">=</span> get_linear_schedule_with_warmup(optimizer,</a>
<a class="sourceLine" id="cb27-29" title="29">                                            num_warmup_steps<span class="op">=</span><span class="dv">0</span>,</a>
<a class="sourceLine" id="cb27-30" title="30">                                            num_training_steps<span class="op">=</span>total_steps)</a>
<a class="sourceLine" id="cb27-31" title="31"></a>
<a class="sourceLine" id="cb27-32" title="32"><span class="co"># create gradient scaler for mixed precision</span></a>
<a class="sourceLine" id="cb27-33" title="33">scaler <span class="op">=</span> GradScaler()</a></code></pre></div>
</div>
<div id="train-and-validate" class="section level2">
<h2><span class="header-section-number">1.7</span> Train and Validate</h2>
<p>Finally we are ready to train. Two containers are created to store the results of each training and validation epoch</p>
<div class="sourceCode" id="cb28"><pre class="sourceCode python"><code class="sourceCode python"><a class="sourceLine" id="cb28-1" title="1"><span class="co"># create training result storage</span></a>
<a class="sourceLine" id="cb28-2" title="2">training_stats <span class="op">=</span> []</a>
<a class="sourceLine" id="cb28-3" title="3">valid_stats <span class="op">=</span> []</a>
<a class="sourceLine" id="cb28-4" title="4">best_valid_loss <span class="op">=</span> <span class="bu">float</span>(<span class="st">&#39;inf&#39;</span>)</a>
<a class="sourceLine" id="cb28-5" title="5"></a>
<a class="sourceLine" id="cb28-6" title="6"><span class="co"># for each epoch</span></a>
<a class="sourceLine" id="cb28-7" title="7"><span class="cf">for</span> epoch <span class="kw">in</span> <span class="bu">range</span>(epochs):</a>
<a class="sourceLine" id="cb28-8" title="8">    <span class="co"># train</span></a>
<a class="sourceLine" id="cb28-9" title="9">    train(model, train_dataloader, optimizer)</a>
<a class="sourceLine" id="cb28-10" title="10">    <span class="co"># validate</span></a>
<a class="sourceLine" id="cb28-11" title="11">    validating(model, valid_dataloader)</a>
<a class="sourceLine" id="cb28-12" title="12">    <span class="co"># check validation loss</span></a>
<a class="sourceLine" id="cb28-13" title="13">    <span class="cf">if</span> valid_stats[epoch][<span class="st">&#39;Val Loss&#39;</span>] <span class="op">&lt;</span> best_valid_loss:</a>
<a class="sourceLine" id="cb28-14" title="14">        best_valid_loss <span class="op">=</span> valid_stats[epoch][<span class="st">&#39;Val Loss&#39;</span>]</a>
<a class="sourceLine" id="cb28-15" title="15">        <span class="co"># save best model for use later</span></a>
<a class="sourceLine" id="cb28-16" title="16">        torch.save(model.state_dict(), <span class="st">&#39;t5-summary.pt&#39;</span>)  <span class="co"># torch save</span></a>
<a class="sourceLine" id="cb28-17" title="17">        model_to_save <span class="op">=</span> model.module <span class="cf">if</span> <span class="bu">hasattr</span>(model, <span class="st">&#39;module&#39;</span>) <span class="cf">else</span> model</a>
<a class="sourceLine" id="cb28-18" title="18">        model_to_save.save_pretrained(<span class="st">&#39;./model_save/t5/&#39;</span>)  <span class="co"># transformers save</span></a>
<a class="sourceLine" id="cb28-19" title="19">        tokenizer.save_pretrained(<span class="st">&#39;./model_save/t5/&#39;</span>)  <span class="co"># transformers save</span></a></code></pre></div>
<pre><code>## 
## ======== Epoch 1 / 6 ========
## Training...
##   Batch    40  of    503.
##   Batch    80  of    503.
##   Batch   120  of    503.
##   Batch   160  of    503.
##   Batch   200  of    503.
##   Batch   240  of    503.
##   Batch   280  of    503.
##   Batch   320  of    503.
##   Batch   360  of    503.
##   Batch   400  of    503.
##   Batch   440  of    503.
##   Batch   480  of    503.
## 
## summary results
## epoch | trn loss | trn time 
##     1 | 1.59530 | 0:01:52
## [{&#39;Train Loss&#39;: 1.5952966689589483}]
## 
## Running Validation...
## 
## summary results
## epoch | val loss | val ppl | val time
##     1 | 0.73627 | 2.088 | 0:00:06
## [{&#39;Val Loss&#39;: 0.7362715734375848, &#39;Val PPL.&#39;: 2.0881355227329945}]
## (&#39;./model_save/t5/spiece.model&#39;, &#39;./model_save/t5/special_tokens_map.json&#39;, &#39;./model_save/t5/added_tokens.json&#39;)
## 
## ======== Epoch 2 / 6 ========
## Training...
##   Batch    40  of    503.
##   Batch    80  of    503.
##   Batch   120  of    503.
##   Batch   160  of    503.
##   Batch   200  of    503.
##   Batch   240  of    503.
##   Batch   280  of    503.
##   Batch   320  of    503.
##   Batch   360  of    503.
##   Batch   400  of    503.
##   Batch   440  of    503.
##   Batch   480  of    503.
## 
## summary results
## epoch | trn loss | trn time 
##     2 | 0.82822 | 0:01:53
## [{&#39;Train Loss&#39;: 1.5952966689589483}, {&#39;Train Loss&#39;: 0.828221511651223}]
## 
## Running Validation...
## 
## summary results
## epoch | val loss | val ppl | val time
##     2 | 0.65254 | 1.920 | 0:00:06
## [{&#39;Val Loss&#39;: 0.7362715734375848, &#39;Val PPL.&#39;: 2.0881355227329945}, {&#39;Val Loss&#39;: 0.6525381051358723, &#39;Val PPL.&#39;: 1.9204088481889185}]
## (&#39;./model_save/t5/spiece.model&#39;, &#39;./model_save/t5/special_tokens_map.json&#39;, &#39;./model_save/t5/added_tokens.json&#39;)
## 
## ======== Epoch 3 / 6 ========
## Training...
##   Batch    40  of    503.
##   Batch    80  of    503.
##   Batch   120  of    503.
##   Batch   160  of    503.
##   Batch   200  of    503.
##   Batch   240  of    503.
##   Batch   280  of    503.
##   Batch   320  of    503.
##   Batch   360  of    503.
##   Batch   400  of    503.
##   Batch   440  of    503.
##   Batch   480  of    503.
## 
## summary results
## epoch | trn loss | trn time 
##     3 | 0.75328 | 0:01:53
## [{&#39;Train Loss&#39;: 1.5952966689589483}, {&#39;Train Loss&#39;: 0.828221511651223}, {&#39;Train Loss&#39;: 0.7532829459808456}]
## 
## Running Validation...
## 
## summary results
## epoch | val loss | val ppl | val time
##     3 | 0.61503 | 1.850 | 0:00:06
## [{&#39;Val Loss&#39;: 0.7362715734375848, &#39;Val PPL.&#39;: 2.0881355227329945}, {&#39;Val Loss&#39;: 0.6525381051358723, &#39;Val PPL.&#39;: 1.9204088481889185}, {&#39;Val Loss&#39;: 0.6150305460369776, &#39;Val PPL.&#39;: 1.8497131001001426}]
## (&#39;./model_save/t5/spiece.model&#39;, &#39;./model_save/t5/special_tokens_map.json&#39;, &#39;./model_save/t5/added_tokens.json&#39;)
## 
## ======== Epoch 4 / 6 ========
## Training...
##   Batch    40  of    503.
##   Batch    80  of    503.
##   Batch   120  of    503.
##   Batch   160  of    503.
##   Batch   200  of    503.
##   Batch   240  of    503.
##   Batch   280  of    503.
##   Batch   320  of    503.
##   Batch   360  of    503.
##   Batch   400  of    503.
##   Batch   440  of    503.
##   Batch   480  of    503.
## 
## summary results
## epoch | trn loss | trn time 
##     4 | 0.71392 | 0:01:52
## [{&#39;Train Loss&#39;: 1.5952966689589483}, {&#39;Train Loss&#39;: 0.828221511651223}, {&#39;Train Loss&#39;: 0.7532829459808456}, {&#39;Train Loss&#39;: 0.713922281388497}]
## 
## Running Validation...
## 
## summary results
## epoch | val loss | val ppl | val time
##     4 | 0.59374 | 1.811 | 0:00:06
## [{&#39;Val Loss&#39;: 0.7362715734375848, &#39;Val PPL.&#39;: 2.0881355227329945}, {&#39;Val Loss&#39;: 0.6525381051358723, &#39;Val PPL.&#39;: 1.9204088481889185}, {&#39;Val Loss&#39;: 0.6150305460369776, &#39;Val PPL.&#39;: 1.8497131001001426}, {&#39;Val Loss&#39;: 0.5937370728878748, &#39;Val PPL.&#39;: 1.8107426642946385}]
## (&#39;./model_save/t5/spiece.model&#39;, &#39;./model_save/t5/special_tokens_map.json&#39;, &#39;./model_save/t5/added_tokens.json&#39;)
## 
## ======== Epoch 5 / 6 ========
## Training...
##   Batch    40  of    503.
##   Batch    80  of    503.
##   Batch   120  of    503.
##   Batch   160  of    503.
##   Batch   200  of    503.
##   Batch   240  of    503.
##   Batch   280  of    503.
##   Batch   320  of    503.
##   Batch   360  of    503.
##   Batch   400  of    503.
##   Batch   440  of    503.
##   Batch   480  of    503.
## 
## summary results
## epoch | trn loss | trn time 
##     5 | 0.69332 | 0:01:53
## [{&#39;Train Loss&#39;: 1.5952966689589483}, {&#39;Train Loss&#39;: 0.828221511651223}, {&#39;Train Loss&#39;: 0.7532829459808456}, {&#39;Train Loss&#39;: 0.713922281388497}, {&#39;Train Loss&#39;: 0.6933188264815519}]
## 
## Running Validation...
## 
## summary results
## epoch | val loss | val ppl | val time
##     5 | 0.58224 | 1.790 | 0:00:06
## [{&#39;Val Loss&#39;: 0.7362715734375848, &#39;Val PPL.&#39;: 2.0881355227329945}, {&#39;Val Loss&#39;: 0.6525381051358723, &#39;Val PPL.&#39;: 1.9204088481889185}, {&#39;Val Loss&#39;: 0.6150305460369776, &#39;Val PPL.&#39;: 1.8497131001001426}, {&#39;Val Loss&#39;: 0.5937370728878748, &#39;Val PPL.&#39;: 1.8107426642946385}, {&#39;Val Loss&#39;: 0.5822378590939536, &#39;Val PPL.&#39;: 1.790039808684565}]
## (&#39;./model_save/t5/spiece.model&#39;, &#39;./model_save/t5/special_tokens_map.json&#39;, &#39;./model_save/t5/added_tokens.json&#39;)
## 
## ======== Epoch 6 / 6 ========
## Training...
##   Batch    40  of    503.
##   Batch    80  of    503.
##   Batch   120  of    503.
##   Batch   160  of    503.
##   Batch   200  of    503.
##   Batch   240  of    503.
##   Batch   280  of    503.
##   Batch   320  of    503.
##   Batch   360  of    503.
##   Batch   400  of    503.
##   Batch   440  of    503.
##   Batch   480  of    503.
## 
## summary results
## epoch | trn loss | trn time 
##     6 | 0.68405 | 0:01:51
## [{&#39;Train Loss&#39;: 1.5952966689589483}, {&#39;Train Loss&#39;: 0.828221511651223}, {&#39;Train Loss&#39;: 0.7532829459808456}, {&#39;Train Loss&#39;: 0.713922281388497}, {&#39;Train Loss&#39;: 0.6933188264815519}, {&#39;Train Loss&#39;: 0.6840471257390843}]
## 
## Running Validation...
## 
## summary results
## epoch | val loss | val ppl | val time
##     6 | 0.57983 | 1.786 | 0:00:06
## [{&#39;Val Loss&#39;: 0.7362715734375848, &#39;Val PPL.&#39;: 2.0881355227329945}, {&#39;Val Loss&#39;: 0.6525381051358723, &#39;Val PPL.&#39;: 1.9204088481889185}, {&#39;Val Loss&#39;: 0.6150305460369776, &#39;Val PPL.&#39;: 1.8497131001001426}, {&#39;Val Loss&#39;: 0.5937370728878748, &#39;Val PPL.&#39;: 1.8107426642946385}, {&#39;Val Loss&#39;: 0.5822378590939536, &#39;Val PPL.&#39;: 1.790039808684565}, {&#39;Val Loss&#39;: 0.5798299776183234, &#39;Val PPL.&#39;: 1.7857347900558997}]
## (&#39;./model_save/t5/spiece.model&#39;, &#39;./model_save/t5/special_tokens_map.json&#39;, &#39;./model_save/t5/added_tokens.json&#39;)
## 
## C:\Users\Andrew\Anaconda3\envs\my_ml\lib\site-packages\torch\optim\lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
##   &quot;https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate&quot;, UserWarning)</code></pre>
<p>Now we can present our training results nicely in a data frame.</p>
<div class="sourceCode" id="cb30"><pre class="sourceCode python"><code class="sourceCode python"><a class="sourceLine" id="cb30-1" title="1"><span class="co"># organize results</span></a>
<a class="sourceLine" id="cb30-2" title="2">pd.set_option(<span class="st">&#39;precision&#39;</span>, <span class="dv">3</span>)</a>
<a class="sourceLine" id="cb30-3" title="3">df_train_stats <span class="op">=</span> pd.DataFrame(data<span class="op">=</span>training_stats)</a>
<a class="sourceLine" id="cb30-4" title="4">df_valid_stats <span class="op">=</span> pd.DataFrame(data<span class="op">=</span>valid_stats)</a>
<a class="sourceLine" id="cb30-5" title="5">df_stats <span class="op">=</span> pd.concat([df_train_stats, df_valid_stats], axis<span class="op">=</span><span class="dv">1</span>)</a>
<a class="sourceLine" id="cb30-6" title="6">df_stats.insert(<span class="dv">0</span>, <span class="st">&#39;Epoch&#39;</span>, <span class="bu">range</span>(<span class="dv">1</span>, <span class="bu">len</span>(df_stats)<span class="op">+</span><span class="dv">1</span>))</a>
<a class="sourceLine" id="cb30-7" title="7">df_stats <span class="op">=</span> df_stats.set_index(<span class="st">&#39;Epoch&#39;</span>)</a>
<a class="sourceLine" id="cb30-8" title="8">df_stats</a></code></pre></div>
<pre><code>##        Train Loss  Val Loss  Val PPL.
## Epoch                                
## 1           1.595     0.736     2.088
## 2           0.828     0.653     1.920
## 3           0.753     0.615     1.850
## 4           0.714     0.594     1.811
## 5           0.693     0.582     1.790
## 6           0.684     0.580     1.786</code></pre>
</div>
<div id="test-and-generate-summaries" class="section level2">
<h2><span class="header-section-number">1.8</span> Test and Generate Summaries</h2>
<p>While also checking our loss in our held-out test data, we also generate predicted summaries to compare with our actual summaries so we can evaluate it by ROGUE metrics.</p>
<div class="sourceCode" id="cb32"><pre class="sourceCode python"><code class="sourceCode python"><a class="sourceLine" id="cb32-1" title="1"><span class="co"># test the model</span></a>
<a class="sourceLine" id="cb32-2" title="2">df2 <span class="op">=</span> pd.DataFrame({<span class="st">&#39;predicted&#39;</span>: [], <span class="st">&#39;actual&#39;</span>: []})</a>
<a class="sourceLine" id="cb32-3" title="3">test_stats <span class="op">=</span> []</a>
<a class="sourceLine" id="cb32-4" title="4">model.load_state_dict(torch.load(<span class="st">&#39;t5-summary.pt&#39;</span>))</a></code></pre></div>
<pre><code>## &lt;All keys matched successfully&gt;</code></pre>
<div class="sourceCode" id="cb34"><pre class="sourceCode python"><code class="sourceCode python"><a class="sourceLine" id="cb34-1" title="1">testing(model, test_dataloader)</a></code></pre></div>
<pre><code>## 
## Running Testing...
##   Batch    40  of     63.    Elapsed: 0:00:56.
## [{&#39;Test Loss&#39;: 0.6018462706179846, &#39;Test PPL.&#39;: 1.8254860322482742}]</code></pre>
<div class="sourceCode" id="cb36"><pre class="sourceCode python"><code class="sourceCode python"><a class="sourceLine" id="cb36-1" title="1"><span class="bu">print</span>(df2.head(<span class="dv">5</span>))</a></code></pre></div>
<pre><code>##                                            predicted                                             actual
## 0  afghan taliban commentary says us never indica...  afghan taliban commentary no other options but...
## 1  taliban say six soldiers killed post captured ...  taliban say posts captured six soldiers killed...
## 2  taliban say two soldiers killed in blasts in a...  taliban report two army soldiers killed in bla...
## 3  taliban say four soldiers killed in attack in ...  taliban say four afghan soldiers killed in wes...
## 4  taliban say two soldiers killed three injured ...  taliban say two soldiers killed three injured ...</code></pre>
</div>
<div id="rouge-metrics" class="section level2">
<h2><span class="header-section-number">1.9</span> ROUGE Metrics</h2>
<p>Evaluating summarization tasks are hard to measure and tune for as there are many criteria:</p>
<ol style="list-style-type: decimal">
<li>Information satisfaction (answer query)</li>
<li>Coverage (summarize corpus)</li>
<li>Fluency (sounds natural)</li>
<li>Concision (not redundant, no fluff)</li>
</ol>
<p>In <em>BLEU</em>, we are about precision: how many n-grams in the candidate are in the reference translation? In <em>ROGUE</em>, we care about recall: how many n-grams in the reference summaries are in the candidate?</p>
<div class="sourceCode" id="cb38"><pre class="sourceCode python"><code class="sourceCode python"><a class="sourceLine" id="cb38-1" title="1"><span class="co"># ROUGE</span></a>
<a class="sourceLine" id="cb38-2" title="2">nlp_rouge <span class="op">=</span> nlp.load_metric(<span class="st">&#39;rouge&#39;</span>)</a>
<a class="sourceLine" id="cb38-3" title="3"></a>
<a class="sourceLine" id="cb38-4" title="4">scores <span class="op">=</span> nlp_rouge.compute(</a>
<a class="sourceLine" id="cb38-5" title="5">    df2.predicted.to_list(), df2.actual.to_list(),</a>
<a class="sourceLine" id="cb38-6" title="6">    rouge_types<span class="op">=</span>[<span class="st">&#39;rouge1&#39;</span>, <span class="st">&#39;rouge2&#39;</span>, <span class="st">&#39;rougeL&#39;</span>, <span class="st">&#39;rougeLsum&#39;</span>],</a>
<a class="sourceLine" id="cb38-7" title="7">    use_agregator<span class="op">=</span><span class="va">True</span>, use_stemmer<span class="op">=</span><span class="va">False</span></a>
<a class="sourceLine" id="cb38-8" title="8">)</a>
<a class="sourceLine" id="cb38-9" title="9"></a>
<a class="sourceLine" id="cb38-10" title="10">df <span class="op">=</span> pd.DataFrame({</a>
<a class="sourceLine" id="cb38-11" title="11">    <span class="st">&#39;rouge1&#39;</span>: [scores[<span class="st">&#39;rouge1&#39;</span>].mid.precision, scores[<span class="st">&#39;rouge1&#39;</span>].mid.recall, scores[<span class="st">&#39;rouge1&#39;</span>].mid.fmeasure],</a>
<a class="sourceLine" id="cb38-12" title="12">    <span class="st">&#39;rouge2&#39;</span>: [scores[<span class="st">&#39;rouge2&#39;</span>].mid.precision, scores[<span class="st">&#39;rouge2&#39;</span>].mid.recall, scores[<span class="st">&#39;rouge2&#39;</span>].mid.fmeasure],</a>
<a class="sourceLine" id="cb38-13" title="13">    <span class="st">&#39;rougeL&#39;</span>: [scores[<span class="st">&#39;rougeL&#39;</span>].mid.precision, scores[<span class="st">&#39;rougeL&#39;</span>].mid.recall, scores[<span class="st">&#39;rougeL&#39;</span>].mid.fmeasure]}, index<span class="op">=</span>[ <span class="st">&#39;P&#39;</span>, <span class="st">&#39;R&#39;</span>, <span class="st">&#39;F&#39;</span>])</a>
<a class="sourceLine" id="cb38-14" title="14"></a>
<a class="sourceLine" id="cb38-15" title="15">df.style.<span class="bu">format</span>({<span class="st">&#39;rouge1&#39;</span>: <span class="st">&quot;</span><span class="sc">{:.4f}</span><span class="st">&quot;</span>, <span class="st">&#39;rouge2&#39;</span>: <span class="st">&quot;</span><span class="sc">{:.4f}</span><span class="st">&quot;</span>, <span class="st">&#39;rougeL&#39;</span>: <span class="st">&quot;</span><span class="sc">{:.4f}</span><span class="st">&quot;</span>})</a></code></pre></div>
<pre><code>## &lt;pandas.io.formats.style.Styler object at 0x0000000009DC7B48&gt;</code></pre>
<p>We interpret the metrics below as follows:</p>
<ul>
<li>ROUGE Recall: x% means that x% of the n-grams in the reference summary are also present in the generated summary.</li>
<li>ROUGE Precision: x% means that x% of the n-grams in the generated summary are also present in the reference summary.</li>
<li>ROUGE F1: like any F1-score.</li>
</ul>
<div class="sourceCode" id="cb40"><pre class="sourceCode python"><code class="sourceCode python"><a class="sourceLine" id="cb40-1" title="1"><span class="co"># show df</span></a>
<a class="sourceLine" id="cb40-2" title="2"><span class="bu">print</span>(df)</a></code></pre></div>
<pre><code>##    rouge1  rouge2  rougeL
## P   0.532   0.309   0.507
## R   0.492   0.289   0.469
## F   0.506   0.296   0.483</code></pre>
</div>
</div>
<div id="sources" class="section level1">
<h1><span class="header-section-number">2</span> Sources</h1>
<ul>
<li>Raffel, Colin, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. “Exploring the limits of transfer learning with a unified text-to-text transformer.” arXiv preprint arXiv:1910.10683 (2019).</li>
</ul>
</div>



<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>
