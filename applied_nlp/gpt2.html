<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml">

<head>

<meta charset="utf-8" />
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="generator" content="pandoc" />

<meta name="viewport" content="width=device-width, initial-scale=1">

<meta name="author" content="Andrew Fogarty" />


<title>Text Generation: (Distil)GPT-2</title>



<style type="text/css">code{white-space: pre;}</style>
<style type="text/css" data-origin="pandoc">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */

</style>
<script>
// apply pandoc div.sourceCode style to pre.sourceCode instead
(function() {
  var sheets = document.styleSheets;
  for (var i = 0; i < sheets.length; i++) {
    if (sheets[i].ownerNode.dataset["origin"] !== "pandoc") continue;
    try { var rules = sheets[i].cssRules; } catch (e) { continue; }
    for (var j = 0; j < rules.length; j++) {
      var rule = rules[j];
      // check if there is a div.sourceCode rule
      if (rule.type !== rule.STYLE_RULE || rule.selectorText !== "div.sourceCode") continue;
      var style = rule.style.cssText;
      // check if color or background-color is set
      if (rule.style.color === '' || rule.style.backgroundColor === '') continue;
      // replace div.sourceCode by a pre.sourceCode rule
      sheets[i].deleteRule(j);
      sheets[i].insertRule('pre.sourceCode{' + style + '}', j);
    }
  }
})();
</script>



<style type="text/css">body {
background-color: #fff;
margin: 1em auto;
max-width: 700px;
overflow: visible;
padding-left: 2em;
padding-right: 2em;
font-family: "Open Sans", "Helvetica Neue", Helvetica, Arial, sans-serif;
font-size: 14px;
line-height: 1.35;
}
#header {
text-align: center;
}
#TOC {
clear: both;
margin: 0 0 10px 10px;
padding: 4px;
width: 400px;
border: 1px solid #CCCCCC;
border-radius: 5px;
background-color: #f6f6f6;
font-size: 13px;
line-height: 1.3;
}
#TOC .toctitle {
font-weight: bold;
font-size: 15px;
margin-left: 5px;
}
#TOC ul {
padding-left: 40px;
margin-left: -1.5em;
margin-top: 5px;
margin-bottom: 5px;
}
#TOC ul ul {
margin-left: -2em;
}
#TOC li {
line-height: 16px;
}
table {
margin: 1em auto;
border-width: 1px;
border-color: #DDDDDD;
border-style: outset;
border-collapse: collapse;
}
table th {
border-width: 2px;
padding: 5px;
border-style: inset;
}
table td {
border-width: 1px;
border-style: inset;
line-height: 18px;
padding: 5px 5px;
}
table, table th, table td {
border-left-style: none;
border-right-style: none;
}
table thead, table tr.even {
background-color: #f7f7f7;
}
p {
margin: 0.5em 0;
}
blockquote {
background-color: #f6f6f6;
padding: 0.25em 0.75em;
}
hr {
border-style: solid;
border: none;
border-top: 1px solid #777;
margin: 28px 0;
}
dl {
margin-left: 0;
}
dl dd {
margin-bottom: 13px;
margin-left: 13px;
}
dl dt {
font-weight: bold;
}
ul {
margin-top: 0;
}
ul li {
list-style: circle outside;
}
ul ul {
margin-bottom: 0;
}
pre, code {
background-color: #f7f7f7;
border-radius: 3px;
color: #333;
white-space: pre-wrap; 
}
pre {
border-radius: 3px;
margin: 5px 0px 10px 0px;
padding: 10px;
}
pre:not([class]) {
background-color: #f7f7f7;
}
code {
font-family: Consolas, Monaco, 'Courier New', monospace;
font-size: 85%;
}
p > code, li > code {
padding: 2px 0px;
}
div.figure {
text-align: center;
}
img {
background-color: #FFFFFF;
padding: 2px;
border: 1px solid #DDDDDD;
border-radius: 3px;
border: 1px solid #CCCCCC;
margin: 0 5px;
}
h1 {
margin-top: 0;
font-size: 35px;
line-height: 40px;
}
h2 {
border-bottom: 4px solid #f7f7f7;
padding-top: 10px;
padding-bottom: 2px;
font-size: 145%;
}
h3 {
border-bottom: 2px solid #f7f7f7;
padding-top: 10px;
font-size: 120%;
}
h4 {
border-bottom: 1px solid #f7f7f7;
margin-left: 8px;
font-size: 105%;
}
h5, h6 {
border-bottom: 1px solid #ccc;
font-size: 105%;
}
a {
color: #0033dd;
text-decoration: none;
}
a:hover {
color: #6666ff; }
a:visited {
color: #800080; }
a:visited:hover {
color: #BB00BB; }
a[href^="http:"] {
text-decoration: underline; }
a[href^="https:"] {
text-decoration: underline; }

code > span.kw { color: #555; font-weight: bold; } 
code > span.dt { color: #902000; } 
code > span.dv { color: #40a070; } 
code > span.bn { color: #d14; } 
code > span.fl { color: #d14; } 
code > span.ch { color: #d14; } 
code > span.st { color: #d14; } 
code > span.co { color: #888888; font-style: italic; } 
code > span.ot { color: #007020; } 
code > span.al { color: #ff0000; font-weight: bold; } 
code > span.fu { color: #900; font-weight: bold; }  code > span.er { color: #a61717; background-color: #e3d2d2; } 
</style>




</head>

<body>




<h1 class="title toc-ignore">Text Generation: (Distil)GPT-2</h1>
<h4 class="author">Andrew Fogarty</h4>
<h4 class="date">7/16/2020</h4>


<div id="TOC">
<ul>
<li><a href="#introduction"><span class="toc-section-number">1</span> Introduction</a></li>
<li><a href="#preparing-the-data"><span class="toc-section-number">2</span> Preparing the Data</a>
<ul>
<li><a href="#tokenizing"><span class="toc-section-number">2.1</span> Tokenizing</a></li>
<li><a href="#instantiating-contiguous-data"><span class="toc-section-number">2.2</span> Instantiating Contiguous Data</a></li>
<li><a href="#instantiating-the-model"><span class="toc-section-number">2.3</span> Instantiating the Model</a></li>
<li><a href="#data-loaders"><span class="toc-section-number">2.4</span> Data Loaders</a></li>
<li><a href="#training-and-helper-functions"><span class="toc-section-number">2.5</span> Training and Helper Functions</a></li>
<li><a href="#training-preparation"><span class="toc-section-number">2.6</span> Training Preparation</a></li>
<li><a href="#train"><span class="toc-section-number">2.7</span> Train</a></li>
<li><a href="#beam-search-text-generation"><span class="toc-section-number">2.8</span> Beam Search: Text Generation</a></li>
</ul></li>
<li><a href="#sources"><span class="toc-section-number">3</span> Sources</a></li>
</ul>
</div>

<div class="sourceCode" id="cb1"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1-1"><a href="#cb1-1"></a><span class="co"># load python</span></span>
<span id="cb1-2"><a href="#cb1-2"></a><span class="kw">library</span>(reticulate)</span>
<span id="cb1-3"><a href="#cb1-3"></a><span class="kw">use_condaenv</span>(<span class="st">&quot;my_ml&quot;</span>)</span></code></pre></div>
<div class="sourceCode" id="cb2"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1"></a><span class="co"># load packages</span></span>
<span id="cb2-2"><a href="#cb2-2"></a><span class="im">import</span> torch</span>
<span id="cb2-3"><a href="#cb2-3"></a><span class="im">import</span> torch.nn <span class="im">as</span> nn</span>
<span id="cb2-4"><a href="#cb2-4"></a><span class="im">import</span> torch.optim <span class="im">as</span> optim</span>
<span id="cb2-5"><a href="#cb2-5"></a><span class="im">import</span> torch.utils.data <span class="im">as</span> datautils</span>
<span id="cb2-6"><a href="#cb2-6"></a><span class="im">from</span> torch.utils.data <span class="im">import</span> DataLoader, RandomSampler, SequentialSampler, TensorDataset, random_split</span>
<span id="cb2-7"><a href="#cb2-7"></a><span class="im">from</span> transformers <span class="im">import</span> AutoConfig, AutoModelForSequenceClassification, AutoTokenizer</span>
<span id="cb2-8"><a href="#cb2-8"></a><span class="im">from</span> transformers <span class="im">import</span> GPT2Tokenizer, GPT2LMHeadModel, TextDataset, LineByLineTextDataset</span>
<span id="cb2-9"><a href="#cb2-9"></a><span class="im">from</span> transformers <span class="im">import</span> get_linear_schedule_with_warmup, AdamW</span>
<span id="cb2-10"><a href="#cb2-10"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb2-11"><a href="#cb2-11"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb2-12"><a href="#cb2-12"></a><span class="im">from</span> tqdm <span class="im">import</span> tqdm</span>
<span id="cb2-13"><a href="#cb2-13"></a><span class="im">from</span> sklearn.metrics <span class="im">import</span> roc_auc_score</span>
<span id="cb2-14"><a href="#cb2-14"></a><span class="im">import</span> time, os, datetime, random, re</span>
<span id="cb2-15"><a href="#cb2-15"></a><span class="im">from</span> torch.cuda.amp <span class="im">import</span> autocast, GradScaler</span>
<span id="cb2-16"><a href="#cb2-16"></a><span class="im">from</span> sklearn.model_selection <span class="im">import</span> train_test_split</span>
<span id="cb2-17"><a href="#cb2-17"></a></span>
<span id="cb2-18"><a href="#cb2-18"></a>SEED <span class="op">=</span> <span class="dv">15</span></span>
<span id="cb2-19"><a href="#cb2-19"></a>random.seed(SEED)</span>
<span id="cb2-20"><a href="#cb2-20"></a>np.random.seed(SEED)</span>
<span id="cb2-21"><a href="#cb2-21"></a>torch.manual_seed(SEED)</span></code></pre></div>
<pre><code>## &lt;torch._C.Generator object at 0x000000001FA5B090&gt;</code></pre>
<div class="sourceCode" id="cb4"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1"></a>torch.backends.cudnn.deterministic <span class="op">=</span> <span class="va">True</span></span>
<span id="cb4-2"><a href="#cb4-2"></a></span>
<span id="cb4-3"><a href="#cb4-3"></a><span class="co"># tell pytorch to use cuda</span></span>
<span id="cb4-4"><a href="#cb4-4"></a>device <span class="op">=</span> torch.device(<span class="st">&quot;cuda&quot;</span>)</span></code></pre></div>
<div id="introduction" class="section level1" number="1">
<h1 number="1"><span class="header-section-number">1</span> Introduction</h1>
<p>Language models are trained to predict the probability the next “token” considering the preceding tokens that came before it. A token can be a word, a letter, or a subcomponent of a word. When generating text with language models, we often provide a starting sequence, like “The Taliban launched an attack”, and then the language model then outputs probabilities for what token comes next, drawing on all possible outcomes from the vocabulary.</p>
<p>In this guide, we use the a decoder-only language model transformer to predict text from our novel insurgent propaganda corpus. Generative Pre-Training (GPT-2) that takes input of up to 1024 byte pair tokens, is comprised of 12 decoder layers and 12 attention heads, and outputs a 768 dimensional vector (for the <em>small</em> version). GPT-2 is trained with Book Corpus via long contiguous runs of text.</p>
</div>
<div id="preparing-the-data" class="section level1" number="2">
<h1 number="2"><span class="header-section-number">2</span> Preparing the Data</h1>
<p>Unlike the other guides, we need to process our data a little bit differently so as to create long contiguous inputs. The code below does minor clean up on the data set and creates a train and validation set that are then saved as text files.</p>
<div class="sourceCode" id="cb5"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1"></a><span class="co"># prepare and load data</span></span>
<span id="cb5-2"><a href="#cb5-2"></a><span class="kw">def</span> prepare_df(pkl_location):</span>
<span id="cb5-3"><a href="#cb5-3"></a>    <span class="co"># read pkl as pandas</span></span>
<span id="cb5-4"><a href="#cb5-4"></a>    df <span class="op">=</span> pd.read_pickle(pkl_location)</span>
<span id="cb5-5"><a href="#cb5-5"></a>    <span class="co"># remove excess white spaces</span></span>
<span id="cb5-6"><a href="#cb5-6"></a>    df[<span class="st">&#39;body&#39;</span>] <span class="op">=</span> df[<span class="st">&#39;body&#39;</span>].<span class="bu">apply</span>(<span class="kw">lambda</span> x: <span class="st">&quot; &quot;</span>.join(x.split()))</span>
<span id="cb5-7"><a href="#cb5-7"></a>    <span class="co"># remove excess spaces near punctuation</span></span>
<span id="cb5-8"><a href="#cb5-8"></a>    df[<span class="st">&#39;body&#39;</span>] <span class="op">=</span> df[<span class="st">&#39;body&#39;</span>].<span class="bu">apply</span>(<span class="kw">lambda</span> x: re.sub(<span class="vs">r&#39;\s([?.!&quot;](?:\s|$))&#39;</span>, <span class="vs">r&#39;\1&#39;</span>, x))</span>
<span id="cb5-9"><a href="#cb5-9"></a>    <span class="co"># split and shuffle data</span></span>
<span id="cb5-10"><a href="#cb5-10"></a>    train, valid <span class="op">=</span> train_test_split(df[<span class="st">&#39;body&#39;</span>], test_size<span class="op">=</span><span class="fl">0.2</span>)</span>
<span id="cb5-11"><a href="#cb5-11"></a>    <span class="cf">return</span> train.reset_index(drop<span class="op">=</span><span class="va">True</span>), valid.reset_index(drop<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb5-12"><a href="#cb5-12"></a></span>
<span id="cb5-13"><a href="#cb5-13"></a></span>
<span id="cb5-14"><a href="#cb5-14"></a><span class="co"># instantiate shuffled train and validation</span></span>
<span id="cb5-15"><a href="#cb5-15"></a>train, valid <span class="op">=</span> prepare_df(<span class="st">&#39;C:</span><span class="ch">\\</span><span class="st">Users</span><span class="ch">\\</span><span class="st">Andrew</span><span class="ch">\\</span><span class="st">Desktop</span><span class="ch">\\</span><span class="st">df.pkl&#39;</span>)</span>
<span id="cb5-16"><a href="#cb5-16"></a></span>
<span id="cb5-17"><a href="#cb5-17"></a><span class="co"># save to text for transformers TextDataset</span></span>
<span id="cb5-18"><a href="#cb5-18"></a>np.savetxt(<span class="st">&#39;C:</span><span class="ch">\\</span><span class="st">Users</span><span class="ch">\\</span><span class="st">Andrew</span><span class="ch">\\</span><span class="st">Desktop</span><span class="ch">\\</span><span class="st">train.txt&#39;</span>, train, fmt<span class="op">=</span><span class="st">&quot;</span><span class="sc">%s</span><span class="st">&quot;</span>)</span>
<span id="cb5-19"><a href="#cb5-19"></a>np.savetxt(<span class="st">&#39;C:</span><span class="ch">\\</span><span class="st">Users</span><span class="ch">\\</span><span class="st">Andrew</span><span class="ch">\\</span><span class="st">Desktop</span><span class="ch">\\</span><span class="st">valid.txt&#39;</span>, valid, fmt<span class="op">=</span><span class="st">&quot;</span><span class="sc">%s</span><span class="st">&quot;</span>)</span></code></pre></div>
<div id="tokenizing" class="section level2" number="2.1">
<h2 number="2.1"><span class="header-section-number">2.1</span> Tokenizing</h2>
<p>Next, we instantiate the GPT-2 tokenizer and add some special tokens to account for the beginning and end of a sentence, as well as padding. However, I am not certain that these tokens are all that necessary.</p>
<div class="sourceCode" id="cb6"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1"></a><span class="co"># instantiate GPT2 tokenizer, byte-level encoding</span></span>
<span id="cb6-2"><a href="#cb6-2"></a>tokenizer <span class="op">=</span> GPT2Tokenizer.from_pretrained(<span class="st">&#39;distilgpt2&#39;</span>)</span>
<span id="cb6-3"><a href="#cb6-3"></a></span>
<span id="cb6-4"><a href="#cb6-4"></a><span class="co"># add special tokens that otherwise all share the same id</span></span>
<span id="cb6-5"><a href="#cb6-5"></a>tokenizer.add_special_tokens({<span class="st">&#39;bos_token&#39;</span>: <span class="st">&#39;&lt;bos&gt;&#39;</span>,</span>
<span id="cb6-6"><a href="#cb6-6"></a>                              <span class="st">&#39;eos_token&#39;</span>: <span class="st">&#39;&lt;eos&gt;&#39;</span>,</span>
<span id="cb6-7"><a href="#cb6-7"></a>                              <span class="st">&#39;pad_token&#39;</span>: <span class="st">&#39;&lt;pad&gt;&#39;</span>})</span>
<span id="cb6-8"><a href="#cb6-8"></a></span>
<span id="cb6-9"><a href="#cb6-9"></a><span class="co"># check token ids</span></span></code></pre></div>
<pre><code>## 3</code></pre>
<div class="sourceCode" id="cb8"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1"></a>tokenizer.eos_token_id</span></code></pre></div>
<pre><code>## 50258</code></pre>
<div class="sourceCode" id="cb10"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb10-1"><a href="#cb10-1"></a>tokenizer.bos_token_id</span></code></pre></div>
<pre><code>## 50257</code></pre>
<div class="sourceCode" id="cb12"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb12-1"><a href="#cb12-1"></a>tokenizer.unk_token_id</span></code></pre></div>
<pre><code>## 50256</code></pre>
<div class="sourceCode" id="cb14"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb14-1"><a href="#cb14-1"></a>tokenizer.pad_token_id</span></code></pre></div>
<pre><code>## 50259</code></pre>
</div>
<div id="instantiating-contiguous-data" class="section level2" number="2.2">
<h2 number="2.2"><span class="header-section-number">2.2</span> Instantiating Contiguous Data</h2>
<p>Next, we use <code>TextDataset</code> from the <code>transformers</code> package to build our contiguous data for language modeling.</p>
<div class="sourceCode" id="cb16"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb16-1"><a href="#cb16-1"></a><span class="co"># Transfomer Data Set -- we need everything the same length</span></span>
<span id="cb16-2"><a href="#cb16-2"></a>train_set <span class="op">=</span> TextDataset(tokenizer<span class="op">=</span>tokenizer,</span>
<span id="cb16-3"><a href="#cb16-3"></a>                        file_path<span class="op">=</span><span class="st">&#39;C:</span><span class="ch">\\</span><span class="st">Users</span><span class="ch">\\</span><span class="st">Andrew</span><span class="ch">\\</span><span class="st">Desktop</span><span class="ch">\\</span><span class="st">train.txt&#39;</span>,</span>
<span id="cb16-4"><a href="#cb16-4"></a>                        block_size<span class="op">=</span><span class="dv">1025</span>)</span>
<span id="cb16-5"><a href="#cb16-5"></a></span>
<span id="cb16-6"><a href="#cb16-6"></a>valid_set <span class="op">=</span> TextDataset(tokenizer<span class="op">=</span>tokenizer,</span>
<span id="cb16-7"><a href="#cb16-7"></a>                        file_path<span class="op">=</span><span class="st">&#39;C:</span><span class="ch">\\</span><span class="st">Users</span><span class="ch">\\</span><span class="st">Andrew</span><span class="ch">\\</span><span class="st">Desktop</span><span class="ch">\\</span><span class="st">valid.txt&#39;</span>,</span>
<span id="cb16-8"><a href="#cb16-8"></a>                        block_size<span class="op">=</span><span class="dv">1025</span>)</span></code></pre></div>
</div>
<div id="instantiating-the-model" class="section level2" number="2.3">
<h2 number="2.3"><span class="header-section-number">2.3</span> Instantiating the Model</h2>
<p>Like most transformer models, we instantiate it and attach it to the GPU like so. Since GPT-2 is such a large model, we use the distilled version.</p>
<div class="sourceCode" id="cb17"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb17-1"><a href="#cb17-1"></a><span class="co"># instantiate model GPT2 transformer with a language modeling head on top</span></span>
<span id="cb17-2"><a href="#cb17-2"></a>model <span class="op">=</span> GPT2LMHeadModel.from_pretrained(<span class="st">&#39;distilgpt2&#39;</span>).cuda()  <span class="co"># to GPU</span></span></code></pre></div>
<pre><code>## Some weights of GPT2LMHeadModel were not initialized from the model checkpoint at distilgpt2 and are newly initialized: [&#39;transformer.h.0.attn.masked_bias&#39;, &#39;transformer.h.1.attn.masked_bias&#39;, &#39;transformer.h.2.attn.masked_bias&#39;, &#39;transformer.h.3.attn.masked_bias&#39;, &#39;transformer.h.4.attn.masked_bias&#39;, &#39;transformer.h.5.attn.masked_bias&#39;]
## You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.</code></pre>
<p>Next, data loaders are prepared. Due to the contiguous nature of our data, ensure to use <code>SequentialSampler</code>.</p>
</div>
<div id="data-loaders" class="section level2" number="2.4">
<h2 number="2.4"><span class="header-section-number">2.4</span> Data Loaders</h2>
<div class="sourceCode" id="cb19"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb19-1"><a href="#cb19-1"></a><span class="co"># prepare data loaders</span></span>
<span id="cb19-2"><a href="#cb19-2"></a>train_dataloader <span class="op">=</span> datautils.DataLoader(dataset<span class="op">=</span>train_set,</span>
<span id="cb19-3"><a href="#cb19-3"></a>                                        sampler<span class="op">=</span>SequentialSampler(train_set),</span>
<span id="cb19-4"><a href="#cb19-4"></a>                                        batch_size<span class="op">=</span><span class="dv">3</span>,</span>
<span id="cb19-5"><a href="#cb19-5"></a>                                        drop_last<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb19-6"><a href="#cb19-6"></a>                                        shuffle<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb19-7"><a href="#cb19-7"></a></span>
<span id="cb19-8"><a href="#cb19-8"></a></span>
<span id="cb19-9"><a href="#cb19-9"></a>valid_dataloader <span class="op">=</span> datautils.DataLoader(dataset<span class="op">=</span>valid_set,</span>
<span id="cb19-10"><a href="#cb19-10"></a>                                        sampler<span class="op">=</span>SequentialSampler(valid_set),</span>
<span id="cb19-11"><a href="#cb19-11"></a>                                        batch_size<span class="op">=</span><span class="dv">3</span>,</span>
<span id="cb19-12"><a href="#cb19-12"></a>                                        drop_last<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb19-13"><a href="#cb19-13"></a>                                        shuffle<span class="op">=</span><span class="va">False</span>)</span></code></pre></div>
<p>Helper training functions are then instantiated. It is important to note that in training, we offset our batch by 1, yielding two 1024-length sequences (the maximum length for DistilGPT-2). Training a language model is training the model to predict the next word. We offset the sequence <code>y</code> by 1 since our “target” per token in <code>x</code> is the token to it’s right. Offsetting our inputs by one space is general practice for language modeling.</p>
</div>
<div id="training-and-helper-functions" class="section level2" number="2.5">
<h2 number="2.5"><span class="header-section-number">2.5</span> Training and Helper Functions</h2>
<div class="sourceCode" id="cb20"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb20-1"><a href="#cb20-1"></a><span class="co"># time function</span></span>
<span id="cb20-2"><a href="#cb20-2"></a><span class="kw">def</span> format_time(elapsed):</span>
<span id="cb20-3"><a href="#cb20-3"></a>    <span class="co">&#39;&#39;&#39;</span></span>
<span id="cb20-4"><a href="#cb20-4"></a><span class="co">    Takes a time in seconds and returns a string hh:mm:ss</span></span>
<span id="cb20-5"><a href="#cb20-5"></a><span class="co">    &#39;&#39;&#39;</span></span>
<span id="cb20-6"><a href="#cb20-6"></a>    <span class="co"># round to the nearest second.</span></span>
<span id="cb20-7"><a href="#cb20-7"></a>    elapsed_rounded <span class="op">=</span> <span class="bu">int</span>(<span class="bu">round</span>((elapsed)))</span>
<span id="cb20-8"><a href="#cb20-8"></a>    <span class="co"># format as hh:mm:ss</span></span>
<span id="cb20-9"><a href="#cb20-9"></a>    <span class="cf">return</span> <span class="bu">str</span>(datetime.timedelta(seconds<span class="op">=</span>elapsed_rounded))</span>
<span id="cb20-10"><a href="#cb20-10"></a></span>
<span id="cb20-11"><a href="#cb20-11"></a></span>
<span id="cb20-12"><a href="#cb20-12"></a><span class="kw">def</span> train(model, dataloader, optimizer):</span>
<span id="cb20-13"><a href="#cb20-13"></a></span>
<span id="cb20-14"><a href="#cb20-14"></a>    <span class="co"># capture time</span></span>
<span id="cb20-15"><a href="#cb20-15"></a>    total_t0 <span class="op">=</span> time.time()</span>
<span id="cb20-16"><a href="#cb20-16"></a></span>
<span id="cb20-17"><a href="#cb20-17"></a>    <span class="co"># Perform one full pass over the training set.</span></span>
<span id="cb20-18"><a href="#cb20-18"></a>    <span class="bu">print</span>(<span class="st">&quot;&quot;</span>)</span>
<span id="cb20-19"><a href="#cb20-19"></a>    <span class="bu">print</span>(<span class="st">&#39;======== Epoch </span><span class="sc">{:}</span><span class="st"> / </span><span class="sc">{:}</span><span class="st"> ========&#39;</span>.<span class="bu">format</span>(epoch <span class="op">+</span> <span class="dv">1</span>, epochs))</span>
<span id="cb20-20"><a href="#cb20-20"></a>    <span class="bu">print</span>(<span class="st">&#39;Training...&#39;</span>)</span>
<span id="cb20-21"><a href="#cb20-21"></a></span>
<span id="cb20-22"><a href="#cb20-22"></a>    <span class="co"># reset total loss for epoch</span></span>
<span id="cb20-23"><a href="#cb20-23"></a>    train_total_loss <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb20-24"><a href="#cb20-24"></a></span>
<span id="cb20-25"><a href="#cb20-25"></a>    <span class="co"># put model into traning mode</span></span>
<span id="cb20-26"><a href="#cb20-26"></a>    model.train()</span>
<span id="cb20-27"><a href="#cb20-27"></a></span>
<span id="cb20-28"><a href="#cb20-28"></a>    <span class="co"># for each batch of training data...</span></span>
<span id="cb20-29"><a href="#cb20-29"></a>    <span class="cf">for</span> step, batch <span class="kw">in</span> <span class="bu">enumerate</span>(dataloader):</span>
<span id="cb20-30"><a href="#cb20-30"></a></span>
<span id="cb20-31"><a href="#cb20-31"></a>        <span class="co"># progress update every 40 batches.</span></span>
<span id="cb20-32"><a href="#cb20-32"></a>        <span class="cf">if</span> step <span class="op">%</span> <span class="dv">40</span> <span class="op">==</span> <span class="dv">0</span> <span class="kw">and</span> <span class="kw">not</span> step <span class="op">==</span> <span class="dv">0</span>:</span>
<span id="cb20-33"><a href="#cb20-33"></a></span>
<span id="cb20-34"><a href="#cb20-34"></a>            <span class="co"># Report progress.</span></span>
<span id="cb20-35"><a href="#cb20-35"></a>            <span class="bu">print</span>(<span class="st">&#39;  Batch {:&gt;5,}  of  {:&gt;5,}.&#39;</span>.<span class="bu">format</span>(step, <span class="bu">len</span>(dataloader)))</span>
<span id="cb20-36"><a href="#cb20-36"></a></span>
<span id="cb20-37"><a href="#cb20-37"></a>        <span class="co"># Unpack this training batch from our dataloader:</span></span>
<span id="cb20-38"><a href="#cb20-38"></a>        <span class="co">#</span></span>
<span id="cb20-39"><a href="#cb20-39"></a>        <span class="co"># As we unpack the batch, we&#39;ll also copy each tensor to the GPU using</span></span>
<span id="cb20-40"><a href="#cb20-40"></a>        <span class="co">#</span></span>
<span id="cb20-41"><a href="#cb20-41"></a>        <span class="co"># `batch` contains our text in a PyTorch tensor</span></span>
<span id="cb20-42"><a href="#cb20-42"></a>        <span class="co">#  that we need to slice opposite ends off</span></span>
<span id="cb20-43"><a href="#cb20-43"></a>        x <span class="op">=</span> batch[:, :<span class="op">-</span><span class="dv">1</span>].cuda()</span>
<span id="cb20-44"><a href="#cb20-44"></a>        y <span class="op">=</span> batch[:, <span class="dv">1</span>:].cuda()</span>
<span id="cb20-45"><a href="#cb20-45"></a></span>
<span id="cb20-46"><a href="#cb20-46"></a>        <span class="co"># clear previously calculated gradients</span></span>
<span id="cb20-47"><a href="#cb20-47"></a>        optimizer.zero_grad()</span>
<span id="cb20-48"><a href="#cb20-48"></a></span>
<span id="cb20-49"><a href="#cb20-49"></a>        <span class="co"># runs the forward pass with autocasting.</span></span>
<span id="cb20-50"><a href="#cb20-50"></a>        <span class="cf">with</span> autocast():</span>
<span id="cb20-51"><a href="#cb20-51"></a>            <span class="co"># forward propagation (evaluate model on training batch)</span></span>
<span id="cb20-52"><a href="#cb20-52"></a>            logits <span class="op">=</span> model(input_ids<span class="op">=</span>x)[<span class="dv">0</span>]</span>
<span id="cb20-53"><a href="#cb20-53"></a></span>
<span id="cb20-54"><a href="#cb20-54"></a>            loss <span class="op">=</span> criterion(logits.flatten(<span class="dv">0</span>, <span class="dv">1</span>), y.flatten(<span class="dv">0</span>))</span>
<span id="cb20-55"><a href="#cb20-55"></a>            <span class="co"># sum the training loss over all batches for average loss at end</span></span>
<span id="cb20-56"><a href="#cb20-56"></a>            <span class="co"># loss is a tensor containing a single value</span></span>
<span id="cb20-57"><a href="#cb20-57"></a>            train_total_loss <span class="op">+=</span> loss.item()</span>
<span id="cb20-58"><a href="#cb20-58"></a></span>
<span id="cb20-59"><a href="#cb20-59"></a>        <span class="co"># Scales loss.  Calls backward() on scaled loss to create scaled gradients.</span></span>
<span id="cb20-60"><a href="#cb20-60"></a>        <span class="co"># Backward passes under autocast are not recommended.</span></span>
<span id="cb20-61"><a href="#cb20-61"></a>        <span class="co"># Backward ops run in the same dtype autocast chose for corresponding forward ops.</span></span>
<span id="cb20-62"><a href="#cb20-62"></a>        scaler.scale(loss).backward()</span>
<span id="cb20-63"><a href="#cb20-63"></a></span>
<span id="cb20-64"><a href="#cb20-64"></a>        <span class="co"># clip the gradients to 1 to reduce exploding gradients</span></span>
<span id="cb20-65"><a href="#cb20-65"></a>        torch.nn.utils.clip_grad_norm_(model.parameters(), <span class="fl">1.0</span>)</span>
<span id="cb20-66"><a href="#cb20-66"></a></span>
<span id="cb20-67"><a href="#cb20-67"></a>        <span class="co"># scaler.step() first unscales the gradients of the optimizer&#39;s assigned params.</span></span>
<span id="cb20-68"><a href="#cb20-68"></a>        <span class="co"># If these gradients do not contain infs or NaNs, optimizer.step() is then called,</span></span>
<span id="cb20-69"><a href="#cb20-69"></a>        <span class="co"># otherwise, optimizer.step() is skipped.</span></span>
<span id="cb20-70"><a href="#cb20-70"></a>        scaler.step(optimizer)</span>
<span id="cb20-71"><a href="#cb20-71"></a></span>
<span id="cb20-72"><a href="#cb20-72"></a>        <span class="co"># Updates the scale for next iteration.</span></span>
<span id="cb20-73"><a href="#cb20-73"></a>        scaler.update()</span>
<span id="cb20-74"><a href="#cb20-74"></a></span>
<span id="cb20-75"><a href="#cb20-75"></a>        <span class="co"># update the learning rate</span></span>
<span id="cb20-76"><a href="#cb20-76"></a>        scheduler.step()</span>
<span id="cb20-77"><a href="#cb20-77"></a></span>
<span id="cb20-78"><a href="#cb20-78"></a>    <span class="co"># calculate the average loss over all of the batches</span></span>
<span id="cb20-79"><a href="#cb20-79"></a>    avg_train_loss <span class="op">=</span> train_total_loss <span class="op">/</span> <span class="bu">len</span>(dataloader)</span>
<span id="cb20-80"><a href="#cb20-80"></a></span>
<span id="cb20-81"><a href="#cb20-81"></a>    <span class="co"># Record all statistics from this epoch.</span></span>
<span id="cb20-82"><a href="#cb20-82"></a>    training_stats.append(</span>
<span id="cb20-83"><a href="#cb20-83"></a>        {</span>
<span id="cb20-84"><a href="#cb20-84"></a>            <span class="st">&#39;Train Loss&#39;</span>: avg_train_loss</span>
<span id="cb20-85"><a href="#cb20-85"></a>        }</span>
<span id="cb20-86"><a href="#cb20-86"></a>    )</span>
<span id="cb20-87"><a href="#cb20-87"></a></span>
<span id="cb20-88"><a href="#cb20-88"></a>    <span class="co"># training time end</span></span>
<span id="cb20-89"><a href="#cb20-89"></a>    training_time <span class="op">=</span> format_time(time.time() <span class="op">-</span> total_t0)</span>
<span id="cb20-90"><a href="#cb20-90"></a></span>
<span id="cb20-91"><a href="#cb20-91"></a>    <span class="co"># print result summaries</span></span>
<span id="cb20-92"><a href="#cb20-92"></a>    <span class="bu">print</span>(<span class="st">&quot;&quot;</span>)</span>
<span id="cb20-93"><a href="#cb20-93"></a>    <span class="bu">print</span>(<span class="st">&quot;summary results&quot;</span>)</span>
<span id="cb20-94"><a href="#cb20-94"></a>    <span class="bu">print</span>(<span class="st">&quot;epoch | trn loss | trn time &quot;</span>)</span>
<span id="cb20-95"><a href="#cb20-95"></a>    <span class="bu">print</span>(<span class="ss">f&quot;</span><span class="sc">{</span>epoch<span class="op">+</span><span class="dv">1</span><span class="sc">:5d}</span><span class="ss"> | </span><span class="sc">{</span>avg_train_loss<span class="sc">:.5f}</span><span class="ss"> | </span><span class="sc">{</span>training_time<span class="sc">:}</span><span class="ss">&quot;</span>)</span>
<span id="cb20-96"><a href="#cb20-96"></a></span>
<span id="cb20-97"><a href="#cb20-97"></a>    <span class="cf">return</span> training_stats</span>
<span id="cb20-98"><a href="#cb20-98"></a></span>
<span id="cb20-99"><a href="#cb20-99"></a></span>
<span id="cb20-100"><a href="#cb20-100"></a><span class="kw">def</span> validating(model, dataloader):</span>
<span id="cb20-101"><a href="#cb20-101"></a></span>
<span id="cb20-102"><a href="#cb20-102"></a>    <span class="co"># capture validation time</span></span>
<span id="cb20-103"><a href="#cb20-103"></a>    total_t0 <span class="op">=</span> time.time()</span>
<span id="cb20-104"><a href="#cb20-104"></a></span>
<span id="cb20-105"><a href="#cb20-105"></a>    <span class="co"># After the completion of each training epoch, measure our performance on</span></span>
<span id="cb20-106"><a href="#cb20-106"></a>    <span class="co"># our validation set.</span></span>
<span id="cb20-107"><a href="#cb20-107"></a>    <span class="bu">print</span>(<span class="st">&quot;&quot;</span>)</span>
<span id="cb20-108"><a href="#cb20-108"></a>    <span class="bu">print</span>(<span class="st">&quot;Running Validation...&quot;</span>)</span>
<span id="cb20-109"><a href="#cb20-109"></a></span>
<span id="cb20-110"><a href="#cb20-110"></a>    <span class="co"># put the model in evaluation mode</span></span>
<span id="cb20-111"><a href="#cb20-111"></a>    model.<span class="bu">eval</span>()</span>
<span id="cb20-112"><a href="#cb20-112"></a></span>
<span id="cb20-113"><a href="#cb20-113"></a>    <span class="co"># track variables</span></span>
<span id="cb20-114"><a href="#cb20-114"></a>    total_valid_loss <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb20-115"><a href="#cb20-115"></a></span>
<span id="cb20-116"><a href="#cb20-116"></a>    <span class="co"># evaluate data for one epoch</span></span>
<span id="cb20-117"><a href="#cb20-117"></a>    <span class="cf">for</span> batch <span class="kw">in</span> dataloader:</span>
<span id="cb20-118"><a href="#cb20-118"></a></span>
<span id="cb20-119"><a href="#cb20-119"></a>        <span class="co"># Unpack this training batch from our dataloader:</span></span>
<span id="cb20-120"><a href="#cb20-120"></a>        <span class="co">#</span></span>
<span id="cb20-121"><a href="#cb20-121"></a>        <span class="co"># As we unpack the batch, we&#39;ll also copy each tensor to the GPU using</span></span>
<span id="cb20-122"><a href="#cb20-122"></a>        <span class="co">#</span></span>
<span id="cb20-123"><a href="#cb20-123"></a>        <span class="co"># `batch` contains our text in a PyTorch tensor</span></span>
<span id="cb20-124"><a href="#cb20-124"></a>        <span class="co">#  that we need to slice opposite ends off</span></span>
<span id="cb20-125"><a href="#cb20-125"></a>        x <span class="op">=</span> batch[:, :<span class="op">-</span><span class="dv">1</span>].cuda()</span>
<span id="cb20-126"><a href="#cb20-126"></a>        y <span class="op">=</span> batch[:, <span class="dv">1</span>:].cuda()</span>
<span id="cb20-127"><a href="#cb20-127"></a></span>
<span id="cb20-128"><a href="#cb20-128"></a>        <span class="co"># tell pytorch not to bother calculating gradients</span></span>
<span id="cb20-129"><a href="#cb20-129"></a>        <span class="co"># as its only necessary for training</span></span>
<span id="cb20-130"><a href="#cb20-130"></a>        <span class="cf">with</span> torch.no_grad():</span>
<span id="cb20-131"><a href="#cb20-131"></a>            <span class="co"># forward propagation (evaluate model on training batch)</span></span>
<span id="cb20-132"><a href="#cb20-132"></a>            logits <span class="op">=</span> model(input_ids<span class="op">=</span>x)[<span class="dv">0</span>]</span>
<span id="cb20-133"><a href="#cb20-133"></a></span>
<span id="cb20-134"><a href="#cb20-134"></a>            loss <span class="op">=</span> criterion(logits.flatten(<span class="dv">0</span>, <span class="dv">1</span>), y.flatten(<span class="dv">0</span>))</span>
<span id="cb20-135"><a href="#cb20-135"></a>            <span class="co"># sum the training loss over all batches for average loss at end</span></span>
<span id="cb20-136"><a href="#cb20-136"></a>            <span class="co"># loss is a tensor containing a single value</span></span>
<span id="cb20-137"><a href="#cb20-137"></a>            total_valid_loss <span class="op">+=</span> loss.item()</span>
<span id="cb20-138"><a href="#cb20-138"></a></span>
<span id="cb20-139"><a href="#cb20-139"></a>    <span class="co"># calculate the average loss over all of the batches.</span></span>
<span id="cb20-140"><a href="#cb20-140"></a>    <span class="kw">global</span> avg_val_loss</span>
<span id="cb20-141"><a href="#cb20-141"></a>    avg_val_loss <span class="op">=</span> total_valid_loss <span class="op">/</span> <span class="bu">len</span>(dataloader)</span>
<span id="cb20-142"><a href="#cb20-142"></a></span>
<span id="cb20-143"><a href="#cb20-143"></a>    <span class="co"># Record all statistics from this epoch.</span></span>
<span id="cb20-144"><a href="#cb20-144"></a>    valid_stats.append(</span>
<span id="cb20-145"><a href="#cb20-145"></a>        {</span>
<span id="cb20-146"><a href="#cb20-146"></a>            <span class="st">&#39;Val Loss&#39;</span>: avg_val_loss,</span>
<span id="cb20-147"><a href="#cb20-147"></a>            <span class="st">&#39;Val PPL.&#39;</span>: np.exp(avg_val_loss)</span>
<span id="cb20-148"><a href="#cb20-148"></a>        }</span>
<span id="cb20-149"><a href="#cb20-149"></a>    )</span>
<span id="cb20-150"><a href="#cb20-150"></a></span>
<span id="cb20-151"><a href="#cb20-151"></a>    <span class="co"># capture end validation time</span></span>
<span id="cb20-152"><a href="#cb20-152"></a>    training_time <span class="op">=</span> format_time(time.time() <span class="op">-</span> total_t0)</span>
<span id="cb20-153"><a href="#cb20-153"></a></span>
<span id="cb20-154"><a href="#cb20-154"></a>    <span class="co"># print result summaries</span></span>
<span id="cb20-155"><a href="#cb20-155"></a>    <span class="bu">print</span>(<span class="st">&quot;&quot;</span>)</span>
<span id="cb20-156"><a href="#cb20-156"></a>    <span class="bu">print</span>(<span class="st">&quot;summary results&quot;</span>)</span>
<span id="cb20-157"><a href="#cb20-157"></a>    <span class="bu">print</span>(<span class="st">&quot;epoch | val loss | val ppl | val time&quot;</span>)</span>
<span id="cb20-158"><a href="#cb20-158"></a>    <span class="bu">print</span>(<span class="ss">f&quot;</span><span class="sc">{</span>epoch<span class="op">+</span><span class="dv">1</span><span class="sc">:5d}</span><span class="ss"> | </span><span class="sc">{</span>avg_val_loss<span class="sc">:.5f}</span><span class="ss"> | </span><span class="sc">{np.</span>exp(avg_val_loss)<span class="sc">:.3f}</span><span class="ss"> | </span><span class="sc">{</span>training_time<span class="sc">:}</span><span class="ss">&quot;</span>)</span>
<span id="cb20-159"><a href="#cb20-159"></a></span>
<span id="cb20-160"><a href="#cb20-160"></a>    <span class="cf">return</span> valid_stats</span></code></pre></div>
</div>
<div id="training-preparation" class="section level2" number="2.6">
<h2 number="2.6"><span class="header-section-number">2.6</span> Training Preparation</h2>
<p>Now we are almost ready to train. A few other preparatory objects are created like the loss criteria, epochs, the optimizer, and our optimizer scheduler.</p>
<div class="sourceCode" id="cb21"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb21-1"><a href="#cb21-1"></a></span>
<span id="cb21-2"><a href="#cb21-2"></a><span class="co"># create gradient scaler for mixed precision</span></span>
<span id="cb21-3"><a href="#cb21-3"></a>scaler <span class="op">=</span> GradScaler()</span>
<span id="cb21-4"><a href="#cb21-4"></a></span>
<span id="cb21-5"><a href="#cb21-5"></a><span class="co"># training length</span></span>
<span id="cb21-6"><a href="#cb21-6"></a>epochs <span class="op">=</span> <span class="dv">8</span></span>
<span id="cb21-7"><a href="#cb21-7"></a></span>
<span id="cb21-8"><a href="#cb21-8"></a><span class="co"># loss function</span></span>
<span id="cb21-9"><a href="#cb21-9"></a>criterion <span class="op">=</span> nn.CrossEntropyLoss()</span>
<span id="cb21-10"><a href="#cb21-10"></a></span>
<span id="cb21-11"><a href="#cb21-11"></a><span class="co"># optimizer: Adam w/ Weight Decay Fix</span></span>
<span id="cb21-12"><a href="#cb21-12"></a><span class="co"># set to optimizer_grouped_parameters or model.parameters()</span></span>
<span id="cb21-13"><a href="#cb21-13"></a>optimizer <span class="op">=</span> AdamW(model.parameters(),</span>
<span id="cb21-14"><a href="#cb21-14"></a>                  lr<span class="op">=</span><span class="fl">2e-5</span>)</span>
<span id="cb21-15"><a href="#cb21-15"></a></span>
<span id="cb21-16"><a href="#cb21-16"></a></span>
<span id="cb21-17"><a href="#cb21-17"></a><span class="co"># Total number of training steps is [number of batches] x [number of epochs].</span></span>
<span id="cb21-18"><a href="#cb21-18"></a><span class="co"># (Note that this is not the same as the number of training samples).</span></span>
<span id="cb21-19"><a href="#cb21-19"></a>total_steps <span class="op">=</span> <span class="bu">len</span>(train_dataloader) <span class="op">*</span> epochs</span>
<span id="cb21-20"><a href="#cb21-20"></a></span>
<span id="cb21-21"><a href="#cb21-21"></a><span class="co"># Create the learning rate scheduler.</span></span>
<span id="cb21-22"><a href="#cb21-22"></a>scheduler <span class="op">=</span> get_linear_schedule_with_warmup(optimizer,</span>
<span id="cb21-23"><a href="#cb21-23"></a>                                            num_warmup_steps<span class="op">=</span><span class="dv">0</span>,</span>
<span id="cb21-24"><a href="#cb21-24"></a>                                            num_training_steps<span class="op">=</span>total_steps)</span></code></pre></div>
</div>
<div id="train" class="section level2" number="2.7">
<h2 number="2.7"><span class="header-section-number">2.7</span> Train</h2>
<p>Now we are ready to train our model.</p>
<div class="sourceCode" id="cb22"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb22-1"><a href="#cb22-1"></a><span class="co"># create training result storage</span></span>
<span id="cb22-2"><a href="#cb22-2"></a>training_stats <span class="op">=</span> []</span>
<span id="cb22-3"><a href="#cb22-3"></a>valid_stats <span class="op">=</span> []</span>
<span id="cb22-4"><a href="#cb22-4"></a>best_valid_loss <span class="op">=</span> <span class="bu">float</span>(<span class="st">&#39;inf&#39;</span>)</span>
<span id="cb22-5"><a href="#cb22-5"></a></span>
<span id="cb22-6"><a href="#cb22-6"></a><span class="co"># for each epoch</span></span>
<span id="cb22-7"><a href="#cb22-7"></a><span class="cf">for</span> epoch <span class="kw">in</span> <span class="bu">range</span>(epochs):</span>
<span id="cb22-8"><a href="#cb22-8"></a>    <span class="co"># train</span></span>
<span id="cb22-9"><a href="#cb22-9"></a>    train(model, train_dataloader, optimizer)</span>
<span id="cb22-10"><a href="#cb22-10"></a>    <span class="co"># validate</span></span>
<span id="cb22-11"><a href="#cb22-11"></a>    validating(model, valid_dataloader)</span>
<span id="cb22-12"><a href="#cb22-12"></a>    <span class="co"># check validation loss</span></span>
<span id="cb22-13"><a href="#cb22-13"></a>    <span class="cf">if</span> valid_stats[epoch][<span class="st">&#39;Val Loss&#39;</span>] <span class="op">&lt;</span> best_valid_loss:</span>
<span id="cb22-14"><a href="#cb22-14"></a>        best_valid_loss <span class="op">=</span> valid_stats[epoch][<span class="st">&#39;Val Loss&#39;</span>]</span>
<span id="cb22-15"><a href="#cb22-15"></a>        <span class="co"># save best model for use later</span></span>
<span id="cb22-16"><a href="#cb22-16"></a>        torch.save(model.state_dict(), <span class="st">&#39;gpt2-model1.pt&#39;</span>)  <span class="co"># torch save</span></span>
<span id="cb22-17"><a href="#cb22-17"></a>        model_to_save <span class="op">=</span> model.module <span class="cf">if</span> <span class="bu">hasattr</span>(model, <span class="st">&#39;module&#39;</span>) <span class="cf">else</span> model</span>
<span id="cb22-18"><a href="#cb22-18"></a>        model_to_save.save_pretrained(<span class="st">&#39;./model_save/gpt2/&#39;</span>)  <span class="co"># transformers save</span></span>
<span id="cb22-19"><a href="#cb22-19"></a>        tokenizer.save_pretrained(<span class="st">&#39;./model_save/gpt2/&#39;</span>)  <span class="co"># transformers save</span></span></code></pre></div>
<pre><code>## 
## ======== Epoch 1 / 8 ========
## Training...
##   Batch    40  of  1,102.
##   Batch    80  of  1,102.
##   Batch   120  of  1,102.
##   Batch   160  of  1,102.
##   Batch   200  of  1,102.
##   Batch   240  of  1,102.
##   Batch   280  of  1,102.
##   Batch   320  of  1,102.
##   Batch   360  of  1,102.
##   Batch   400  of  1,102.
##   Batch   440  of  1,102.
##   Batch   480  of  1,102.
##   Batch   520  of  1,102.
##   Batch   560  of  1,102.
##   Batch   600  of  1,102.
##   Batch   640  of  1,102.
##   Batch   680  of  1,102.
##   Batch   720  of  1,102.
##   Batch   760  of  1,102.
##   Batch   800  of  1,102.
##   Batch   840  of  1,102.
##   Batch   880  of  1,102.
##   Batch   920  of  1,102.
##   Batch   960  of  1,102.
##   Batch 1,000  of  1,102.
##   Batch 1,040  of  1,102.
##   Batch 1,080  of  1,102.
## 
## summary results
## epoch | trn loss | trn time 
##     1 | 3.70191 | 0:03:05
## [{&#39;Train Loss&#39;: 3.701909479125658}]
## 
## Running Validation...
## 
## summary results
## epoch | val loss | val ppl | val time
##     1 | 3.56233 | 35.245 | 0:00:19
## [{&#39;Val Loss&#39;: 3.5623276340030827, &#39;Val PPL.&#39;: 35.24513952738014}]
## (&#39;./model_save/gpt2/vocab.json&#39;, &#39;./model_save/gpt2/merges.txt&#39;, &#39;./model_save/gpt2/special_tokens_map.json&#39;, &#39;./model_save/gpt2/added_tokens.json&#39;)
## 
## ======== Epoch 2 / 8 ========
## Training...
##   Batch    40  of  1,102.
##   Batch    80  of  1,102.
##   Batch   120  of  1,102.
##   Batch   160  of  1,102.
##   Batch   200  of  1,102.
##   Batch   240  of  1,102.
##   Batch   280  of  1,102.
##   Batch   320  of  1,102.
##   Batch   360  of  1,102.
##   Batch   400  of  1,102.
##   Batch   440  of  1,102.
##   Batch   480  of  1,102.
##   Batch   520  of  1,102.
##   Batch   560  of  1,102.
##   Batch   600  of  1,102.
##   Batch   640  of  1,102.
##   Batch   680  of  1,102.
##   Batch   720  of  1,102.
##   Batch   760  of  1,102.
##   Batch   800  of  1,102.
##   Batch   840  of  1,102.
##   Batch   880  of  1,102.
##   Batch   920  of  1,102.
##   Batch   960  of  1,102.
##   Batch 1,000  of  1,102.
##   Batch 1,040  of  1,102.
##   Batch 1,080  of  1,102.
## 
## summary results
## epoch | trn loss | trn time 
##     2 | 3.62447 | 0:03:08
## [{&#39;Train Loss&#39;: 3.701909479125658}, {&#39;Train Loss&#39;: 3.624465454944465}]
## 
## Running Validation...
## 
## summary results
## epoch | val loss | val ppl | val time
##     2 | 3.51926 | 33.759 | 0:00:19
## [{&#39;Val Loss&#39;: 3.5623276340030827, &#39;Val PPL.&#39;: 35.24513952738014}, {&#39;Val Loss&#39;: 3.5192573451641325, &#39;Val PPL.&#39;: 33.759347609035665}]
## (&#39;./model_save/gpt2/vocab.json&#39;, &#39;./model_save/gpt2/merges.txt&#39;, &#39;./model_save/gpt2/special_tokens_map.json&#39;, &#39;./model_save/gpt2/added_tokens.json&#39;)
## 
## ======== Epoch 3 / 8 ========
## Training...
##   Batch    40  of  1,102.
##   Batch    80  of  1,102.
##   Batch   120  of  1,102.
##   Batch   160  of  1,102.
##   Batch   200  of  1,102.
##   Batch   240  of  1,102.
##   Batch   280  of  1,102.
##   Batch   320  of  1,102.
##   Batch   360  of  1,102.
##   Batch   400  of  1,102.
##   Batch   440  of  1,102.
##   Batch   480  of  1,102.
##   Batch   520  of  1,102.
##   Batch   560  of  1,102.
##   Batch   600  of  1,102.
##   Batch   640  of  1,102.
##   Batch   680  of  1,102.
##   Batch   720  of  1,102.
##   Batch   760  of  1,102.
##   Batch   800  of  1,102.
##   Batch   840  of  1,102.
##   Batch   880  of  1,102.
##   Batch   920  of  1,102.
##   Batch   960  of  1,102.
##   Batch 1,000  of  1,102.
##   Batch 1,040  of  1,102.
##   Batch 1,080  of  1,102.
## 
## summary results
## epoch | trn loss | trn time 
##     3 | 3.59240 | 0:03:03
## [{&#39;Train Loss&#39;: 3.701909479125658}, {&#39;Train Loss&#39;: 3.624465454944465}, {&#39;Train Loss&#39;: 3.5924020607545026}]
## 
## Running Validation...
## 
## summary results
## epoch | val loss | val ppl | val time
##     3 | 3.49755 | 33.034 | 0:00:19
## [{&#39;Val Loss&#39;: 3.5623276340030827, &#39;Val PPL.&#39;: 35.24513952738014}, {&#39;Val Loss&#39;: 3.5192573451641325, &#39;Val PPL.&#39;: 33.759347609035665}, {&#39;Val Loss&#39;: 3.4975468048818934, &#39;Val PPL.&#39;: 33.03431285932696}]
## (&#39;./model_save/gpt2/vocab.json&#39;, &#39;./model_save/gpt2/merges.txt&#39;, &#39;./model_save/gpt2/special_tokens_map.json&#39;, &#39;./model_save/gpt2/added_tokens.json&#39;)
## 
## ======== Epoch 4 / 8 ========
## Training...
##   Batch    40  of  1,102.
##   Batch    80  of  1,102.
##   Batch   120  of  1,102.
##   Batch   160  of  1,102.
##   Batch   200  of  1,102.
##   Batch   240  of  1,102.
##   Batch   280  of  1,102.
##   Batch   320  of  1,102.
##   Batch   360  of  1,102.
##   Batch   400  of  1,102.
##   Batch   440  of  1,102.
##   Batch   480  of  1,102.
##   Batch   520  of  1,102.
##   Batch   560  of  1,102.
##   Batch   600  of  1,102.
##   Batch   640  of  1,102.
##   Batch   680  of  1,102.
##   Batch   720  of  1,102.
##   Batch   760  of  1,102.
##   Batch   800  of  1,102.
##   Batch   840  of  1,102.
##   Batch   880  of  1,102.
##   Batch   920  of  1,102.
##   Batch   960  of  1,102.
##   Batch 1,000  of  1,102.
##   Batch 1,040  of  1,102.
##   Batch 1,080  of  1,102.
## 
## summary results
## epoch | trn loss | trn time 
##     4 | 3.57474 | 0:03:03
## [{&#39;Train Loss&#39;: 3.701909479125658}, {&#39;Train Loss&#39;: 3.624465454944465}, {&#39;Train Loss&#39;: 3.5924020607545026}, {&#39;Train Loss&#39;: 3.574735596478526}]
## 
## Running Validation...
## 
## summary results
## epoch | val loss | val ppl | val time
##     4 | 3.48494 | 32.620 | 0:00:19
## [{&#39;Val Loss&#39;: 3.5623276340030827, &#39;Val PPL.&#39;: 35.24513952738014}, {&#39;Val Loss&#39;: 3.5192573451641325, &#39;Val PPL.&#39;: 33.759347609035665}, {&#39;Val Loss&#39;: 3.4975468048818934, &#39;Val PPL.&#39;: 33.03431285932696}, {&#39;Val Loss&#39;: 3.4849378885389704, &#39;Val PPL.&#39;: 32.620400946173945}]
## (&#39;./model_save/gpt2/vocab.json&#39;, &#39;./model_save/gpt2/merges.txt&#39;, &#39;./model_save/gpt2/special_tokens_map.json&#39;, &#39;./model_save/gpt2/added_tokens.json&#39;)
## 
## ======== Epoch 5 / 8 ========
## Training...
##   Batch    40  of  1,102.
##   Batch    80  of  1,102.
##   Batch   120  of  1,102.
##   Batch   160  of  1,102.
##   Batch   200  of  1,102.
##   Batch   240  of  1,102.
##   Batch   280  of  1,102.
##   Batch   320  of  1,102.
##   Batch   360  of  1,102.
##   Batch   400  of  1,102.
##   Batch   440  of  1,102.
##   Batch   480  of  1,102.
##   Batch   520  of  1,102.
##   Batch   560  of  1,102.
##   Batch   600  of  1,102.
##   Batch   640  of  1,102.
##   Batch   680  of  1,102.
##   Batch   720  of  1,102.
##   Batch   760  of  1,102.
##   Batch   800  of  1,102.
##   Batch   840  of  1,102.
##   Batch   880  of  1,102.
##   Batch   920  of  1,102.
##   Batch   960  of  1,102.
##   Batch 1,000  of  1,102.
##   Batch 1,040  of  1,102.
##   Batch 1,080  of  1,102.
## 
## summary results
## epoch | trn loss | trn time 
##     5 | 3.56358 | 0:03:06
## [{&#39;Train Loss&#39;: 3.701909479125658}, {&#39;Train Loss&#39;: 3.624465454944465}, {&#39;Train Loss&#39;: 3.5924020607545026}, {&#39;Train Loss&#39;: 3.574735596478526}, {&#39;Train Loss&#39;: 3.5635797964466462}]
## 
## Running Validation...
## 
## summary results
## epoch | val loss | val ppl | val time
##     5 | 3.47629 | 32.340 | 0:00:19
## [{&#39;Val Loss&#39;: 3.5623276340030827, &#39;Val PPL.&#39;: 35.24513952738014}, {&#39;Val Loss&#39;: 3.5192573451641325, &#39;Val PPL.&#39;: 33.759347609035665}, {&#39;Val Loss&#39;: 3.4975468048818934, &#39;Val PPL.&#39;: 33.03431285932696}, {&#39;Val Loss&#39;: 3.4849378885389704, &#39;Val PPL.&#39;: 32.620400946173945}, {&#39;Val Loss&#39;: 3.476289682671926, &#39;Val PPL.&#39;: 32.33950935815459}]
## (&#39;./model_save/gpt2/vocab.json&#39;, &#39;./model_save/gpt2/merges.txt&#39;, &#39;./model_save/gpt2/special_tokens_map.json&#39;, &#39;./model_save/gpt2/added_tokens.json&#39;)
## 
## ======== Epoch 6 / 8 ========
## Training...
##   Batch    40  of  1,102.
##   Batch    80  of  1,102.
##   Batch   120  of  1,102.
##   Batch   160  of  1,102.
##   Batch   200  of  1,102.
##   Batch   240  of  1,102.
##   Batch   280  of  1,102.
##   Batch   320  of  1,102.
##   Batch   360  of  1,102.
##   Batch   400  of  1,102.
##   Batch   440  of  1,102.
##   Batch   480  of  1,102.
##   Batch   520  of  1,102.
##   Batch   560  of  1,102.
##   Batch   600  of  1,102.
##   Batch   640  of  1,102.
##   Batch   680  of  1,102.
##   Batch   720  of  1,102.
##   Batch   760  of  1,102.
##   Batch   800  of  1,102.
##   Batch   840  of  1,102.
##   Batch   880  of  1,102.
##   Batch   920  of  1,102.
##   Batch   960  of  1,102.
##   Batch 1,000  of  1,102.
##   Batch 1,040  of  1,102.
##   Batch 1,080  of  1,102.
## 
## summary results
## epoch | trn loss | trn time 
##     6 | 3.55630 | 0:03:09
## [{&#39;Train Loss&#39;: 3.701909479125658}, {&#39;Train Loss&#39;: 3.624465454944465}, {&#39;Train Loss&#39;: 3.5924020607545026}, {&#39;Train Loss&#39;: 3.574735596478526}, {&#39;Train Loss&#39;: 3.5635797964466462}, {&#39;Train Loss&#39;: 3.5563041921535983}]
## 
## Running Validation...
## 
## summary results
## epoch | val loss | val ppl | val time
##     6 | 3.47130 | 32.179 | 0:00:19
## [{&#39;Val Loss&#39;: 3.5623276340030827, &#39;Val PPL.&#39;: 35.24513952738014}, {&#39;Val Loss&#39;: 3.5192573451641325, &#39;Val PPL.&#39;: 33.759347609035665}, {&#39;Val Loss&#39;: 3.4975468048818934, &#39;Val PPL.&#39;: 33.03431285932696}, {&#39;Val Loss&#39;: 3.4849378885389704, &#39;Val PPL.&#39;: 32.620400946173945}, {&#39;Val Loss&#39;: 3.476289682671926, &#39;Val PPL.&#39;: 32.33950935815459}, {&#39;Val Loss&#39;: 3.4713046940729075, &#39;Val PPL.&#39;: 32.17869842605189}]
## (&#39;./model_save/gpt2/vocab.json&#39;, &#39;./model_save/gpt2/merges.txt&#39;, &#39;./model_save/gpt2/special_tokens_map.json&#39;, &#39;./model_save/gpt2/added_tokens.json&#39;)
## 
## ======== Epoch 7 / 8 ========
## Training...
##   Batch    40  of  1,102.
##   Batch    80  of  1,102.
##   Batch   120  of  1,102.
##   Batch   160  of  1,102.
##   Batch   200  of  1,102.
##   Batch   240  of  1,102.
##   Batch   280  of  1,102.
##   Batch   320  of  1,102.
##   Batch   360  of  1,102.
##   Batch   400  of  1,102.
##   Batch   440  of  1,102.
##   Batch   480  of  1,102.
##   Batch   520  of  1,102.
##   Batch   560  of  1,102.
##   Batch   600  of  1,102.
##   Batch   640  of  1,102.
##   Batch   680  of  1,102.
##   Batch   720  of  1,102.
##   Batch   760  of  1,102.
##   Batch   800  of  1,102.
##   Batch   840  of  1,102.
##   Batch   880  of  1,102.
##   Batch   920  of  1,102.
##   Batch   960  of  1,102.
##   Batch 1,000  of  1,102.
##   Batch 1,040  of  1,102.
##   Batch 1,080  of  1,102.
## 
## summary results
## epoch | trn loss | trn time 
##     7 | 3.55258 | 0:03:08
## [{&#39;Train Loss&#39;: 3.701909479125658}, {&#39;Train Loss&#39;: 3.624465454944465}, {&#39;Train Loss&#39;: 3.5924020607545026}, {&#39;Train Loss&#39;: 3.574735596478526}, {&#39;Train Loss&#39;: 3.5635797964466462}, {&#39;Train Loss&#39;: 3.5563041921535983}, {&#39;Train Loss&#39;: 3.5525805800882746}]
## 
## Running Validation...
## 
## summary results
## epoch | val loss | val ppl | val time
##     7 | 3.46844 | 32.087 | 0:00:19
## [{&#39;Val Loss&#39;: 3.5623276340030827, &#39;Val PPL.&#39;: 35.24513952738014}, {&#39;Val Loss&#39;: 3.5192573451641325, &#39;Val PPL.&#39;: 33.759347609035665}, {&#39;Val Loss&#39;: 3.4975468048818934, &#39;Val PPL.&#39;: 33.03431285932696}, {&#39;Val Loss&#39;: 3.4849378885389704, &#39;Val PPL.&#39;: 32.620400946173945}, {&#39;Val Loss&#39;: 3.476289682671926, &#39;Val PPL.&#39;: 32.33950935815459}, {&#39;Val Loss&#39;: 3.4713046940729075, &#39;Val PPL.&#39;: 32.17869842605189}, {&#39;Val Loss&#39;: 3.4684418701327866, &#39;Val PPL.&#39;: 32.086708216550335}]
## (&#39;./model_save/gpt2/vocab.json&#39;, &#39;./model_save/gpt2/merges.txt&#39;, &#39;./model_save/gpt2/special_tokens_map.json&#39;, &#39;./model_save/gpt2/added_tokens.json&#39;)
## 
## ======== Epoch 8 / 8 ========
## Training...
##   Batch    40  of  1,102.
##   Batch    80  of  1,102.
##   Batch   120  of  1,102.
##   Batch   160  of  1,102.
##   Batch   200  of  1,102.
##   Batch   240  of  1,102.
##   Batch   280  of  1,102.
##   Batch   320  of  1,102.
##   Batch   360  of  1,102.
##   Batch   400  of  1,102.
##   Batch   440  of  1,102.
##   Batch   480  of  1,102.
##   Batch   520  of  1,102.
##   Batch   560  of  1,102.
##   Batch   600  of  1,102.
##   Batch   640  of  1,102.
##   Batch   680  of  1,102.
##   Batch   720  of  1,102.
##   Batch   760  of  1,102.
##   Batch   800  of  1,102.
##   Batch   840  of  1,102.
##   Batch   880  of  1,102.
##   Batch   920  of  1,102.
##   Batch   960  of  1,102.
##   Batch 1,000  of  1,102.
##   Batch 1,040  of  1,102.
##   Batch 1,080  of  1,102.
## 
## summary results
## epoch | trn loss | trn time 
##     8 | 3.54971 | 0:03:07
## [{&#39;Train Loss&#39;: 3.701909479125658}, {&#39;Train Loss&#39;: 3.624465454944465}, {&#39;Train Loss&#39;: 3.5924020607545026}, {&#39;Train Loss&#39;: 3.574735596478526}, {&#39;Train Loss&#39;: 3.5635797964466462}, {&#39;Train Loss&#39;: 3.5563041921535983}, {&#39;Train Loss&#39;: 3.5525805800882746}, {&#39;Train Loss&#39;: 3.5497063517354146}]
## 
## Running Validation...
## 
## summary results
## epoch | val loss | val ppl | val time
##     8 | 3.46751 | 32.057 | 0:00:19
## [{&#39;Val Loss&#39;: 3.5623276340030827, &#39;Val PPL.&#39;: 35.24513952738014}, {&#39;Val Loss&#39;: 3.5192573451641325, &#39;Val PPL.&#39;: 33.759347609035665}, {&#39;Val Loss&#39;: 3.4975468048818934, &#39;Val PPL.&#39;: 33.03431285932696}, {&#39;Val Loss&#39;: 3.4849378885389704, &#39;Val PPL.&#39;: 32.620400946173945}, {&#39;Val Loss&#39;: 3.476289682671926, &#39;Val PPL.&#39;: 32.33950935815459}, {&#39;Val Loss&#39;: 3.4713046940729075, &#39;Val PPL.&#39;: 32.17869842605189}, {&#39;Val Loss&#39;: 3.4684418701327866, &#39;Val PPL.&#39;: 32.086708216550335}, {&#39;Val Loss&#39;: 3.4675112727849458, &#39;Val PPL.&#39;: 32.05686230040219}]
## (&#39;./model_save/gpt2/vocab.json&#39;, &#39;./model_save/gpt2/merges.txt&#39;, &#39;./model_save/gpt2/special_tokens_map.json&#39;, &#39;./model_save/gpt2/added_tokens.json&#39;)
## 
## C:\Users\Andrew\Anaconda3\envs\my_ml\lib\site-packages\torch\optim\lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
##   &quot;https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate&quot;, UserWarning)</code></pre>
</div>
<div id="beam-search-text-generation" class="section level2" number="2.8">
<h2 number="2.8"><span class="header-section-number">2.8</span> Beam Search: Text Generation</h2>
<p>While there are a couple of different ways to generate predictions, I found that beam search provided the best results. There are several parameters you should play around with to get great predictions:</p>
<ol style="list-style-type: decimal">
<li><p><code>temperature</code> – very low values, circa below .5 tended to create near unintelligble text while values above 1, the default, seemed better. This value impacts the next token’s probability.</p></li>
<li><p><code>top_k</code> – values below the default, 50, seemed to work better. <code>Top_k</code> represents the number of highest probability vocabulary tokens to keep.</p></li>
<li><p><code>top_p</code> – values below the default, 1.0, seemed to work better. <code>Top_p</code> represents the cumulative probability of parameter highest probability vocabulary tokens to keep for nucleus sampling</p></li>
<li><p><code>num_beams</code> – open for experimentation. <code>Num_beams</code>represents how many sequences will be generated, returning the sequence whose overall probability is the highest</p></li>
<li><p><code>num_return_sequences</code> – how many examples you want returned.</p></li>
</ol>
<div class="sourceCode" id="cb24"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb24-1"><a href="#cb24-1"></a>model.<span class="bu">eval</span>()<span class="op">;</span></span>
<span id="cb24-2"><a href="#cb24-2"></a><span class="co"># beam search</span></span></code></pre></div>
<pre><code>## GPT2LMHeadModel(
##   (transformer): GPT2Model(
##     (wte): Embedding(50257, 768)
##     (wpe): Embedding(1024, 768)
##     (drop): Dropout(p=0.1, inplace=False)
##     (h): ModuleList(
##       (0): Block(
##         (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
##         (attn): Attention(
##           (c_attn): Conv1D()
##           (c_proj): Conv1D()
##           (attn_dropout): Dropout(p=0.1, inplace=False)
##           (resid_dropout): Dropout(p=0.1, inplace=False)
##         )
##         (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
##         (mlp): MLP(
##           (c_fc): Conv1D()
##           (c_proj): Conv1D()
##           (dropout): Dropout(p=0.1, inplace=False)
##         )
##       )
##       (1): Block(
##         (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
##         (attn): Attention(
##           (c_attn): Conv1D()
##           (c_proj): Conv1D()
##           (attn_dropout): Dropout(p=0.1, inplace=False)
##           (resid_dropout): Dropout(p=0.1, inplace=False)
##         )
##         (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
##         (mlp): MLP(
##           (c_fc): Conv1D()
##           (c_proj): Conv1D()
##           (dropout): Dropout(p=0.1, inplace=False)
##         )
##       )
##       (2): Block(
##         (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
##         (attn): Attention(
##           (c_attn): Conv1D()
##           (c_proj): Conv1D()
##           (attn_dropout): Dropout(p=0.1, inplace=False)
##           (resid_dropout): Dropout(p=0.1, inplace=False)
##         )
##         (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
##         (mlp): MLP(
##           (c_fc): Conv1D()
##           (c_proj): Conv1D()
##           (dropout): Dropout(p=0.1, inplace=False)
##         )
##       )
##       (3): Block(
##         (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
##         (attn): Attention(
##           (c_attn): Conv1D()
##           (c_proj): Conv1D()
##           (attn_dropout): Dropout(p=0.1, inplace=False)
##           (resid_dropout): Dropout(p=0.1, inplace=False)
##         )
##         (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
##         (mlp): MLP(
##           (c_fc): Conv1D()
##           (c_proj): Conv1D()
##           (dropout): Dropout(p=0.1, inplace=False)
##         )
##       )
##       (4): Block(
##         (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
##         (attn): Attention(
##           (c_attn): Conv1D()
##           (c_proj): Conv1D()
##           (attn_dropout): Dropout(p=0.1, inplace=False)
##           (resid_dropout): Dropout(p=0.1, inplace=False)
##         )
##         (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
##         (mlp): MLP(
##           (c_fc): Conv1D()
##           (c_proj): Conv1D()
##           (dropout): Dropout(p=0.1, inplace=False)
##         )
##       )
##       (5): Block(
##         (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
##         (attn): Attention(
##           (c_attn): Conv1D()
##           (c_proj): Conv1D()
##           (attn_dropout): Dropout(p=0.1, inplace=False)
##           (resid_dropout): Dropout(p=0.1, inplace=False)
##         )
##         (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
##         (mlp): MLP(
##           (c_fc): Conv1D()
##           (c_proj): Conv1D()
##           (dropout): Dropout(p=0.1, inplace=False)
##         )
##       )
##     )
##     (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
##   )
##   (lm_head): Linear(in_features=768, out_features=50257, bias=False)
## )</code></pre>
<div class="sourceCode" id="cb26"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb26-1"><a href="#cb26-1"></a>text <span class="op">=</span> <span class="st">&quot;The Afghan National Army reported&quot;</span></span>
<span id="cb26-2"><a href="#cb26-2"></a>ids <span class="op">=</span> torch.tensor(tokenizer.encode(text)).unsqueeze(<span class="dv">0</span>).cuda()</span>
<span id="cb26-3"><a href="#cb26-3"></a>generated_ids <span class="op">=</span> model.generate(</span>
<span id="cb26-4"><a href="#cb26-4"></a>                        input_ids<span class="op">=</span>ids,  <span class="co"># input</span></span>
<span id="cb26-5"><a href="#cb26-5"></a>                        max_length<span class="op">=</span><span class="dv">45</span>,  <span class="co"># default 20</span></span>
<span id="cb26-6"><a href="#cb26-6"></a>                        min_length<span class="op">=</span><span class="dv">0</span>,  <span class="co"># default 0</span></span>
<span id="cb26-7"><a href="#cb26-7"></a>                        do_sample<span class="op">=</span><span class="va">True</span>,  <span class="co"># don&#39;t use greedy decoding</span></span>
<span id="cb26-8"><a href="#cb26-8"></a>                        early_stopping<span class="op">=</span><span class="va">False</span>,  <span class="co"># search is stopped when at least num_beams sentences finished</span></span>
<span id="cb26-9"><a href="#cb26-9"></a>                        temperature<span class="op">=</span><span class="fl">2.45</span>,  <span class="co"># default 1.0</span></span>
<span id="cb26-10"><a href="#cb26-10"></a>                        top_k<span class="op">=</span><span class="dv">45</span>,  <span class="co"># default 50</span></span>
<span id="cb26-11"><a href="#cb26-11"></a>                        top_p<span class="op">=</span><span class="fl">0.7</span>,  <span class="co"># default 1.0</span></span>
<span id="cb26-12"><a href="#cb26-12"></a>                        repetition_penalty<span class="op">=</span><span class="fl">2.0</span>,  <span class="co"># rep. penalty</span></span>
<span id="cb26-13"><a href="#cb26-13"></a>                        num_beams<span class="op">=</span><span class="dv">6</span>,</span>
<span id="cb26-14"><a href="#cb26-14"></a>                        num_return_sequences<span class="op">=</span><span class="dv">2</span>, <span class="co">#  num ind. computed returned sequences</span></span>
<span id="cb26-15"><a href="#cb26-15"></a>                        bos_token_id<span class="op">=</span>tokenizer.bos_token_id</span>
<span id="cb26-16"><a href="#cb26-16"></a>                        )</span></code></pre></div>
<pre><code>## Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence</code></pre>
<div class="sourceCode" id="cb28"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb28-1"><a href="#cb28-1"></a>results <span class="op">=</span> [tokenizer.decode(g, skip_special_tokens<span class="op">=</span><span class="va">True</span>, clean_up_tokenization_spaces<span class="op">=</span><span class="va">True</span>) <span class="cf">for</span> g <span class="kw">in</span> generated_ids]</span>
<span id="cb28-2"><a href="#cb28-2"></a></span>
<span id="cb28-3"><a href="#cb28-3"></a><span class="cf">for</span> i <span class="kw">in</span> results:</span>
<span id="cb28-4"><a href="#cb28-4"></a>    <span class="bu">print</span>(i, end<span class="op">=</span><span class="st">&#39;</span><span class="ch">\n</span><span class="st"> </span><span class="ch">\n</span><span class="st">&#39;</span>)</span></code></pre></div>
<pre><code>## The Afghan National Army reported that a mine explosion on the Kabul Ghouta district, at around 9:00am on Saturday night, killed at least 17 people and destroyed several properties. The blast took place as it was being
##  
## The Afghan National Army reported a bomb blast that killed at least six Afghan soldiers in Kabul yesterday morning, but the exact number is not known. However, officials of the Taliban have denied the attack and said no such incident took place
## </code></pre>
<div class="sourceCode" id="cb30"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb30-1"><a href="#cb30-1"></a><span class="co"># beam search</span></span>
<span id="cb30-2"><a href="#cb30-2"></a>text <span class="op">=</span> <span class="st">&quot;In Helmand Province, the Taliban&quot;</span></span>
<span id="cb30-3"><a href="#cb30-3"></a>ids <span class="op">=</span> torch.tensor(tokenizer.encode(text)).unsqueeze(<span class="dv">0</span>).cuda()</span>
<span id="cb30-4"><a href="#cb30-4"></a>generated_ids <span class="op">=</span> model.generate(</span>
<span id="cb30-5"><a href="#cb30-5"></a>                        input_ids<span class="op">=</span>ids,  <span class="co"># input</span></span>
<span id="cb30-6"><a href="#cb30-6"></a>                        max_length<span class="op">=</span><span class="dv">45</span>,  <span class="co"># default 20</span></span>
<span id="cb30-7"><a href="#cb30-7"></a>                        min_length<span class="op">=</span><span class="dv">0</span>,  <span class="co"># default 0</span></span>
<span id="cb30-8"><a href="#cb30-8"></a>                        do_sample<span class="op">=</span><span class="va">True</span>,  <span class="co"># don&#39;t use greedy decoding</span></span>
<span id="cb30-9"><a href="#cb30-9"></a>                        early_stopping<span class="op">=</span><span class="va">False</span>,  <span class="co"># search is stopped when at least num_beams sentences finished</span></span>
<span id="cb30-10"><a href="#cb30-10"></a>                        temperature<span class="op">=</span><span class="fl">2.45</span>,  <span class="co"># default 1.0</span></span>
<span id="cb30-11"><a href="#cb30-11"></a>                        top_k<span class="op">=</span><span class="dv">45</span>,  <span class="co"># default 50</span></span>
<span id="cb30-12"><a href="#cb30-12"></a>                        top_p<span class="op">=</span><span class="fl">0.7</span>,  <span class="co"># default 1.0</span></span>
<span id="cb30-13"><a href="#cb30-13"></a>                        repetition_penalty<span class="op">=</span><span class="fl">2.0</span>,  <span class="co"># rep. penalty</span></span>
<span id="cb30-14"><a href="#cb30-14"></a>                        num_beams<span class="op">=</span><span class="dv">6</span>,</span>
<span id="cb30-15"><a href="#cb30-15"></a>                        num_return_sequences<span class="op">=</span><span class="dv">2</span>, <span class="co">#  num ind. computed returned sequences</span></span>
<span id="cb30-16"><a href="#cb30-16"></a>                        bos_token_id<span class="op">=</span>tokenizer.bos_token_id</span>
<span id="cb30-17"><a href="#cb30-17"></a>                        )</span></code></pre></div>
<pre><code>## Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence</code></pre>
<div class="sourceCode" id="cb32"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb32-1"><a href="#cb32-1"></a>results <span class="op">=</span> [tokenizer.decode(g, skip_special_tokens<span class="op">=</span><span class="va">True</span>, clean_up_tokenization_spaces<span class="op">=</span><span class="va">True</span>) <span class="cf">for</span> g <span class="kw">in</span> generated_ids]</span>
<span id="cb32-2"><a href="#cb32-2"></a></span>
<span id="cb32-3"><a href="#cb32-3"></a><span class="cf">for</span> i <span class="kw">in</span> results:</span>
<span id="cb32-4"><a href="#cb32-4"></a>    <span class="bu">print</span>(i, end<span class="op">=</span><span class="st">&#39;</span><span class="ch">\n</span><span class="st"> </span><span class="ch">\n</span><span class="st">&#39;</span>)</span></code></pre></div>
<pre><code>## In Helmand Province, the Taliban have killed over 40 soldiers and more than 20 others in a brutal attack against security forces. On Monday, Afghan officials announced they had killed at least 22 policemen and dozens of other people in a
##  
## In Helmand Province, the Taliban and its supporters are fighting an Islamic-inspired attack in Kabul this month. (AP Photo/Umm Ismail Aliuddin)
## 
## 
## 
## 
## 
## 
## 
## 
## 
## 
## 
## 
## </code></pre>
</div>
</div>
<div id="sources" class="section level1" number="3">
<h1 number="3"><span class="header-section-number">3</span> Sources</h1>
<ul>
<li><a href="https://github.com/jcblaisecruz02/pytorch-nlp-tpu/tree/master/notebooks" class="uri">https://github.com/jcblaisecruz02/pytorch-nlp-tpu/tree/master/notebooks</a></li>
</ul>
</div>



<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>
