<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml">

<head>

<meta charset="utf-8" />
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="generator" content="pandoc" />

<meta name="viewport" content="width=device-width, initial-scale=1">

<meta name="author" content="Andrew Fogarty" />


<title>Classification: BERT-CNN Ensemble</title>



<style type="text/css">code{white-space: pre;}</style>
<style type="text/css" data-origin="pandoc">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */

</style>
<script>
// apply pandoc div.sourceCode style to pre.sourceCode instead
(function() {
  var sheets = document.styleSheets;
  for (var i = 0; i < sheets.length; i++) {
    if (sheets[i].ownerNode.dataset["origin"] !== "pandoc") continue;
    try { var rules = sheets[i].cssRules; } catch (e) { continue; }
    for (var j = 0; j < rules.length; j++) {
      var rule = rules[j];
      // check if there is a div.sourceCode rule
      if (rule.type !== rule.STYLE_RULE || rule.selectorText !== "div.sourceCode") continue;
      var style = rule.style.cssText;
      // check if color or background-color is set
      if (rule.style.color === '' || rule.style.backgroundColor === '') continue;
      // replace div.sourceCode by a pre.sourceCode rule
      sheets[i].deleteRule(j);
      sheets[i].insertRule('pre.sourceCode{' + style + '}', j);
    }
  }
})();
</script>



<style type="text/css">body {
background-color: #fff;
margin: 1em auto;
max-width: 700px;
overflow: visible;
padding-left: 2em;
padding-right: 2em;
font-family: "Open Sans", "Helvetica Neue", Helvetica, Arial, sans-serif;
font-size: 14px;
line-height: 1.35;
}
#header {
text-align: center;
}
#TOC {
clear: both;
margin: 0 0 10px 10px;
padding: 4px;
width: 400px;
border: 1px solid #CCCCCC;
border-radius: 5px;
background-color: #f6f6f6;
font-size: 13px;
line-height: 1.3;
}
#TOC .toctitle {
font-weight: bold;
font-size: 15px;
margin-left: 5px;
}
#TOC ul {
padding-left: 40px;
margin-left: -1.5em;
margin-top: 5px;
margin-bottom: 5px;
}
#TOC ul ul {
margin-left: -2em;
}
#TOC li {
line-height: 16px;
}
table {
margin: 1em auto;
border-width: 1px;
border-color: #DDDDDD;
border-style: outset;
border-collapse: collapse;
}
table th {
border-width: 2px;
padding: 5px;
border-style: inset;
}
table td {
border-width: 1px;
border-style: inset;
line-height: 18px;
padding: 5px 5px;
}
table, table th, table td {
border-left-style: none;
border-right-style: none;
}
table thead, table tr.even {
background-color: #f7f7f7;
}
p {
margin: 0.5em 0;
}
blockquote {
background-color: #f6f6f6;
padding: 0.25em 0.75em;
}
hr {
border-style: solid;
border: none;
border-top: 1px solid #777;
margin: 28px 0;
}
dl {
margin-left: 0;
}
dl dd {
margin-bottom: 13px;
margin-left: 13px;
}
dl dt {
font-weight: bold;
}
ul {
margin-top: 0;
}
ul li {
list-style: circle outside;
}
ul ul {
margin-bottom: 0;
}
pre, code {
background-color: #f7f7f7;
border-radius: 3px;
color: #333;
white-space: pre-wrap; 
}
pre {
border-radius: 3px;
margin: 5px 0px 10px 0px;
padding: 10px;
}
pre:not([class]) {
background-color: #f7f7f7;
}
code {
font-family: Consolas, Monaco, 'Courier New', monospace;
font-size: 85%;
}
p > code, li > code {
padding: 2px 0px;
}
div.figure {
text-align: center;
}
img {
background-color: #FFFFFF;
padding: 2px;
border: 1px solid #DDDDDD;
border-radius: 3px;
border: 1px solid #CCCCCC;
margin: 0 5px;
}
h1 {
margin-top: 0;
font-size: 35px;
line-height: 40px;
}
h2 {
border-bottom: 4px solid #f7f7f7;
padding-top: 10px;
padding-bottom: 2px;
font-size: 145%;
}
h3 {
border-bottom: 2px solid #f7f7f7;
padding-top: 10px;
font-size: 120%;
}
h4 {
border-bottom: 1px solid #f7f7f7;
margin-left: 8px;
font-size: 105%;
}
h5, h6 {
border-bottom: 1px solid #ccc;
font-size: 105%;
}
a {
color: #0033dd;
text-decoration: none;
}
a:hover {
color: #6666ff; }
a:visited {
color: #800080; }
a:visited:hover {
color: #BB00BB; }
a[href^="http:"] {
text-decoration: underline; }
a[href^="https:"] {
text-decoration: underline; }

code > span.kw { color: #555; font-weight: bold; } 
code > span.dt { color: #902000; } 
code > span.dv { color: #40a070; } 
code > span.bn { color: #d14; } 
code > span.fl { color: #d14; } 
code > span.ch { color: #d14; } 
code > span.st { color: #d14; } 
code > span.co { color: #888888; font-style: italic; } 
code > span.ot { color: #007020; } 
code > span.al { color: #ff0000; font-weight: bold; } 
code > span.fu { color: #900; font-weight: bold; }  code > span.er { color: #a61717; background-color: #e3d2d2; } 
</style>




</head>

<body>




<h1 class="title toc-ignore">Classification: BERT-CNN Ensemble</h1>
<h4 class="author">Andrew Fogarty</h4>
<h4 class="date">7/18/2020</h4>


<div id="TOC">
<ul>
<li><a href="#introduction"><span class="toc-section-number">1</span> Introduction</a>
<ul>
<li><a href="#prepare-data"><span class="toc-section-number">1.1</span> Prepare Data</a></li>
<li><a href="#tokenize"><span class="toc-section-number">1.2</span> Tokenize</a></li>
<li><a href="#create-tensor-data-sets"><span class="toc-section-number">1.3</span> Create Tensor Data Sets</a></li>
<li><a href="#samplers-and-helper-functions-for-imbalanced-classification"><span class="toc-section-number">1.4</span> Samplers and Helper Functions for Imbalanced Classification</a></li>
<li><a href="#prepare-data-loaders"><span class="toc-section-number">1.5</span> Prepare Data Loaders</a></li>
<li><a href="#prepare-modified-kim-cnn"><span class="toc-section-number">1.6</span> Prepare Modified Kim CNN</a></li>
<li><a href="#instantiate-training-functions"><span class="toc-section-number">1.7</span> Instantiate Training Functions</a></li>
<li><a href="#prepare-models-for-training"><span class="toc-section-number">1.8</span> Prepare Models for Training</a></li>
<li><a href="#train"><span class="toc-section-number">1.9</span> Train</a></li>
<li><a href="#show-results"><span class="toc-section-number">1.10</span> Show Results</a></li>
<li><a href="#test-the-model"><span class="toc-section-number">1.11</span> Test the Model</a></li>
</ul></li>
<li><a href="#sources"><span class="toc-section-number">2</span> Sources</a></li>
</ul>
</div>

<div class="sourceCode" id="cb1"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true"></a><span class="co"># load python</span></span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true"></a><span class="kw">library</span>(reticulate)</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true"></a><span class="kw">use_condaenv</span>(<span class="st">&quot;my_ml&quot;</span>)</span></code></pre></div>
<div class="sourceCode" id="cb2"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true"></a><span class="co"># load packages</span></span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true"></a><span class="im">import</span> torch</span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true"></a><span class="im">import</span> torch.nn <span class="im">as</span> nn</span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true"></a><span class="im">import</span> torch.nn.functional <span class="im">as</span> F</span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true"></a><span class="im">from</span> transformers <span class="im">import</span> get_linear_schedule_with_warmup, AdamW</span>
<span id="cb2-6"><a href="#cb2-6" aria-hidden="true"></a><span class="im">from</span> torch.utils.data <span class="im">import</span> TensorDataset, random_split, DataLoader, RandomSampler, SequentialSampler</span>
<span id="cb2-7"><a href="#cb2-7" aria-hidden="true"></a><span class="im">import</span> time, datetime, random, optuna, re, string</span>
<span id="cb2-8"><a href="#cb2-8" aria-hidden="true"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb2-9"><a href="#cb2-9" aria-hidden="true"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb2-10"><a href="#cb2-10" aria-hidden="true"></a><span class="im">from</span> sklearn.metrics <span class="im">import</span> accuracy_score, f1_score, precision_score, recall_score</span>
<span id="cb2-11"><a href="#cb2-11" aria-hidden="true"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb2-12"><a href="#cb2-12" aria-hidden="true"></a><span class="im">import</span> seaborn <span class="im">as</span> sns</span>
<span id="cb2-13"><a href="#cb2-13" aria-hidden="true"></a><span class="im">from</span> optuna.pruners <span class="im">import</span> SuccessiveHalvingPruner</span>
<span id="cb2-14"><a href="#cb2-14" aria-hidden="true"></a><span class="im">from</span> optuna.samplers <span class="im">import</span> TPESampler</span>
<span id="cb2-15"><a href="#cb2-15" aria-hidden="true"></a><span class="im">from</span> torch.cuda.amp <span class="im">import</span> autocast, GradScaler</span>
<span id="cb2-16"><a href="#cb2-16" aria-hidden="true"></a><span class="im">from</span> sklearn.model_selection <span class="im">import</span> train_test_split</span>
<span id="cb2-17"><a href="#cb2-17" aria-hidden="true"></a><span class="im">from</span> collections <span class="im">import</span> Counter</span>
<span id="cb2-18"><a href="#cb2-18" aria-hidden="true"></a><span class="im">from</span> transformers <span class="im">import</span> BertModel, BertTokenizer</span>
<span id="cb2-19"><a href="#cb2-19" aria-hidden="true"></a></span>
<span id="cb2-20"><a href="#cb2-20" aria-hidden="true"></a>SEED <span class="op">=</span> <span class="dv">15</span></span>
<span id="cb2-21"><a href="#cb2-21" aria-hidden="true"></a>random.seed(SEED)</span>
<span id="cb2-22"><a href="#cb2-22" aria-hidden="true"></a>np.random.seed(SEED)</span>
<span id="cb2-23"><a href="#cb2-23" aria-hidden="true"></a>torch.manual_seed(SEED)</span></code></pre></div>
<pre><code>## &lt;torch._C.Generator object at 0x000000002189E070&gt;</code></pre>
<div class="sourceCode" id="cb4"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true"></a>torch.backends.cudnn.deterministic <span class="op">=</span> <span class="va">True</span></span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true"></a>torch.cuda.amp.autocast(enabled<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true"></a></span>
<span id="cb4-4"><a href="#cb4-4" aria-hidden="true"></a></span>
<span id="cb4-5"><a href="#cb4-5" aria-hidden="true"></a><span class="co"># tell pytorch to use cuda</span></span></code></pre></div>
<pre><code>## &lt;torch.cuda.amp.autocast_mode.autocast object at 0x00000000347738C8&gt;</code></pre>
<div class="sourceCode" id="cb6"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true"></a>device <span class="op">=</span> torch.device(<span class="st">&quot;cuda&quot;</span>)</span></code></pre></div>
<div id="introduction" class="section level1" number="1">
<h1 number="1"><span class="header-section-number">1</span> Introduction</h1>
<p>In this guide, we prepare a BERT-CNN ensemble which takes the embeddings generated by the BERT base model and feeds them into a CNN. The general logic from this guide can be used to replace the CNN with any other NN of your choice. Future guides will explore other models like Bi-Directional LSTMs and the use of self-attention in embedding layer aggregation.</p>
<p>Like other guides, this walk through provides a complete treatment of the data preparation and training of the BERT-CNN ensemble in PyTorch.</p>
<div id="prepare-data" class="section level2" number="1.1">
<h2 number="1.1"><span class="header-section-number">1.1</span> Prepare Data</h2>
<p>We begin by loading and lightly editing our data prior to tokenization.</p>
<div class="sourceCode" id="cb7"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true"></a><span class="co"># prepare and load data</span></span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true"></a><span class="kw">def</span> prepare_df(pkl_location):</span>
<span id="cb7-3"><a href="#cb7-3" aria-hidden="true"></a>    <span class="co"># read pkl as pandas</span></span>
<span id="cb7-4"><a href="#cb7-4" aria-hidden="true"></a>    df <span class="op">=</span> pd.read_pickle(pkl_location)</span>
<span id="cb7-5"><a href="#cb7-5" aria-hidden="true"></a>    <span class="co"># just keep us/kabul labels</span></span>
<span id="cb7-6"><a href="#cb7-6" aria-hidden="true"></a>    df <span class="op">=</span> df.loc[(df[<span class="st">&#39;target&#39;</span>] <span class="op">==</span> <span class="st">&#39;US&#39;</span>) <span class="op">|</span> (df[<span class="st">&#39;target&#39;</span>] <span class="op">==</span> <span class="st">&#39;Kabul&#39;</span>)]</span>
<span id="cb7-7"><a href="#cb7-7" aria-hidden="true"></a>    <span class="co"># mask DV to recode</span></span>
<span id="cb7-8"><a href="#cb7-8" aria-hidden="true"></a>    us <span class="op">=</span> df[<span class="st">&#39;target&#39;</span>] <span class="op">==</span> <span class="st">&#39;US&#39;</span></span>
<span id="cb7-9"><a href="#cb7-9" aria-hidden="true"></a>    kabul <span class="op">=</span> df[<span class="st">&#39;target&#39;</span>] <span class="op">==</span> <span class="st">&#39;Kabul&#39;</span></span>
<span id="cb7-10"><a href="#cb7-10" aria-hidden="true"></a>    <span class="co"># apply mask</span></span>
<span id="cb7-11"><a href="#cb7-11" aria-hidden="true"></a>    df.loc[us, <span class="st">&#39;target&#39;</span>] <span class="op">=</span> <span class="dv">1</span></span>
<span id="cb7-12"><a href="#cb7-12" aria-hidden="true"></a>    df.loc[kabul, <span class="st">&#39;target&#39;</span>] <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb7-13"><a href="#cb7-13" aria-hidden="true"></a>    <span class="co"># reset index</span></span>
<span id="cb7-14"><a href="#cb7-14" aria-hidden="true"></a>    df <span class="op">=</span> df.reset_index(drop<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb7-15"><a href="#cb7-15" aria-hidden="true"></a>    <span class="cf">return</span> df</span>
<span id="cb7-16"><a href="#cb7-16" aria-hidden="true"></a></span>
<span id="cb7-17"><a href="#cb7-17" aria-hidden="true"></a></span>
<span id="cb7-18"><a href="#cb7-18" aria-hidden="true"></a>df <span class="op">=</span> prepare_df(<span class="st">&#39;C:</span><span class="ch">\\</span><span class="st">Users</span><span class="ch">\\</span><span class="st">Andrew</span><span class="ch">\\</span><span class="st">Desktop</span><span class="ch">\\</span><span class="st">df.pkl&#39;</span>)</span>
<span id="cb7-19"><a href="#cb7-19" aria-hidden="true"></a></span>
<span id="cb7-20"><a href="#cb7-20" aria-hidden="true"></a></span>
<span id="cb7-21"><a href="#cb7-21" aria-hidden="true"></a><span class="co"># prepare data</span></span>
<span id="cb7-22"><a href="#cb7-22" aria-hidden="true"></a><span class="kw">def</span> clean_df(df):</span>
<span id="cb7-23"><a href="#cb7-23" aria-hidden="true"></a>    <span class="co"># strip dash but keep a space</span></span>
<span id="cb7-24"><a href="#cb7-24" aria-hidden="true"></a>    df[<span class="st">&#39;body&#39;</span>] <span class="op">=</span> df[<span class="st">&#39;body&#39;</span>].<span class="bu">str</span>.replace(<span class="st">&#39;-&#39;</span>, <span class="st">&#39; &#39;</span>)</span>
<span id="cb7-25"><a href="#cb7-25" aria-hidden="true"></a>    <span class="co"># prepare keys for punctuation removal</span></span>
<span id="cb7-26"><a href="#cb7-26" aria-hidden="true"></a>    translator <span class="op">=</span> <span class="bu">str</span>.maketrans(<span class="bu">dict</span>.fromkeys(string.punctuation))</span>
<span id="cb7-27"><a href="#cb7-27" aria-hidden="true"></a>    <span class="co"># lower case the data</span></span>
<span id="cb7-28"><a href="#cb7-28" aria-hidden="true"></a>    df[<span class="st">&#39;body&#39;</span>] <span class="op">=</span> df[<span class="st">&#39;body&#39;</span>].<span class="bu">apply</span>(<span class="kw">lambda</span> x: x.lower())</span>
<span id="cb7-29"><a href="#cb7-29" aria-hidden="true"></a>    <span class="co"># remove excess spaces near punctuation</span></span>
<span id="cb7-30"><a href="#cb7-30" aria-hidden="true"></a>    df[<span class="st">&#39;body&#39;</span>] <span class="op">=</span> df[<span class="st">&#39;body&#39;</span>].<span class="bu">apply</span>(<span class="kw">lambda</span> x: re.sub(<span class="vs">r&#39;\s([?.!&quot;](?:\s|$))&#39;</span>, <span class="vs">r&#39;\1&#39;</span>, x))</span>
<span id="cb7-31"><a href="#cb7-31" aria-hidden="true"></a>    <span class="co"># remove punctuation  -- f1 improves by .05 by disabling this</span></span>
<span id="cb7-32"><a href="#cb7-32" aria-hidden="true"></a>    <span class="co">#df[&#39;body&#39;] = df[&#39;body&#39;].apply(lambda x: x.translate(translator))</span></span>
<span id="cb7-33"><a href="#cb7-33" aria-hidden="true"></a>    <span class="co"># generate a word count</span></span>
<span id="cb7-34"><a href="#cb7-34" aria-hidden="true"></a>    df[<span class="st">&#39;word_count&#39;</span>] <span class="op">=</span> df[<span class="st">&#39;body&#39;</span>].<span class="bu">apply</span>(<span class="kw">lambda</span> x: <span class="bu">len</span>(x.split()))</span>
<span id="cb7-35"><a href="#cb7-35" aria-hidden="true"></a>    <span class="co"># remove excess white spaces</span></span>
<span id="cb7-36"><a href="#cb7-36" aria-hidden="true"></a>    df[<span class="st">&#39;body&#39;</span>] <span class="op">=</span> df[<span class="st">&#39;body&#39;</span>].<span class="bu">apply</span>(<span class="kw">lambda</span> x: <span class="st">&quot; &quot;</span>.join(x.split()))</span>
<span id="cb7-37"><a href="#cb7-37" aria-hidden="true"></a></span>
<span id="cb7-38"><a href="#cb7-38" aria-hidden="true"></a>    <span class="cf">return</span> df</span>
<span id="cb7-39"><a href="#cb7-39" aria-hidden="true"></a></span>
<span id="cb7-40"><a href="#cb7-40" aria-hidden="true"></a></span>
<span id="cb7-41"><a href="#cb7-41" aria-hidden="true"></a>df <span class="op">=</span> clean_df(df)</span>
<span id="cb7-42"><a href="#cb7-42" aria-hidden="true"></a></span>
<span id="cb7-43"><a href="#cb7-43" aria-hidden="true"></a></span>
<span id="cb7-44"><a href="#cb7-44" aria-hidden="true"></a><span class="co"># lets remove rare words</span></span>
<span id="cb7-45"><a href="#cb7-45" aria-hidden="true"></a><span class="kw">def</span> remove_rare_words(df):</span>
<span id="cb7-46"><a href="#cb7-46" aria-hidden="true"></a>    <span class="co"># get counts of each word -- necessary for vocab</span></span>
<span id="cb7-47"><a href="#cb7-47" aria-hidden="true"></a>    counts <span class="op">=</span> Counter(<span class="st">&quot; &quot;</span>.join(df[<span class="st">&#39;body&#39;</span>].values.tolist()).split(<span class="st">&quot; &quot;</span>))</span>
<span id="cb7-48"><a href="#cb7-48" aria-hidden="true"></a>    <span class="co"># remove low counts -- keep those above 2</span></span>
<span id="cb7-49"><a href="#cb7-49" aria-hidden="true"></a>    counts <span class="op">=</span> {key: value <span class="cf">for</span> key, value <span class="kw">in</span> counts.items() <span class="cf">if</span> value <span class="op">&gt;</span> <span class="dv">2</span>}</span>
<span id="cb7-50"><a href="#cb7-50" aria-hidden="true"></a></span>
<span id="cb7-51"><a href="#cb7-51" aria-hidden="true"></a>    <span class="co"># remove rare words from corpus</span></span>
<span id="cb7-52"><a href="#cb7-52" aria-hidden="true"></a>    <span class="kw">def</span> remove_rare(x):</span>
<span id="cb7-53"><a href="#cb7-53" aria-hidden="true"></a>        <span class="cf">return</span> <span class="st">&#39; &#39;</span>.join(<span class="bu">list</span>(<span class="bu">filter</span>(<span class="kw">lambda</span> x: x <span class="kw">in</span> counts.keys(), x.split())))</span>
<span id="cb7-54"><a href="#cb7-54" aria-hidden="true"></a></span>
<span id="cb7-55"><a href="#cb7-55" aria-hidden="true"></a>    <span class="co"># apply funx</span></span>
<span id="cb7-56"><a href="#cb7-56" aria-hidden="true"></a>    df[<span class="st">&#39;body&#39;</span>] <span class="op">=</span> df[<span class="st">&#39;body&#39;</span>].<span class="bu">apply</span>(remove_rare)</span>
<span id="cb7-57"><a href="#cb7-57" aria-hidden="true"></a>    <span class="cf">return</span> df</span>
<span id="cb7-58"><a href="#cb7-58" aria-hidden="true"></a></span>
<span id="cb7-59"><a href="#cb7-59" aria-hidden="true"></a></span>
<span id="cb7-60"><a href="#cb7-60" aria-hidden="true"></a>df <span class="op">=</span> remove_rare_words(df)</span>
<span id="cb7-61"><a href="#cb7-61" aria-hidden="true"></a></span>
<span id="cb7-62"><a href="#cb7-62" aria-hidden="true"></a></span>
<span id="cb7-63"><a href="#cb7-63" aria-hidden="true"></a><span class="co"># remove transliterated words that GloVe can&#39;t find</span></span>
<span id="cb7-64"><a href="#cb7-64" aria-hidden="true"></a>no_matches <span class="op">=</span> np.load(<span class="st">&#39;C:</span><span class="ch">\\</span><span class="st">Users</span><span class="ch">\\</span><span class="st">Andrew</span><span class="ch">\\</span><span class="st">translit_no_match.npy&#39;</span>)</span>
<span id="cb7-65"><a href="#cb7-65" aria-hidden="true"></a>no_matches <span class="op">=</span> <span class="bu">dict</span>(<span class="bu">zip</span>(<span class="bu">set</span>(no_matches), <span class="bu">range</span>(<span class="bu">len</span>(<span class="bu">set</span>(no_matches)))))</span>
<span id="cb7-66"><a href="#cb7-66" aria-hidden="true"></a></span>
<span id="cb7-67"><a href="#cb7-67" aria-hidden="true"></a><span class="co"># remove transliterated words from corpus</span></span>
<span id="cb7-68"><a href="#cb7-68" aria-hidden="true"></a>df[<span class="st">&#39;body&#39;</span>] <span class="op">=</span> df[<span class="st">&#39;body&#39;</span>].<span class="bu">apply</span>(<span class="kw">lambda</span> x: <span class="st">&#39; &#39;</span>.join(<span class="bu">list</span>(<span class="bu">filter</span>(<span class="kw">lambda</span> x: x <span class="kw">not</span> <span class="kw">in</span> no_matches.keys(), x.split()))))</span></code></pre></div>
</div>
<div id="tokenize" class="section level2" number="1.2">
<h2 number="1.2"><span class="header-section-number">1.2</span> Tokenize</h2>
<p>Next, we instantiate the BERT tokenizer from <code>transformers</code> and tokenize our entire corpus.</p>
<div class="sourceCode" id="cb8"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true"></a><span class="co"># instantiate BERT tokenizer with upper + lower case</span></span>
<span id="cb8-2"><a href="#cb8-2" aria-hidden="true"></a>tokenizer <span class="op">=</span> BertTokenizer.from_pretrained(<span class="st">&#39;bert-base-uncased&#39;</span>)</span>
<span id="cb8-3"><a href="#cb8-3" aria-hidden="true"></a></span>
<span id="cb8-4"><a href="#cb8-4" aria-hidden="true"></a></span>
<span id="cb8-5"><a href="#cb8-5" aria-hidden="true"></a><span class="co"># a look at some of the BERT vocab</span></span></code></pre></div>
<pre><code>## 
Downloading:   0%|          | 0.00/232k [00:00&lt;?, ?B/s]
Downloading: 100%|##########| 232k/232k [00:00&lt;00:00, 7.99MB/s]</code></pre>
<div class="sourceCode" id="cb10"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true"></a>word_map <span class="op">=</span> <span class="bu">dict</span>(<span class="bu">zip</span>(tokenizer.vocab.keys(), <span class="bu">range</span>(<span class="bu">len</span>(tokenizer))))</span>
<span id="cb10-2"><a href="#cb10-2" aria-hidden="true"></a>word_map.get(<span class="st">&#39;the&#39;</span>)  <span class="co"># find index value</span></span></code></pre></div>
<pre><code>## 1996</code></pre>
<div class="sourceCode" id="cb12"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb12-1"><a href="#cb12-1" aria-hidden="true"></a><span class="bu">list</span>(tokenizer.vocab.keys())[<span class="dv">2000</span>:<span class="dv">2010</span>]</span></code></pre></div>
<pre><code>## [&#39;to&#39;, &#39;was&#39;, &#39;he&#39;, &#39;is&#39;, &#39;as&#39;, &#39;for&#39;, &#39;on&#39;, &#39;with&#39;, &#39;that&#39;, &#39;it&#39;]</code></pre>
<div class="sourceCode" id="cb14"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb14-1"><a href="#cb14-1" aria-hidden="true"></a><span class="bu">len</span>(tokenizer)</span></code></pre></div>
<pre><code>## 30522</code></pre>
<div class="sourceCode" id="cb16"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb16-1"><a href="#cb16-1" aria-hidden="true"></a><span class="co"># tokenize corpus using BERT</span></span>
<span id="cb16-2"><a href="#cb16-2" aria-hidden="true"></a><span class="kw">def</span> tokenize_corpus(df, tokenizer, max_len):</span>
<span id="cb16-3"><a href="#cb16-3" aria-hidden="true"></a>    <span class="co"># token ID storage</span></span>
<span id="cb16-4"><a href="#cb16-4" aria-hidden="true"></a>    input_ids <span class="op">=</span> []</span>
<span id="cb16-5"><a href="#cb16-5" aria-hidden="true"></a>    <span class="co"># attension mask storage</span></span>
<span id="cb16-6"><a href="#cb16-6" aria-hidden="true"></a>    attention_masks <span class="op">=</span> []</span>
<span id="cb16-7"><a href="#cb16-7" aria-hidden="true"></a>    <span class="co"># max len -- 512 is max</span></span>
<span id="cb16-8"><a href="#cb16-8" aria-hidden="true"></a>    max_len <span class="op">=</span> max_len</span>
<span id="cb16-9"><a href="#cb16-9" aria-hidden="true"></a>    <span class="co"># for every document:</span></span>
<span id="cb16-10"><a href="#cb16-10" aria-hidden="true"></a>    <span class="cf">for</span> doc <span class="kw">in</span> df:</span>
<span id="cb16-11"><a href="#cb16-11" aria-hidden="true"></a>        <span class="co"># `encode_plus` will:</span></span>
<span id="cb16-12"><a href="#cb16-12" aria-hidden="true"></a>        <span class="co">#   (1) Tokenize the sentence.</span></span>
<span id="cb16-13"><a href="#cb16-13" aria-hidden="true"></a>        <span class="co">#   (2) Prepend the `[CLS]` token to the start.</span></span>
<span id="cb16-14"><a href="#cb16-14" aria-hidden="true"></a>        <span class="co">#   (3) Append the `[SEP]` token to the end.</span></span>
<span id="cb16-15"><a href="#cb16-15" aria-hidden="true"></a>        <span class="co">#   (4) Map tokens to their IDs.</span></span>
<span id="cb16-16"><a href="#cb16-16" aria-hidden="true"></a>        <span class="co">#   (5) Pad or truncate the sentence to `max_length`</span></span>
<span id="cb16-17"><a href="#cb16-17" aria-hidden="true"></a>        <span class="co">#   (6) Create attention masks for [PAD] tokens.</span></span>
<span id="cb16-18"><a href="#cb16-18" aria-hidden="true"></a>        encoded_dict <span class="op">=</span> tokenizer.encode_plus(</span>
<span id="cb16-19"><a href="#cb16-19" aria-hidden="true"></a>                            doc,  <span class="co"># document to encode.</span></span>
<span id="cb16-20"><a href="#cb16-20" aria-hidden="true"></a>                            add_special_tokens<span class="op">=</span><span class="va">True</span>,  <span class="co"># add &#39;[CLS]&#39; and &#39;[SEP]&#39;</span></span>
<span id="cb16-21"><a href="#cb16-21" aria-hidden="true"></a>                            max_length<span class="op">=</span>max_len,  <span class="co"># set max length</span></span>
<span id="cb16-22"><a href="#cb16-22" aria-hidden="true"></a>                            truncation<span class="op">=</span><span class="va">True</span>,  <span class="co"># truncate longer messages</span></span>
<span id="cb16-23"><a href="#cb16-23" aria-hidden="true"></a>                            pad_to_max_length<span class="op">=</span><span class="va">True</span>,  <span class="co"># add padding</span></span>
<span id="cb16-24"><a href="#cb16-24" aria-hidden="true"></a>                            return_attention_mask<span class="op">=</span><span class="va">True</span>,  <span class="co"># create attn. masks</span></span>
<span id="cb16-25"><a href="#cb16-25" aria-hidden="true"></a>                            return_tensors<span class="op">=</span><span class="st">&#39;pt&#39;</span>  <span class="co"># return pytorch tensors</span></span>
<span id="cb16-26"><a href="#cb16-26" aria-hidden="true"></a>                       )</span>
<span id="cb16-27"><a href="#cb16-27" aria-hidden="true"></a></span>
<span id="cb16-28"><a href="#cb16-28" aria-hidden="true"></a>        <span class="co"># add the tokenized sentence to the list</span></span>
<span id="cb16-29"><a href="#cb16-29" aria-hidden="true"></a>        input_ids.append(encoded_dict[<span class="st">&#39;input_ids&#39;</span>])</span>
<span id="cb16-30"><a href="#cb16-30" aria-hidden="true"></a></span>
<span id="cb16-31"><a href="#cb16-31" aria-hidden="true"></a>        <span class="co"># and its attention mask (differentiates padding from non-padding)</span></span>
<span id="cb16-32"><a href="#cb16-32" aria-hidden="true"></a>        attention_masks.append(encoded_dict[<span class="st">&#39;attention_mask&#39;</span>])</span>
<span id="cb16-33"><a href="#cb16-33" aria-hidden="true"></a></span>
<span id="cb16-34"><a href="#cb16-34" aria-hidden="true"></a>    <span class="cf">return</span> torch.cat(input_ids, dim<span class="op">=</span><span class="dv">0</span>), torch.cat(attention_masks, dim<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb16-35"><a href="#cb16-35" aria-hidden="true"></a></span>
<span id="cb16-36"><a href="#cb16-36" aria-hidden="true"></a></span>
<span id="cb16-37"><a href="#cb16-37" aria-hidden="true"></a><span class="co"># create tokenized data</span></span>
<span id="cb16-38"><a href="#cb16-38" aria-hidden="true"></a>input_ids, attention_masks <span class="op">=</span> tokenize_corpus(df[<span class="st">&#39;body&#39;</span>].values, tokenizer, <span class="dv">512</span>)</span>
<span id="cb16-39"><a href="#cb16-39" aria-hidden="true"></a></span>
<span id="cb16-40"><a href="#cb16-40" aria-hidden="true"></a><span class="co"># convert the labels into tensors.</span></span>
<span id="cb16-41"><a href="#cb16-41" aria-hidden="true"></a>labels <span class="op">=</span> torch.tensor(df[<span class="st">&#39;target&#39;</span>].values.astype(np.float32))</span></code></pre></div>
</div>
<div id="create-tensor-data-sets" class="section level2" number="1.3">
<h2 number="1.3"><span class="header-section-number">1.3</span> Create Tensor Data Sets</h2>
<p>With the corpus tokenized, we now proceed to prepare our data for analysis in PyTorch. The code below creates a <code>TensorDataset</code> comprised of our features, attention masks, and our labels. It then proceeds to spit the data sets into train, validation, and test sets.</p>
<p>For our purposes, we will not actually use the labels as we are simply using the BERT transformer without any specific head on top. The CNN will be our head that we place on-top of the network.</p>
<div class="sourceCode" id="cb17"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb17-1"><a href="#cb17-1" aria-hidden="true"></a><span class="co"># prepare tensor data sets</span></span>
<span id="cb17-2"><a href="#cb17-2" aria-hidden="true"></a><span class="kw">def</span> prepare_dataset(padded_tokens, attention_masks, target):</span>
<span id="cb17-3"><a href="#cb17-3" aria-hidden="true"></a>    <span class="co"># prepare target into np array</span></span>
<span id="cb17-4"><a href="#cb17-4" aria-hidden="true"></a>    target <span class="op">=</span> np.array(target.values, dtype<span class="op">=</span>np.int64).reshape(<span class="op">-</span><span class="dv">1</span>, <span class="dv">1</span>)</span>
<span id="cb17-5"><a href="#cb17-5" aria-hidden="true"></a>    <span class="co"># create tensor data sets</span></span>
<span id="cb17-6"><a href="#cb17-6" aria-hidden="true"></a>    tensor_df <span class="op">=</span> TensorDataset(padded_tokens, attention_masks, torch.from_numpy(target))</span>
<span id="cb17-7"><a href="#cb17-7" aria-hidden="true"></a>    <span class="co"># 80% of df</span></span>
<span id="cb17-8"><a href="#cb17-8" aria-hidden="true"></a>    train_size <span class="op">=</span> <span class="bu">int</span>(<span class="fl">0.8</span> <span class="op">*</span> <span class="bu">len</span>(df))</span>
<span id="cb17-9"><a href="#cb17-9" aria-hidden="true"></a>    <span class="co"># 20% of df</span></span>
<span id="cb17-10"><a href="#cb17-10" aria-hidden="true"></a>    val_size <span class="op">=</span> <span class="bu">len</span>(df) <span class="op">-</span> train_size</span>
<span id="cb17-11"><a href="#cb17-11" aria-hidden="true"></a>    <span class="co"># 50% of validation</span></span>
<span id="cb17-12"><a href="#cb17-12" aria-hidden="true"></a>    test_size <span class="op">=</span> <span class="bu">int</span>(val_size <span class="op">-</span> <span class="fl">0.5</span><span class="op">*</span>val_size)</span>
<span id="cb17-13"><a href="#cb17-13" aria-hidden="true"></a>    <span class="co"># divide the dataset by randomly selecting samples</span></span>
<span id="cb17-14"><a href="#cb17-14" aria-hidden="true"></a>    train_dataset, val_dataset <span class="op">=</span> random_split(tensor_df, [train_size, val_size])</span>
<span id="cb17-15"><a href="#cb17-15" aria-hidden="true"></a>    <span class="co"># divide validation by randomly selecting samples</span></span>
<span id="cb17-16"><a href="#cb17-16" aria-hidden="true"></a>    val_dataset, test_dataset <span class="op">=</span> random_split(val_dataset, [test_size, test_size<span class="op">+</span><span class="dv">1</span>])</span>
<span id="cb17-17"><a href="#cb17-17" aria-hidden="true"></a></span>
<span id="cb17-18"><a href="#cb17-18" aria-hidden="true"></a>    <span class="cf">return</span> train_dataset, val_dataset, test_dataset</span>
<span id="cb17-19"><a href="#cb17-19" aria-hidden="true"></a></span>
<span id="cb17-20"><a href="#cb17-20" aria-hidden="true"></a></span>
<span id="cb17-21"><a href="#cb17-21" aria-hidden="true"></a><span class="co"># create tensor data sets</span></span>
<span id="cb17-22"><a href="#cb17-22" aria-hidden="true"></a>train_dataset, val_dataset, test_dataset <span class="op">=</span> prepare_dataset(input_ids,</span>
<span id="cb17-23"><a href="#cb17-23" aria-hidden="true"></a>                                                           attention_masks,</span>
<span id="cb17-24"><a href="#cb17-24" aria-hidden="true"></a>                                                           df[<span class="st">&#39;target&#39;</span>])</span></code></pre></div>
</div>
<div id="samplers-and-helper-functions-for-imbalanced-classification" class="section level2" number="1.4">
<h2 number="1.4"><span class="header-section-number">1.4</span> Samplers and Helper Functions for Imbalanced Classification</h2>
<p>Since my corpus is imbalanced, we produce weighted samplers to help balance the distribution of data as it is fed via the data loaders.</p>
<div class="sourceCode" id="cb18"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb18-1"><a href="#cb18-1" aria-hidden="true"></a><span class="co"># helper function to count target distribution inside tensor data sets</span></span>
<span id="cb18-2"><a href="#cb18-2" aria-hidden="true"></a><span class="kw">def</span> target_count(tensor_dataset):</span>
<span id="cb18-3"><a href="#cb18-3" aria-hidden="true"></a>    <span class="co"># set empty count containers</span></span>
<span id="cb18-4"><a href="#cb18-4" aria-hidden="true"></a>    count0 <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb18-5"><a href="#cb18-5" aria-hidden="true"></a>    count1 <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb18-6"><a href="#cb18-6" aria-hidden="true"></a>    <span class="co"># set total container to turn into torch tensor</span></span>
<span id="cb18-7"><a href="#cb18-7" aria-hidden="true"></a>    total <span class="op">=</span> []</span>
<span id="cb18-8"><a href="#cb18-8" aria-hidden="true"></a>    <span class="co"># for every item in the tensor data set</span></span>
<span id="cb18-9"><a href="#cb18-9" aria-hidden="true"></a>    <span class="cf">for</span> i <span class="kw">in</span> tensor_dataset:</span>
<span id="cb18-10"><a href="#cb18-10" aria-hidden="true"></a>        <span class="co"># if the target is equal to 0</span></span>
<span id="cb18-11"><a href="#cb18-11" aria-hidden="true"></a>        <span class="cf">if</span> i[<span class="dv">2</span>].item() <span class="op">==</span> <span class="dv">0</span>:</span>
<span id="cb18-12"><a href="#cb18-12" aria-hidden="true"></a>            count0 <span class="op">+=</span> <span class="dv">1</span></span>
<span id="cb18-13"><a href="#cb18-13" aria-hidden="true"></a>        <span class="co"># if the target is equal to 1</span></span>
<span id="cb18-14"><a href="#cb18-14" aria-hidden="true"></a>        <span class="cf">elif</span> i[<span class="dv">2</span>].item() <span class="op">==</span> <span class="dv">1</span>:</span>
<span id="cb18-15"><a href="#cb18-15" aria-hidden="true"></a>            count1 <span class="op">+=</span> <span class="dv">1</span></span>
<span id="cb18-16"><a href="#cb18-16" aria-hidden="true"></a>    total.append(count0)</span>
<span id="cb18-17"><a href="#cb18-17" aria-hidden="true"></a>    total.append(count1)</span>
<span id="cb18-18"><a href="#cb18-18" aria-hidden="true"></a>    <span class="cf">return</span> torch.tensor(total)</span>
<span id="cb18-19"><a href="#cb18-19" aria-hidden="true"></a></span>
<span id="cb18-20"><a href="#cb18-20" aria-hidden="true"></a><span class="co"># prepare weighted sampling for imbalanced classification</span></span>
<span id="cb18-21"><a href="#cb18-21" aria-hidden="true"></a><span class="kw">def</span> create_sampler(target_tensor, tensor_dataset):</span>
<span id="cb18-22"><a href="#cb18-22" aria-hidden="true"></a>    <span class="co"># generate class distributions [x, y]</span></span>
<span id="cb18-23"><a href="#cb18-23" aria-hidden="true"></a>    class_sample_count <span class="op">=</span> target_count(tensor_dataset)</span>
<span id="cb18-24"><a href="#cb18-24" aria-hidden="true"></a>    <span class="co"># weight</span></span>
<span id="cb18-25"><a href="#cb18-25" aria-hidden="true"></a>    weight <span class="op">=</span> <span class="fl">1.</span> <span class="op">/</span> class_sample_count.<span class="bu">float</span>()</span>
<span id="cb18-26"><a href="#cb18-26" aria-hidden="true"></a>    <span class="co"># produce weights for each observation in the data set</span></span>
<span id="cb18-27"><a href="#cb18-27" aria-hidden="true"></a>    samples_weight <span class="op">=</span> torch.tensor([weight[t[<span class="dv">2</span>]] <span class="cf">for</span> t <span class="kw">in</span> tensor_dataset])</span>
<span id="cb18-28"><a href="#cb18-28" aria-hidden="true"></a>    <span class="co"># prepare sampler</span></span>
<span id="cb18-29"><a href="#cb18-29" aria-hidden="true"></a>    sampler <span class="op">=</span> torch.utils.data.WeightedRandomSampler(weights<span class="op">=</span>samples_weight,</span>
<span id="cb18-30"><a href="#cb18-30" aria-hidden="true"></a>                                                     num_samples<span class="op">=</span><span class="bu">len</span>(samples_weight),</span>
<span id="cb18-31"><a href="#cb18-31" aria-hidden="true"></a>                                                     replacement<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb18-32"><a href="#cb18-32" aria-hidden="true"></a>    <span class="cf">return</span> sampler</span>
<span id="cb18-33"><a href="#cb18-33" aria-hidden="true"></a></span>
<span id="cb18-34"><a href="#cb18-34" aria-hidden="true"></a></span>
<span id="cb18-35"><a href="#cb18-35" aria-hidden="true"></a><span class="co"># create samplers for just the training set</span></span>
<span id="cb18-36"><a href="#cb18-36" aria-hidden="true"></a>train_sampler <span class="op">=</span> create_sampler(target_count(train_dataset), train_dataset)</span>
<span id="cb18-37"><a href="#cb18-37" aria-hidden="true"></a></span>
<span id="cb18-38"><a href="#cb18-38" aria-hidden="true"></a></span>
<span id="cb18-39"><a href="#cb18-39" aria-hidden="true"></a><span class="co"># time function</span></span>
<span id="cb18-40"><a href="#cb18-40" aria-hidden="true"></a><span class="kw">def</span> format_time(elapsed):</span>
<span id="cb18-41"><a href="#cb18-41" aria-hidden="true"></a>    <span class="co">&#39;&#39;&#39;</span></span>
<span id="cb18-42"><a href="#cb18-42" aria-hidden="true"></a><span class="co">    Takes a time in seconds and returns a string hh:mm:ss</span></span>
<span id="cb18-43"><a href="#cb18-43" aria-hidden="true"></a><span class="co">    &#39;&#39;&#39;</span></span>
<span id="cb18-44"><a href="#cb18-44" aria-hidden="true"></a>    <span class="co"># round to the nearest second.</span></span>
<span id="cb18-45"><a href="#cb18-45" aria-hidden="true"></a>    elapsed_rounded <span class="op">=</span> <span class="bu">int</span>(<span class="bu">round</span>((elapsed)))</span>
<span id="cb18-46"><a href="#cb18-46" aria-hidden="true"></a>    <span class="co"># format as hh:mm:ss</span></span>
<span id="cb18-47"><a href="#cb18-47" aria-hidden="true"></a>    <span class="cf">return</span> <span class="bu">str</span>(datetime.timedelta(seconds<span class="op">=</span>elapsed_rounded))</span></code></pre></div>
</div>
<div id="prepare-data-loaders" class="section level2" number="1.5">
<h2 number="1.5"><span class="header-section-number">1.5</span> Prepare Data Loaders</h2>
<p>Now we instantiate the data loaders.</p>
<div class="sourceCode" id="cb19"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb19-1"><a href="#cb19-1" aria-hidden="true"></a><span class="co"># create DataLoaders with samplers</span></span>
<span id="cb19-2"><a href="#cb19-2" aria-hidden="true"></a>train_dataloader <span class="op">=</span> DataLoader(train_dataset,</span>
<span id="cb19-3"><a href="#cb19-3" aria-hidden="true"></a>                              batch_size<span class="op">=</span><span class="dv">8</span>,</span>
<span id="cb19-4"><a href="#cb19-4" aria-hidden="true"></a>                              sampler<span class="op">=</span>train_sampler,</span>
<span id="cb19-5"><a href="#cb19-5" aria-hidden="true"></a>                              shuffle<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb19-6"><a href="#cb19-6" aria-hidden="true"></a></span>
<span id="cb19-7"><a href="#cb19-7" aria-hidden="true"></a>valid_dataloader <span class="op">=</span> DataLoader(val_dataset,</span>
<span id="cb19-8"><a href="#cb19-8" aria-hidden="true"></a>                              batch_size<span class="op">=</span><span class="dv">8</span>,</span>
<span id="cb19-9"><a href="#cb19-9" aria-hidden="true"></a>                              shuffle<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb19-10"><a href="#cb19-10" aria-hidden="true"></a></span>
<span id="cb19-11"><a href="#cb19-11" aria-hidden="true"></a>test_dataloader <span class="op">=</span> DataLoader(test_dataset,</span>
<span id="cb19-12"><a href="#cb19-12" aria-hidden="true"></a>                              batch_size<span class="op">=</span><span class="dv">8</span>,</span>
<span id="cb19-13"><a href="#cb19-13" aria-hidden="true"></a>                              shuffle<span class="op">=</span><span class="va">True</span>)</span></code></pre></div>
</div>
<div id="prepare-modified-kim-cnn" class="section level2" number="1.6">
<h2 number="1.6"><span class="header-section-number">1.6</span> Prepare Modified Kim CNN</h2>
<p>Here, we modify the previously used CNN class. We strip out the <code>nn.Embedding</code> layers as we are no longer providing a look-up table for embedding vectors. Instead, we are injecting the embedding vectors directly into the CNN from BERT.</p>
<div class="sourceCode" id="cb20"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb20-1"><a href="#cb20-1" aria-hidden="true"></a><span class="co"># Build Kim Yoon CNN</span></span>
<span id="cb20-2"><a href="#cb20-2" aria-hidden="true"></a><span class="kw">class</span> KimCNN(nn.Module):</span>
<span id="cb20-3"><a href="#cb20-3" aria-hidden="true"></a></span>
<span id="cb20-4"><a href="#cb20-4" aria-hidden="true"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, config):</span>
<span id="cb20-5"><a href="#cb20-5" aria-hidden="true"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb20-6"><a href="#cb20-6" aria-hidden="true"></a>        output_channel <span class="op">=</span> config.output_channel  <span class="co"># number of kernels</span></span>
<span id="cb20-7"><a href="#cb20-7" aria-hidden="true"></a>        num_classes <span class="op">=</span> config.num_classes  <span class="co"># number of targets to predict</span></span>
<span id="cb20-8"><a href="#cb20-8" aria-hidden="true"></a>        dropout <span class="op">=</span> config.dropout  <span class="co"># dropout value</span></span>
<span id="cb20-9"><a href="#cb20-9" aria-hidden="true"></a>        embedding_dim <span class="op">=</span> config.embedding_dim  <span class="co"># length of embedding dim</span></span>
<span id="cb20-10"><a href="#cb20-10" aria-hidden="true"></a></span>
<span id="cb20-11"><a href="#cb20-11" aria-hidden="true"></a>        ks <span class="op">=</span> <span class="dv">3</span>  <span class="co"># three conv nets here</span></span>
<span id="cb20-12"><a href="#cb20-12" aria-hidden="true"></a></span>
<span id="cb20-13"><a href="#cb20-13" aria-hidden="true"></a>        <span class="co"># input_channel = word embeddings at a value of 1; 3 for RGB images</span></span>
<span id="cb20-14"><a href="#cb20-14" aria-hidden="true"></a>        input_channel <span class="op">=</span> <span class="dv">4</span>  <span class="co"># for single embedding, input_channel = 1</span></span>
<span id="cb20-15"><a href="#cb20-15" aria-hidden="true"></a></span>
<span id="cb20-16"><a href="#cb20-16" aria-hidden="true"></a>        <span class="co"># [3, 4, 5] = window height</span></span>
<span id="cb20-17"><a href="#cb20-17" aria-hidden="true"></a>        <span class="co"># padding = padding to account for height of search window</span></span>
<span id="cb20-18"><a href="#cb20-18" aria-hidden="true"></a></span>
<span id="cb20-19"><a href="#cb20-19" aria-hidden="true"></a>        <span class="co"># 3 convolutional nets</span></span>
<span id="cb20-20"><a href="#cb20-20" aria-hidden="true"></a>        <span class="va">self</span>.conv1 <span class="op">=</span> nn.Conv2d(input_channel, output_channel, (<span class="dv">3</span>, embedding_dim), padding<span class="op">=</span>(<span class="dv">2</span>, <span class="dv">0</span>), groups<span class="op">=</span><span class="dv">4</span>)</span>
<span id="cb20-21"><a href="#cb20-21" aria-hidden="true"></a>        <span class="va">self</span>.conv2 <span class="op">=</span> nn.Conv2d(input_channel, output_channel, (<span class="dv">4</span>, embedding_dim), padding<span class="op">=</span>(<span class="dv">3</span>, <span class="dv">0</span>), groups<span class="op">=</span><span class="dv">4</span>)</span>
<span id="cb20-22"><a href="#cb20-22" aria-hidden="true"></a>        <span class="va">self</span>.conv3 <span class="op">=</span> nn.Conv2d(input_channel, output_channel, (<span class="dv">5</span>, embedding_dim), padding<span class="op">=</span>(<span class="dv">4</span>, <span class="dv">0</span>), groups<span class="op">=</span><span class="dv">4</span>)</span>
<span id="cb20-23"><a href="#cb20-23" aria-hidden="true"></a></span>
<span id="cb20-24"><a href="#cb20-24" aria-hidden="true"></a>        <span class="co"># apply dropout</span></span>
<span id="cb20-25"><a href="#cb20-25" aria-hidden="true"></a>        <span class="va">self</span>.dropout <span class="op">=</span> nn.Dropout(dropout)</span>
<span id="cb20-26"><a href="#cb20-26" aria-hidden="true"></a></span>
<span id="cb20-27"><a href="#cb20-27" aria-hidden="true"></a>        <span class="co"># fully connected layer for classification</span></span>
<span id="cb20-28"><a href="#cb20-28" aria-hidden="true"></a>        <span class="co"># 3x conv nets * output channel</span></span>
<span id="cb20-29"><a href="#cb20-29" aria-hidden="true"></a>        <span class="va">self</span>.fc1 <span class="op">=</span> nn.Linear(ks <span class="op">*</span> output_channel, num_classes)</span>
<span id="cb20-30"><a href="#cb20-30" aria-hidden="true"></a></span>
<span id="cb20-31"><a href="#cb20-31" aria-hidden="true"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x, <span class="op">**</span>kwargs):</span>
<span id="cb20-32"><a href="#cb20-32" aria-hidden="true"></a>        <span class="co">#x = x.unsqueeze(1)  # get another dimension at first index pos</span></span>
<span id="cb20-33"><a href="#cb20-33" aria-hidden="true"></a>        <span class="co"># squeeze to get size; (batch, channel_output, ~=sent_len) * ks</span></span>
<span id="cb20-34"><a href="#cb20-34" aria-hidden="true"></a>        x <span class="op">=</span> [F.relu(<span class="va">self</span>.conv1(x)).squeeze(<span class="dv">3</span>), F.relu(<span class="va">self</span>.conv2(x)).squeeze(<span class="dv">3</span>), F.relu(<span class="va">self</span>.conv3(x)).squeeze(<span class="dv">3</span>)]</span>
<span id="cb20-35"><a href="#cb20-35" aria-hidden="true"></a>        <span class="co"># max-over-time pooling; # (batch, channel_output) * ks</span></span>
<span id="cb20-36"><a href="#cb20-36" aria-hidden="true"></a>        x <span class="op">=</span> [F.max_pool1d(i, i.size(<span class="dv">2</span>)).squeeze(<span class="dv">2</span>) <span class="cf">for</span> i <span class="kw">in</span> x]</span>
<span id="cb20-37"><a href="#cb20-37" aria-hidden="true"></a>        <span class="co"># concat results; (batch, channel_output * ks)</span></span>
<span id="cb20-38"><a href="#cb20-38" aria-hidden="true"></a>        x <span class="op">=</span> torch.cat(x, <span class="dv">1</span>)</span>
<span id="cb20-39"><a href="#cb20-39" aria-hidden="true"></a>        <span class="co"># add dropout</span></span>
<span id="cb20-40"><a href="#cb20-40" aria-hidden="true"></a>        x <span class="op">=</span> <span class="va">self</span>.dropout(x)</span>
<span id="cb20-41"><a href="#cb20-41" aria-hidden="true"></a>        <span class="co"># generate logits (batch, target_size)</span></span>
<span id="cb20-42"><a href="#cb20-42" aria-hidden="true"></a>        logit <span class="op">=</span> <span class="va">self</span>.fc1(x)</span>
<span id="cb20-43"><a href="#cb20-43" aria-hidden="true"></a>        <span class="cf">return</span> logit</span></code></pre></div>
</div>
<div id="instantiate-training-functions" class="section level2" number="1.7">
<h2 number="1.7"><span class="header-section-number">1.7</span> Instantiate Training Functions</h2>
<p>Now, we prepare functions to train, validate, and test our data.</p>
<div class="sourceCode" id="cb21"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb21-1"><a href="#cb21-1" aria-hidden="true"></a><span class="kw">def</span> train(model, dataloader, optimizer):</span>
<span id="cb21-2"><a href="#cb21-2" aria-hidden="true"></a></span>
<span id="cb21-3"><a href="#cb21-3" aria-hidden="true"></a>    <span class="co"># capture time</span></span>
<span id="cb21-4"><a href="#cb21-4" aria-hidden="true"></a>    total_t0 <span class="op">=</span> time.time()</span>
<span id="cb21-5"><a href="#cb21-5" aria-hidden="true"></a></span>
<span id="cb21-6"><a href="#cb21-6" aria-hidden="true"></a>    <span class="co"># Perform one full pass over the training set.</span></span>
<span id="cb21-7"><a href="#cb21-7" aria-hidden="true"></a>    <span class="bu">print</span>(<span class="st">&quot;&quot;</span>)</span>
<span id="cb21-8"><a href="#cb21-8" aria-hidden="true"></a>    <span class="bu">print</span>(<span class="st">&#39;======== Epoch </span><span class="sc">{:}</span><span class="st"> / </span><span class="sc">{:}</span><span class="st"> ========&#39;</span>.<span class="bu">format</span>(epoch <span class="op">+</span> <span class="dv">1</span>, epochs))</span>
<span id="cb21-9"><a href="#cb21-9" aria-hidden="true"></a>    <span class="bu">print</span>(<span class="st">&#39;Training...&#39;</span>)</span>
<span id="cb21-10"><a href="#cb21-10" aria-hidden="true"></a></span>
<span id="cb21-11"><a href="#cb21-11" aria-hidden="true"></a>    <span class="co"># reset total loss for epoch</span></span>
<span id="cb21-12"><a href="#cb21-12" aria-hidden="true"></a>    train_total_loss <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb21-13"><a href="#cb21-13" aria-hidden="true"></a>    total_train_f1 <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb21-14"><a href="#cb21-14" aria-hidden="true"></a></span>
<span id="cb21-15"><a href="#cb21-15" aria-hidden="true"></a>    <span class="co"># put both models into traning mode</span></span>
<span id="cb21-16"><a href="#cb21-16" aria-hidden="true"></a>    model.train()</span>
<span id="cb21-17"><a href="#cb21-17" aria-hidden="true"></a>    kim_model.train()</span>
<span id="cb21-18"><a href="#cb21-18" aria-hidden="true"></a></span>
<span id="cb21-19"><a href="#cb21-19" aria-hidden="true"></a>    <span class="co"># for each batch of training data...</span></span>
<span id="cb21-20"><a href="#cb21-20" aria-hidden="true"></a>    <span class="cf">for</span> step, batch <span class="kw">in</span> <span class="bu">enumerate</span>(dataloader):</span>
<span id="cb21-21"><a href="#cb21-21" aria-hidden="true"></a></span>
<span id="cb21-22"><a href="#cb21-22" aria-hidden="true"></a>        <span class="co"># progress update every 40 batches.</span></span>
<span id="cb21-23"><a href="#cb21-23" aria-hidden="true"></a>        <span class="cf">if</span> step <span class="op">%</span> <span class="dv">40</span> <span class="op">==</span> <span class="dv">0</span> <span class="kw">and</span> <span class="kw">not</span> step <span class="op">==</span> <span class="dv">0</span>:</span>
<span id="cb21-24"><a href="#cb21-24" aria-hidden="true"></a></span>
<span id="cb21-25"><a href="#cb21-25" aria-hidden="true"></a>            <span class="co"># Report progress.</span></span>
<span id="cb21-26"><a href="#cb21-26" aria-hidden="true"></a>            <span class="bu">print</span>(<span class="st">&#39;  Batch {:&gt;5,}  of  {:&gt;5,}.&#39;</span>.<span class="bu">format</span>(step, <span class="bu">len</span>(dataloader)))</span>
<span id="cb21-27"><a href="#cb21-27" aria-hidden="true"></a></span>
<span id="cb21-28"><a href="#cb21-28" aria-hidden="true"></a>        <span class="co"># Unpack this training batch from our dataloader:</span></span>
<span id="cb21-29"><a href="#cb21-29" aria-hidden="true"></a>        <span class="co">#</span></span>
<span id="cb21-30"><a href="#cb21-30" aria-hidden="true"></a>        <span class="co"># As we unpack the batch, we&#39;ll also copy each tensor to the GPU</span></span>
<span id="cb21-31"><a href="#cb21-31" aria-hidden="true"></a>        <span class="co">#</span></span>
<span id="cb21-32"><a href="#cb21-32" aria-hidden="true"></a>        <span class="co"># `batch` contains three pytorch tensors:</span></span>
<span id="cb21-33"><a href="#cb21-33" aria-hidden="true"></a>        <span class="co">#   [0]: input ids</span></span>
<span id="cb21-34"><a href="#cb21-34" aria-hidden="true"></a>        <span class="co">#   [1]: attention masks</span></span>
<span id="cb21-35"><a href="#cb21-35" aria-hidden="true"></a>        <span class="co">#   [2]: labels</span></span>
<span id="cb21-36"><a href="#cb21-36" aria-hidden="true"></a>        b_input_ids <span class="op">=</span> batch[<span class="dv">0</span>].cuda()</span>
<span id="cb21-37"><a href="#cb21-37" aria-hidden="true"></a>        b_input_mask <span class="op">=</span> batch[<span class="dv">1</span>].cuda()</span>
<span id="cb21-38"><a href="#cb21-38" aria-hidden="true"></a>        b_labels <span class="op">=</span> batch[<span class="dv">2</span>].cuda().<span class="bu">long</span>()</span>
<span id="cb21-39"><a href="#cb21-39" aria-hidden="true"></a></span>
<span id="cb21-40"><a href="#cb21-40" aria-hidden="true"></a>        <span class="co"># clear previously calculated gradients</span></span>
<span id="cb21-41"><a href="#cb21-41" aria-hidden="true"></a>        optimizer.zero_grad()</span>
<span id="cb21-42"><a href="#cb21-42" aria-hidden="true"></a></span>
<span id="cb21-43"><a href="#cb21-43" aria-hidden="true"></a>        <span class="co"># runs the forward pass with autocasting.</span></span>
<span id="cb21-44"><a href="#cb21-44" aria-hidden="true"></a>        <span class="cf">with</span> autocast():</span>
<span id="cb21-45"><a href="#cb21-45" aria-hidden="true"></a>            <span class="co"># forward propagation (evaluate model on training batch)</span></span>
<span id="cb21-46"><a href="#cb21-46" aria-hidden="true"></a>            outputs <span class="op">=</span> model(input_ids<span class="op">=</span>b_input_ids, attention_mask<span class="op">=</span>b_input_mask)</span>
<span id="cb21-47"><a href="#cb21-47" aria-hidden="true"></a></span>
<span id="cb21-48"><a href="#cb21-48" aria-hidden="true"></a>            hidden_layers <span class="op">=</span> outputs[<span class="dv">2</span>]  <span class="co"># get hidden layers</span></span>
<span id="cb21-49"><a href="#cb21-49" aria-hidden="true"></a></span>
<span id="cb21-50"><a href="#cb21-50" aria-hidden="true"></a>            hidden_layers <span class="op">=</span> torch.stack(hidden_layers, dim<span class="op">=</span><span class="dv">1</span>)  <span class="co"># stack the layers</span></span>
<span id="cb21-51"><a href="#cb21-51" aria-hidden="true"></a></span>
<span id="cb21-52"><a href="#cb21-52" aria-hidden="true"></a>            hidden_layers <span class="op">=</span> hidden_layers[:, <span class="op">-</span><span class="dv">4</span>:]  <span class="co"># get the last 4 layers</span></span>
<span id="cb21-53"><a href="#cb21-53" aria-hidden="true"></a></span>
<span id="cb21-54"><a href="#cb21-54" aria-hidden="true"></a>        logits <span class="op">=</span> kim_model(hidden_layers)</span>
<span id="cb21-55"><a href="#cb21-55" aria-hidden="true"></a></span>
<span id="cb21-56"><a href="#cb21-56" aria-hidden="true"></a>        loss <span class="op">=</span> criterion(logits.view(<span class="op">-</span><span class="dv">1</span>, <span class="dv">2</span>), b_labels.view(<span class="op">-</span><span class="dv">1</span>))</span>
<span id="cb21-57"><a href="#cb21-57" aria-hidden="true"></a></span>
<span id="cb21-58"><a href="#cb21-58" aria-hidden="true"></a>        <span class="co"># sum the training loss over all batches for average loss at end</span></span>
<span id="cb21-59"><a href="#cb21-59" aria-hidden="true"></a>        <span class="co"># loss is a tensor containing a single value</span></span>
<span id="cb21-60"><a href="#cb21-60" aria-hidden="true"></a>        train_total_loss <span class="op">+=</span> loss.item()</span>
<span id="cb21-61"><a href="#cb21-61" aria-hidden="true"></a></span>
<span id="cb21-62"><a href="#cb21-62" aria-hidden="true"></a>        <span class="co"># Scales loss. Calls backward() on scaled loss to create scaled gradients.</span></span>
<span id="cb21-63"><a href="#cb21-63" aria-hidden="true"></a>        <span class="co"># Backward passes under autocast are not recommended.</span></span>
<span id="cb21-64"><a href="#cb21-64" aria-hidden="true"></a>        <span class="co"># Backward ops run in the same dtype autocast chose for corresponding forward ops.</span></span>
<span id="cb21-65"><a href="#cb21-65" aria-hidden="true"></a>        scaler.scale(loss).backward()</span>
<span id="cb21-66"><a href="#cb21-66" aria-hidden="true"></a></span>
<span id="cb21-67"><a href="#cb21-67" aria-hidden="true"></a>        <span class="co"># scaler.step() first unscales the gradients of the optimizer&#39;s assigned params.</span></span>
<span id="cb21-68"><a href="#cb21-68" aria-hidden="true"></a>        <span class="co"># If these gradients do not contain infs or NaNs, optimizer.step() is then called,</span></span>
<span id="cb21-69"><a href="#cb21-69" aria-hidden="true"></a>        <span class="co"># otherwise, optimizer.step() is skipped.</span></span>
<span id="cb21-70"><a href="#cb21-70" aria-hidden="true"></a>        scaler.step(optimizer)</span>
<span id="cb21-71"><a href="#cb21-71" aria-hidden="true"></a></span>
<span id="cb21-72"><a href="#cb21-72" aria-hidden="true"></a>        <span class="co"># Updates the scale for next iteration.</span></span>
<span id="cb21-73"><a href="#cb21-73" aria-hidden="true"></a>        scaler.update()</span>
<span id="cb21-74"><a href="#cb21-74" aria-hidden="true"></a></span>
<span id="cb21-75"><a href="#cb21-75" aria-hidden="true"></a>        <span class="co"># Update the scheduler</span></span>
<span id="cb21-76"><a href="#cb21-76" aria-hidden="true"></a>        scheduler.step()</span>
<span id="cb21-77"><a href="#cb21-77" aria-hidden="true"></a></span>
<span id="cb21-78"><a href="#cb21-78" aria-hidden="true"></a>        <span class="co"># calculate preds</span></span>
<span id="cb21-79"><a href="#cb21-79" aria-hidden="true"></a>        _, predicted <span class="op">=</span> torch.<span class="bu">max</span>(logits, <span class="dv">1</span>)</span>
<span id="cb21-80"><a href="#cb21-80" aria-hidden="true"></a></span>
<span id="cb21-81"><a href="#cb21-81" aria-hidden="true"></a>        <span class="co"># move logits and labels to CPU</span></span>
<span id="cb21-82"><a href="#cb21-82" aria-hidden="true"></a>        predicted <span class="op">=</span> predicted.detach().cpu().numpy()</span>
<span id="cb21-83"><a href="#cb21-83" aria-hidden="true"></a>        y_true <span class="op">=</span> b_labels.detach().cpu().numpy()</span>
<span id="cb21-84"><a href="#cb21-84" aria-hidden="true"></a></span>
<span id="cb21-85"><a href="#cb21-85" aria-hidden="true"></a>        <span class="co"># calculate f1</span></span>
<span id="cb21-86"><a href="#cb21-86" aria-hidden="true"></a>        total_train_f1 <span class="op">+=</span> f1_score(predicted, y_true,</span>
<span id="cb21-87"><a href="#cb21-87" aria-hidden="true"></a>                                   average<span class="op">=</span><span class="st">&#39;weighted&#39;</span>,</span>
<span id="cb21-88"><a href="#cb21-88" aria-hidden="true"></a>                                   labels<span class="op">=</span>np.unique(predicted))</span>
<span id="cb21-89"><a href="#cb21-89" aria-hidden="true"></a></span>
<span id="cb21-90"><a href="#cb21-90" aria-hidden="true"></a>    <span class="co"># calculate the average loss over all of the batches</span></span>
<span id="cb21-91"><a href="#cb21-91" aria-hidden="true"></a>    avg_train_loss <span class="op">=</span> train_total_loss <span class="op">/</span> <span class="bu">len</span>(dataloader)</span>
<span id="cb21-92"><a href="#cb21-92" aria-hidden="true"></a></span>
<span id="cb21-93"><a href="#cb21-93" aria-hidden="true"></a>    <span class="co"># calculate the average f1 over all of the batches</span></span>
<span id="cb21-94"><a href="#cb21-94" aria-hidden="true"></a>    avg_train_f1 <span class="op">=</span> total_train_f1 <span class="op">/</span> <span class="bu">len</span>(dataloader)</span>
<span id="cb21-95"><a href="#cb21-95" aria-hidden="true"></a></span>
<span id="cb21-96"><a href="#cb21-96" aria-hidden="true"></a>    <span class="co"># training time end</span></span>
<span id="cb21-97"><a href="#cb21-97" aria-hidden="true"></a>    training_time <span class="op">=</span> format_time(time.time() <span class="op">-</span> total_t0)</span>
<span id="cb21-98"><a href="#cb21-98" aria-hidden="true"></a></span>
<span id="cb21-99"><a href="#cb21-99" aria-hidden="true"></a>    <span class="co"># Record all statistics from this epoch.</span></span>
<span id="cb21-100"><a href="#cb21-100" aria-hidden="true"></a>    training_stats.append(</span>
<span id="cb21-101"><a href="#cb21-101" aria-hidden="true"></a>        {</span>
<span id="cb21-102"><a href="#cb21-102" aria-hidden="true"></a>            <span class="st">&#39;Train Loss&#39;</span>: avg_train_loss,</span>
<span id="cb21-103"><a href="#cb21-103" aria-hidden="true"></a>            <span class="st">&#39;Train F1&#39;</span>: avg_train_f1,</span>
<span id="cb21-104"><a href="#cb21-104" aria-hidden="true"></a>            <span class="st">&#39;Train Time&#39;</span>: training_time</span>
<span id="cb21-105"><a href="#cb21-105" aria-hidden="true"></a>        }</span>
<span id="cb21-106"><a href="#cb21-106" aria-hidden="true"></a>    )</span>
<span id="cb21-107"><a href="#cb21-107" aria-hidden="true"></a></span>
<span id="cb21-108"><a href="#cb21-108" aria-hidden="true"></a>    <span class="co"># print result summaries</span></span>
<span id="cb21-109"><a href="#cb21-109" aria-hidden="true"></a>    <span class="bu">print</span>(<span class="st">&quot;&quot;</span>)</span>
<span id="cb21-110"><a href="#cb21-110" aria-hidden="true"></a>    <span class="bu">print</span>(<span class="st">&quot;summary results&quot;</span>)</span>
<span id="cb21-111"><a href="#cb21-111" aria-hidden="true"></a>    <span class="bu">print</span>(<span class="st">&quot;epoch | trn loss | trn f1 | trn time &quot;</span>)</span>
<span id="cb21-112"><a href="#cb21-112" aria-hidden="true"></a>    <span class="bu">print</span>(<span class="ss">f&quot;</span><span class="sc">{</span>epoch<span class="op">+</span><span class="dv">1</span><span class="sc">:5d}</span><span class="ss"> | </span><span class="sc">{</span>avg_train_loss<span class="sc">:.5f}</span><span class="ss"> | </span><span class="sc">{</span>avg_train_f1<span class="sc">:.5f}</span><span class="ss"> | </span><span class="sc">{</span>training_time<span class="sc">:}</span><span class="ss">&quot;</span>)</span>
<span id="cb21-113"><a href="#cb21-113" aria-hidden="true"></a></span>
<span id="cb21-114"><a href="#cb21-114" aria-hidden="true"></a>    <span class="co">#torch.cuda.empty_cache()</span></span>
<span id="cb21-115"><a href="#cb21-115" aria-hidden="true"></a></span>
<span id="cb21-116"><a href="#cb21-116" aria-hidden="true"></a>    <span class="cf">return</span> <span class="va">None</span></span>
<span id="cb21-117"><a href="#cb21-117" aria-hidden="true"></a></span>
<span id="cb21-118"><a href="#cb21-118" aria-hidden="true"></a></span>
<span id="cb21-119"><a href="#cb21-119" aria-hidden="true"></a><span class="kw">def</span> validating(model, dataloader):</span>
<span id="cb21-120"><a href="#cb21-120" aria-hidden="true"></a></span>
<span id="cb21-121"><a href="#cb21-121" aria-hidden="true"></a>    <span class="co"># capture validation time</span></span>
<span id="cb21-122"><a href="#cb21-122" aria-hidden="true"></a>    total_t0 <span class="op">=</span> time.time()</span>
<span id="cb21-123"><a href="#cb21-123" aria-hidden="true"></a></span>
<span id="cb21-124"><a href="#cb21-124" aria-hidden="true"></a>    <span class="co"># After the completion of each training epoch, measure our performance on</span></span>
<span id="cb21-125"><a href="#cb21-125" aria-hidden="true"></a>    <span class="co"># our validation set.</span></span>
<span id="cb21-126"><a href="#cb21-126" aria-hidden="true"></a>    <span class="bu">print</span>(<span class="st">&quot;&quot;</span>)</span>
<span id="cb21-127"><a href="#cb21-127" aria-hidden="true"></a>    <span class="bu">print</span>(<span class="st">&quot;Running Validation...&quot;</span>)</span>
<span id="cb21-128"><a href="#cb21-128" aria-hidden="true"></a></span>
<span id="cb21-129"><a href="#cb21-129" aria-hidden="true"></a>    <span class="co"># put both models in evaluation mode</span></span>
<span id="cb21-130"><a href="#cb21-130" aria-hidden="true"></a>    model.<span class="bu">eval</span>()</span>
<span id="cb21-131"><a href="#cb21-131" aria-hidden="true"></a>    kim_model.<span class="bu">eval</span>()</span>
<span id="cb21-132"><a href="#cb21-132" aria-hidden="true"></a></span>
<span id="cb21-133"><a href="#cb21-133" aria-hidden="true"></a>    <span class="co"># track variables</span></span>
<span id="cb21-134"><a href="#cb21-134" aria-hidden="true"></a>    total_valid_accuracy <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb21-135"><a href="#cb21-135" aria-hidden="true"></a>    total_valid_loss <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb21-136"><a href="#cb21-136" aria-hidden="true"></a>    total_valid_f1 <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb21-137"><a href="#cb21-137" aria-hidden="true"></a>    total_valid_recall <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb21-138"><a href="#cb21-138" aria-hidden="true"></a>    total_valid_precision <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb21-139"><a href="#cb21-139" aria-hidden="true"></a>    total_bert_valid_loss <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb21-140"><a href="#cb21-140" aria-hidden="true"></a></span>
<span id="cb21-141"><a href="#cb21-141" aria-hidden="true"></a>    <span class="co"># evaluate data for one epoch</span></span>
<span id="cb21-142"><a href="#cb21-142" aria-hidden="true"></a>    <span class="cf">for</span> batch <span class="kw">in</span> dataloader:</span>
<span id="cb21-143"><a href="#cb21-143" aria-hidden="true"></a></span>
<span id="cb21-144"><a href="#cb21-144" aria-hidden="true"></a>        <span class="co"># Unpack this training batch from our dataloader:</span></span>
<span id="cb21-145"><a href="#cb21-145" aria-hidden="true"></a>        <span class="co"># `batch` contains three pytorch tensors:</span></span>
<span id="cb21-146"><a href="#cb21-146" aria-hidden="true"></a>        <span class="co">#   [0]: input ids</span></span>
<span id="cb21-147"><a href="#cb21-147" aria-hidden="true"></a>        <span class="co">#   [1]: attention masks</span></span>
<span id="cb21-148"><a href="#cb21-148" aria-hidden="true"></a>        <span class="co">#   [2]: labels</span></span>
<span id="cb21-149"><a href="#cb21-149" aria-hidden="true"></a>        b_input_ids <span class="op">=</span> batch[<span class="dv">0</span>].cuda()</span>
<span id="cb21-150"><a href="#cb21-150" aria-hidden="true"></a>        b_input_mask <span class="op">=</span> batch[<span class="dv">1</span>].cuda()</span>
<span id="cb21-151"><a href="#cb21-151" aria-hidden="true"></a>        b_labels <span class="op">=</span> batch[<span class="dv">2</span>].cuda().<span class="bu">long</span>()</span>
<span id="cb21-152"><a href="#cb21-152" aria-hidden="true"></a></span>
<span id="cb21-153"><a href="#cb21-153" aria-hidden="true"></a>        <span class="co"># tell pytorch not to bother calculating gradients</span></span>
<span id="cb21-154"><a href="#cb21-154" aria-hidden="true"></a>        <span class="cf">with</span> torch.no_grad():</span>
<span id="cb21-155"><a href="#cb21-155" aria-hidden="true"></a>            <span class="co"># forward propagation (evaluate model on training batch)</span></span>
<span id="cb21-156"><a href="#cb21-156" aria-hidden="true"></a>            outputs <span class="op">=</span> model(input_ids<span class="op">=</span>b_input_ids, attention_mask<span class="op">=</span>b_input_mask)</span>
<span id="cb21-157"><a href="#cb21-157" aria-hidden="true"></a></span>
<span id="cb21-158"><a href="#cb21-158" aria-hidden="true"></a>            hidden_layers <span class="op">=</span> outputs[<span class="dv">2</span>]  <span class="co"># get hidden layers</span></span>
<span id="cb21-159"><a href="#cb21-159" aria-hidden="true"></a></span>
<span id="cb21-160"><a href="#cb21-160" aria-hidden="true"></a>            hidden_layers <span class="op">=</span> torch.stack(hidden_layers, dim<span class="op">=</span><span class="dv">1</span>)  <span class="co"># stack the layers</span></span>
<span id="cb21-161"><a href="#cb21-161" aria-hidden="true"></a></span>
<span id="cb21-162"><a href="#cb21-162" aria-hidden="true"></a>            hidden_layers <span class="op">=</span> hidden_layers[:, <span class="op">-</span><span class="dv">4</span>:]  <span class="co"># get the last 4 layers</span></span>
<span id="cb21-163"><a href="#cb21-163" aria-hidden="true"></a></span>
<span id="cb21-164"><a href="#cb21-164" aria-hidden="true"></a>        logits <span class="op">=</span> kim_model(hidden_layers)</span>
<span id="cb21-165"><a href="#cb21-165" aria-hidden="true"></a></span>
<span id="cb21-166"><a href="#cb21-166" aria-hidden="true"></a>        loss <span class="op">=</span> criterion(logits.view(<span class="op">-</span><span class="dv">1</span>, <span class="dv">2</span>), b_labels.view(<span class="op">-</span><span class="dv">1</span>))</span>
<span id="cb21-167"><a href="#cb21-167" aria-hidden="true"></a></span>
<span id="cb21-168"><a href="#cb21-168" aria-hidden="true"></a>        <span class="co"># accumulate validation loss</span></span>
<span id="cb21-169"><a href="#cb21-169" aria-hidden="true"></a>        total_valid_loss <span class="op">+=</span> loss.item()</span>
<span id="cb21-170"><a href="#cb21-170" aria-hidden="true"></a></span>
<span id="cb21-171"><a href="#cb21-171" aria-hidden="true"></a>        <span class="co"># calculate preds</span></span>
<span id="cb21-172"><a href="#cb21-172" aria-hidden="true"></a>        _, predicted <span class="op">=</span> torch.<span class="bu">max</span>(logits, <span class="dv">1</span>)</span>
<span id="cb21-173"><a href="#cb21-173" aria-hidden="true"></a></span>
<span id="cb21-174"><a href="#cb21-174" aria-hidden="true"></a>        <span class="co"># move logits and labels to CPU</span></span>
<span id="cb21-175"><a href="#cb21-175" aria-hidden="true"></a>        predicted <span class="op">=</span> predicted.detach().cpu().numpy()</span>
<span id="cb21-176"><a href="#cb21-176" aria-hidden="true"></a>        y_true <span class="op">=</span> b_labels.detach().cpu().numpy()</span>
<span id="cb21-177"><a href="#cb21-177" aria-hidden="true"></a></span>
<span id="cb21-178"><a href="#cb21-178" aria-hidden="true"></a>        <span class="co"># calculate f1</span></span>
<span id="cb21-179"><a href="#cb21-179" aria-hidden="true"></a>        total_valid_f1 <span class="op">+=</span> f1_score(predicted, y_true,</span>
<span id="cb21-180"><a href="#cb21-180" aria-hidden="true"></a>                                   average<span class="op">=</span><span class="st">&#39;weighted&#39;</span>,</span>
<span id="cb21-181"><a href="#cb21-181" aria-hidden="true"></a>                                   labels<span class="op">=</span>np.unique(predicted))</span>
<span id="cb21-182"><a href="#cb21-182" aria-hidden="true"></a></span>
<span id="cb21-183"><a href="#cb21-183" aria-hidden="true"></a>        <span class="co"># calculate accuracy</span></span>
<span id="cb21-184"><a href="#cb21-184" aria-hidden="true"></a>        total_valid_accuracy <span class="op">+=</span> accuracy_score(predicted, y_true)</span>
<span id="cb21-185"><a href="#cb21-185" aria-hidden="true"></a></span>
<span id="cb21-186"><a href="#cb21-186" aria-hidden="true"></a>        <span class="co"># calculate precision</span></span>
<span id="cb21-187"><a href="#cb21-187" aria-hidden="true"></a>        total_valid_precision <span class="op">+=</span> precision_score(predicted, y_true,</span>
<span id="cb21-188"><a href="#cb21-188" aria-hidden="true"></a>                                                 average<span class="op">=</span><span class="st">&#39;weighted&#39;</span>,</span>
<span id="cb21-189"><a href="#cb21-189" aria-hidden="true"></a>                                                 labels<span class="op">=</span>np.unique(predicted))</span>
<span id="cb21-190"><a href="#cb21-190" aria-hidden="true"></a></span>
<span id="cb21-191"><a href="#cb21-191" aria-hidden="true"></a>        <span class="co"># calculate recall</span></span>
<span id="cb21-192"><a href="#cb21-192" aria-hidden="true"></a>        total_valid_recall <span class="op">+=</span> recall_score(predicted, y_true,</span>
<span id="cb21-193"><a href="#cb21-193" aria-hidden="true"></a>                                                 average<span class="op">=</span><span class="st">&#39;weighted&#39;</span>,</span>
<span id="cb21-194"><a href="#cb21-194" aria-hidden="true"></a>                                                 labels<span class="op">=</span>np.unique(predicted))</span>
<span id="cb21-195"><a href="#cb21-195" aria-hidden="true"></a></span>
<span id="cb21-196"><a href="#cb21-196" aria-hidden="true"></a>    <span class="co"># report final accuracy of validation run</span></span>
<span id="cb21-197"><a href="#cb21-197" aria-hidden="true"></a>    avg_accuracy <span class="op">=</span> total_valid_accuracy <span class="op">/</span> <span class="bu">len</span>(dataloader)</span>
<span id="cb21-198"><a href="#cb21-198" aria-hidden="true"></a></span>
<span id="cb21-199"><a href="#cb21-199" aria-hidden="true"></a>    <span class="co"># report final f1 of validation run</span></span>
<span id="cb21-200"><a href="#cb21-200" aria-hidden="true"></a>    <span class="kw">global</span> avg_val_f1</span>
<span id="cb21-201"><a href="#cb21-201" aria-hidden="true"></a>    avg_val_f1 <span class="op">=</span> total_valid_f1 <span class="op">/</span> <span class="bu">len</span>(dataloader)</span>
<span id="cb21-202"><a href="#cb21-202" aria-hidden="true"></a></span>
<span id="cb21-203"><a href="#cb21-203" aria-hidden="true"></a>    <span class="co"># report final f1 of validation run</span></span>
<span id="cb21-204"><a href="#cb21-204" aria-hidden="true"></a>    avg_precision <span class="op">=</span> total_valid_precision <span class="op">/</span> <span class="bu">len</span>(dataloader)</span>
<span id="cb21-205"><a href="#cb21-205" aria-hidden="true"></a></span>
<span id="cb21-206"><a href="#cb21-206" aria-hidden="true"></a>    <span class="co"># report final f1 of validation run</span></span>
<span id="cb21-207"><a href="#cb21-207" aria-hidden="true"></a>    avg_recall <span class="op">=</span> total_valid_recall <span class="op">/</span> <span class="bu">len</span>(dataloader)</span>
<span id="cb21-208"><a href="#cb21-208" aria-hidden="true"></a></span>
<span id="cb21-209"><a href="#cb21-209" aria-hidden="true"></a>    <span class="co"># calculate the average loss over all of the batches.</span></span>
<span id="cb21-210"><a href="#cb21-210" aria-hidden="true"></a>    <span class="kw">global</span> avg_val_loss</span>
<span id="cb21-211"><a href="#cb21-211" aria-hidden="true"></a>    avg_val_loss <span class="op">=</span> total_valid_loss <span class="op">/</span> <span class="bu">len</span>(dataloader)</span>
<span id="cb21-212"><a href="#cb21-212" aria-hidden="true"></a></span>
<span id="cb21-213"><a href="#cb21-213" aria-hidden="true"></a>    <span class="co"># capture end validation time</span></span>
<span id="cb21-214"><a href="#cb21-214" aria-hidden="true"></a>    training_time <span class="op">=</span> format_time(time.time() <span class="op">-</span> total_t0)</span>
<span id="cb21-215"><a href="#cb21-215" aria-hidden="true"></a></span>
<span id="cb21-216"><a href="#cb21-216" aria-hidden="true"></a>    <span class="co"># Record all statistics from this epoch.</span></span>
<span id="cb21-217"><a href="#cb21-217" aria-hidden="true"></a>    valid_stats.append(</span>
<span id="cb21-218"><a href="#cb21-218" aria-hidden="true"></a>        {</span>
<span id="cb21-219"><a href="#cb21-219" aria-hidden="true"></a>            <span class="st">&#39;Val Loss&#39;</span>: avg_val_loss,</span>
<span id="cb21-220"><a href="#cb21-220" aria-hidden="true"></a>            <span class="st">&#39;Val Accur.&#39;</span>: avg_accuracy,</span>
<span id="cb21-221"><a href="#cb21-221" aria-hidden="true"></a>            <span class="st">&#39;Val precision&#39;</span>: avg_precision,</span>
<span id="cb21-222"><a href="#cb21-222" aria-hidden="true"></a>            <span class="st">&#39;Val recall&#39;</span>: avg_recall,</span>
<span id="cb21-223"><a href="#cb21-223" aria-hidden="true"></a>            <span class="st">&#39;Val F1&#39;</span>: avg_val_f1,</span>
<span id="cb21-224"><a href="#cb21-224" aria-hidden="true"></a>            <span class="st">&#39;Val Time&#39;</span>: training_time</span>
<span id="cb21-225"><a href="#cb21-225" aria-hidden="true"></a>        }</span>
<span id="cb21-226"><a href="#cb21-226" aria-hidden="true"></a>    )</span>
<span id="cb21-227"><a href="#cb21-227" aria-hidden="true"></a></span>
<span id="cb21-228"><a href="#cb21-228" aria-hidden="true"></a>    <span class="co"># print result summaries</span></span>
<span id="cb21-229"><a href="#cb21-229" aria-hidden="true"></a>    <span class="bu">print</span>(<span class="st">&quot;&quot;</span>)</span>
<span id="cb21-230"><a href="#cb21-230" aria-hidden="true"></a>    <span class="bu">print</span>(<span class="st">&quot;summary results&quot;</span>)</span>
<span id="cb21-231"><a href="#cb21-231" aria-hidden="true"></a>    <span class="bu">print</span>(<span class="st">&quot;epoch | val loss | val f1 | val time&quot;</span>)</span>
<span id="cb21-232"><a href="#cb21-232" aria-hidden="true"></a>    <span class="bu">print</span>(<span class="ss">f&quot;</span><span class="sc">{</span>epoch<span class="op">+</span><span class="dv">1</span><span class="sc">:5d}</span><span class="ss"> | </span><span class="sc">{</span>avg_val_loss<span class="sc">:.5f}</span><span class="ss"> | </span><span class="sc">{</span>avg_val_f1<span class="sc">:.5f}</span><span class="ss"> | </span><span class="sc">{</span>training_time<span class="sc">:}</span><span class="ss">&quot;</span>)</span>
<span id="cb21-233"><a href="#cb21-233" aria-hidden="true"></a></span>
<span id="cb21-234"><a href="#cb21-234" aria-hidden="true"></a>    <span class="cf">return</span> <span class="va">None</span></span>
<span id="cb21-235"><a href="#cb21-235" aria-hidden="true"></a></span>
<span id="cb21-236"><a href="#cb21-236" aria-hidden="true"></a></span>
<span id="cb21-237"><a href="#cb21-237" aria-hidden="true"></a><span class="kw">def</span> testing(model, dataloader):</span>
<span id="cb21-238"><a href="#cb21-238" aria-hidden="true"></a></span>
<span id="cb21-239"><a href="#cb21-239" aria-hidden="true"></a>    <span class="bu">print</span>(<span class="st">&quot;&quot;</span>)</span>
<span id="cb21-240"><a href="#cb21-240" aria-hidden="true"></a>    <span class="bu">print</span>(<span class="st">&quot;Running Testing...&quot;</span>)</span>
<span id="cb21-241"><a href="#cb21-241" aria-hidden="true"></a></span>
<span id="cb21-242"><a href="#cb21-242" aria-hidden="true"></a>    <span class="co"># capture test time</span></span>
<span id="cb21-243"><a href="#cb21-243" aria-hidden="true"></a>    total_t0 <span class="op">=</span> time.time()</span>
<span id="cb21-244"><a href="#cb21-244" aria-hidden="true"></a></span>
<span id="cb21-245"><a href="#cb21-245" aria-hidden="true"></a>    <span class="co"># put both models in evaluation mode</span></span>
<span id="cb21-246"><a href="#cb21-246" aria-hidden="true"></a>    model.<span class="bu">eval</span>()</span>
<span id="cb21-247"><a href="#cb21-247" aria-hidden="true"></a>    kim_model.<span class="bu">eval</span>()</span>
<span id="cb21-248"><a href="#cb21-248" aria-hidden="true"></a></span>
<span id="cb21-249"><a href="#cb21-249" aria-hidden="true"></a>    <span class="co"># track variables</span></span>
<span id="cb21-250"><a href="#cb21-250" aria-hidden="true"></a>    total_test_accuracy <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb21-251"><a href="#cb21-251" aria-hidden="true"></a>    total_test_loss <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb21-252"><a href="#cb21-252" aria-hidden="true"></a>    total_test_f1 <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb21-253"><a href="#cb21-253" aria-hidden="true"></a>    total_test_recall <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb21-254"><a href="#cb21-254" aria-hidden="true"></a>    total_test_precision <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb21-255"><a href="#cb21-255" aria-hidden="true"></a></span>
<span id="cb21-256"><a href="#cb21-256" aria-hidden="true"></a>    <span class="co"># evaluate data for one epoch</span></span>
<span id="cb21-257"><a href="#cb21-257" aria-hidden="true"></a>    <span class="cf">for</span> batch <span class="kw">in</span> dataloader:</span>
<span id="cb21-258"><a href="#cb21-258" aria-hidden="true"></a></span>
<span id="cb21-259"><a href="#cb21-259" aria-hidden="true"></a>        <span class="co"># Unpack this training batch from our dataloader:</span></span>
<span id="cb21-260"><a href="#cb21-260" aria-hidden="true"></a>        <span class="co"># `batch` contains three pytorch tensors:</span></span>
<span id="cb21-261"><a href="#cb21-261" aria-hidden="true"></a>        <span class="co">#   [0]: input ids</span></span>
<span id="cb21-262"><a href="#cb21-262" aria-hidden="true"></a>        <span class="co">#   [1]: attention masks</span></span>
<span id="cb21-263"><a href="#cb21-263" aria-hidden="true"></a>        <span class="co">#   [2]: labels</span></span>
<span id="cb21-264"><a href="#cb21-264" aria-hidden="true"></a>        b_input_ids <span class="op">=</span> batch[<span class="dv">0</span>].cuda()</span>
<span id="cb21-265"><a href="#cb21-265" aria-hidden="true"></a>        b_input_mask <span class="op">=</span> batch[<span class="dv">1</span>].cuda()</span>
<span id="cb21-266"><a href="#cb21-266" aria-hidden="true"></a>        b_labels <span class="op">=</span> batch[<span class="dv">2</span>].cuda().<span class="bu">long</span>()</span>
<span id="cb21-267"><a href="#cb21-267" aria-hidden="true"></a></span>
<span id="cb21-268"><a href="#cb21-268" aria-hidden="true"></a>        <span class="co"># tell pytorch not to bother calculating gradients</span></span>
<span id="cb21-269"><a href="#cb21-269" aria-hidden="true"></a>        <span class="cf">with</span> torch.no_grad():</span>
<span id="cb21-270"><a href="#cb21-270" aria-hidden="true"></a>            <span class="co"># forward propagation (evaluate model on training batch)</span></span>
<span id="cb21-271"><a href="#cb21-271" aria-hidden="true"></a>            outputs <span class="op">=</span> model(input_ids<span class="op">=</span>b_input_ids, attention_mask<span class="op">=</span>b_input_mask)</span>
<span id="cb21-272"><a href="#cb21-272" aria-hidden="true"></a></span>
<span id="cb21-273"><a href="#cb21-273" aria-hidden="true"></a>            hidden_layers <span class="op">=</span> outputs[<span class="dv">2</span>]  <span class="co"># get hidden layers</span></span>
<span id="cb21-274"><a href="#cb21-274" aria-hidden="true"></a></span>
<span id="cb21-275"><a href="#cb21-275" aria-hidden="true"></a>            hidden_layers <span class="op">=</span> torch.stack(hidden_layers, dim<span class="op">=</span><span class="dv">1</span>)  <span class="co"># stack the layers</span></span>
<span id="cb21-276"><a href="#cb21-276" aria-hidden="true"></a></span>
<span id="cb21-277"><a href="#cb21-277" aria-hidden="true"></a>            hidden_layers <span class="op">=</span> hidden_layers[:, <span class="op">-</span><span class="dv">4</span>:]  <span class="co"># get the last 4 layers</span></span>
<span id="cb21-278"><a href="#cb21-278" aria-hidden="true"></a></span>
<span id="cb21-279"><a href="#cb21-279" aria-hidden="true"></a>        logits <span class="op">=</span> kim_model(hidden_layers)</span>
<span id="cb21-280"><a href="#cb21-280" aria-hidden="true"></a></span>
<span id="cb21-281"><a href="#cb21-281" aria-hidden="true"></a>        loss <span class="op">=</span> criterion(logits.view(<span class="op">-</span><span class="dv">1</span>, <span class="dv">2</span>), b_labels.view(<span class="op">-</span><span class="dv">1</span>))</span>
<span id="cb21-282"><a href="#cb21-282" aria-hidden="true"></a></span>
<span id="cb21-283"><a href="#cb21-283" aria-hidden="true"></a>        <span class="co"># accumulate validation loss</span></span>
<span id="cb21-284"><a href="#cb21-284" aria-hidden="true"></a>        total_test_loss <span class="op">+=</span> loss.item()</span>
<span id="cb21-285"><a href="#cb21-285" aria-hidden="true"></a></span>
<span id="cb21-286"><a href="#cb21-286" aria-hidden="true"></a>        <span class="co"># calculate preds</span></span>
<span id="cb21-287"><a href="#cb21-287" aria-hidden="true"></a>        _, predicted <span class="op">=</span> torch.<span class="bu">max</span>(logits, <span class="dv">1</span>)</span>
<span id="cb21-288"><a href="#cb21-288" aria-hidden="true"></a></span>
<span id="cb21-289"><a href="#cb21-289" aria-hidden="true"></a>        <span class="co"># move logits and labels to CPU</span></span>
<span id="cb21-290"><a href="#cb21-290" aria-hidden="true"></a>        predicted <span class="op">=</span> predicted.detach().cpu().numpy()</span>
<span id="cb21-291"><a href="#cb21-291" aria-hidden="true"></a>        y_true <span class="op">=</span> b_labels.detach().cpu().numpy()</span>
<span id="cb21-292"><a href="#cb21-292" aria-hidden="true"></a></span>
<span id="cb21-293"><a href="#cb21-293" aria-hidden="true"></a>        <span class="co"># calculate f1</span></span>
<span id="cb21-294"><a href="#cb21-294" aria-hidden="true"></a>        total_test_f1 <span class="op">+=</span> f1_score(predicted, y_true,</span>
<span id="cb21-295"><a href="#cb21-295" aria-hidden="true"></a>                                   average<span class="op">=</span><span class="st">&#39;weighted&#39;</span>,</span>
<span id="cb21-296"><a href="#cb21-296" aria-hidden="true"></a>                                   labels<span class="op">=</span>np.unique(predicted))</span>
<span id="cb21-297"><a href="#cb21-297" aria-hidden="true"></a></span>
<span id="cb21-298"><a href="#cb21-298" aria-hidden="true"></a>        <span class="co"># calculate accuracy</span></span>
<span id="cb21-299"><a href="#cb21-299" aria-hidden="true"></a>        total_test_accuracy <span class="op">+=</span> accuracy_score(predicted, y_true)</span>
<span id="cb21-300"><a href="#cb21-300" aria-hidden="true"></a></span>
<span id="cb21-301"><a href="#cb21-301" aria-hidden="true"></a>        <span class="co"># calculate precision</span></span>
<span id="cb21-302"><a href="#cb21-302" aria-hidden="true"></a>        total_test_precision <span class="op">+=</span> precision_score(predicted, y_true,</span>
<span id="cb21-303"><a href="#cb21-303" aria-hidden="true"></a>                                                 average<span class="op">=</span><span class="st">&#39;weighted&#39;</span>,</span>
<span id="cb21-304"><a href="#cb21-304" aria-hidden="true"></a>                                                 labels<span class="op">=</span>np.unique(predicted))</span>
<span id="cb21-305"><a href="#cb21-305" aria-hidden="true"></a></span>
<span id="cb21-306"><a href="#cb21-306" aria-hidden="true"></a>        <span class="co"># calculate recall</span></span>
<span id="cb21-307"><a href="#cb21-307" aria-hidden="true"></a>        total_test_recall <span class="op">+=</span> recall_score(predicted, y_true,</span>
<span id="cb21-308"><a href="#cb21-308" aria-hidden="true"></a>                                                 average<span class="op">=</span><span class="st">&#39;weighted&#39;</span>,</span>
<span id="cb21-309"><a href="#cb21-309" aria-hidden="true"></a>                                                 labels<span class="op">=</span>np.unique(predicted))</span>
<span id="cb21-310"><a href="#cb21-310" aria-hidden="true"></a></span>
<span id="cb21-311"><a href="#cb21-311" aria-hidden="true"></a>    <span class="co"># report final accuracy of test run</span></span>
<span id="cb21-312"><a href="#cb21-312" aria-hidden="true"></a>    avg_accuracy <span class="op">=</span> total_test_accuracy <span class="op">/</span> <span class="bu">len</span>(dataloader)</span>
<span id="cb21-313"><a href="#cb21-313" aria-hidden="true"></a></span>
<span id="cb21-314"><a href="#cb21-314" aria-hidden="true"></a>    <span class="co"># report final f1 of test run</span></span>
<span id="cb21-315"><a href="#cb21-315" aria-hidden="true"></a>    avg_test_f1 <span class="op">=</span> total_test_f1 <span class="op">/</span> <span class="bu">len</span>(dataloader)</span>
<span id="cb21-316"><a href="#cb21-316" aria-hidden="true"></a></span>
<span id="cb21-317"><a href="#cb21-317" aria-hidden="true"></a>    <span class="co"># report final f1 of test run</span></span>
<span id="cb21-318"><a href="#cb21-318" aria-hidden="true"></a>    avg_precision <span class="op">=</span> total_test_precision <span class="op">/</span> <span class="bu">len</span>(dataloader)</span>
<span id="cb21-319"><a href="#cb21-319" aria-hidden="true"></a></span>
<span id="cb21-320"><a href="#cb21-320" aria-hidden="true"></a>    <span class="co"># report final f1 of test run</span></span>
<span id="cb21-321"><a href="#cb21-321" aria-hidden="true"></a>    avg_recall <span class="op">=</span> total_test_recall <span class="op">/</span> <span class="bu">len</span>(dataloader)</span>
<span id="cb21-322"><a href="#cb21-322" aria-hidden="true"></a></span>
<span id="cb21-323"><a href="#cb21-323" aria-hidden="true"></a>    <span class="co"># calculate the average loss over all of the batches.</span></span>
<span id="cb21-324"><a href="#cb21-324" aria-hidden="true"></a>    avg_test_loss <span class="op">=</span> total_test_loss <span class="op">/</span> <span class="bu">len</span>(dataloader)</span>
<span id="cb21-325"><a href="#cb21-325" aria-hidden="true"></a></span>
<span id="cb21-326"><a href="#cb21-326" aria-hidden="true"></a>    <span class="co"># capture end testing time</span></span>
<span id="cb21-327"><a href="#cb21-327" aria-hidden="true"></a>    training_time <span class="op">=</span> format_time(time.time() <span class="op">-</span> total_t0)</span>
<span id="cb21-328"><a href="#cb21-328" aria-hidden="true"></a></span>
<span id="cb21-329"><a href="#cb21-329" aria-hidden="true"></a>    <span class="co"># Record all statistics from this epoch.</span></span>
<span id="cb21-330"><a href="#cb21-330" aria-hidden="true"></a>    test_stats.append(</span>
<span id="cb21-331"><a href="#cb21-331" aria-hidden="true"></a>        {</span>
<span id="cb21-332"><a href="#cb21-332" aria-hidden="true"></a>            <span class="st">&#39;Test Loss&#39;</span>: avg_test_loss,</span>
<span id="cb21-333"><a href="#cb21-333" aria-hidden="true"></a>            <span class="st">&#39;Test Accur.&#39;</span>: avg_accuracy,</span>
<span id="cb21-334"><a href="#cb21-334" aria-hidden="true"></a>            <span class="st">&#39;Test precision&#39;</span>: avg_precision,</span>
<span id="cb21-335"><a href="#cb21-335" aria-hidden="true"></a>            <span class="st">&#39;Test recall&#39;</span>: avg_recall,</span>
<span id="cb21-336"><a href="#cb21-336" aria-hidden="true"></a>            <span class="st">&#39;Test F1&#39;</span>: avg_test_f1,</span>
<span id="cb21-337"><a href="#cb21-337" aria-hidden="true"></a>            <span class="st">&#39;Test Time&#39;</span>: training_time</span>
<span id="cb21-338"><a href="#cb21-338" aria-hidden="true"></a>        }</span>
<span id="cb21-339"><a href="#cb21-339" aria-hidden="true"></a>    )</span>
<span id="cb21-340"><a href="#cb21-340" aria-hidden="true"></a>    <span class="co"># print result summaries</span></span>
<span id="cb21-341"><a href="#cb21-341" aria-hidden="true"></a>    <span class="bu">print</span>(<span class="st">&quot;&quot;</span>)</span>
<span id="cb21-342"><a href="#cb21-342" aria-hidden="true"></a>    <span class="bu">print</span>(<span class="st">&quot;summary results&quot;</span>)</span>
<span id="cb21-343"><a href="#cb21-343" aria-hidden="true"></a>    <span class="bu">print</span>(<span class="st">&quot;epoch | test loss | test f1 | test time&quot;</span>)</span>
<span id="cb21-344"><a href="#cb21-344" aria-hidden="true"></a>    <span class="bu">print</span>(<span class="ss">f&quot;</span><span class="sc">{</span>epoch<span class="op">+</span><span class="dv">1</span><span class="sc">:5d}</span><span class="ss"> | </span><span class="sc">{</span>avg_test_loss<span class="sc">:.5f}</span><span class="ss"> | </span><span class="sc">{</span>avg_test_f1<span class="sc">:.5f}</span><span class="ss"> | </span><span class="sc">{</span>training_time<span class="sc">:}</span><span class="ss">&quot;</span>)</span>
<span id="cb21-345"><a href="#cb21-345" aria-hidden="true"></a></span>
<span id="cb21-346"><a href="#cb21-346" aria-hidden="true"></a>    <span class="cf">return</span> <span class="va">None</span></span></code></pre></div>
</div>
<div id="prepare-models-for-training" class="section level2" number="1.8">
<h2 number="1.8"><span class="header-section-number">1.8</span> Prepare Models for Training</h2>
<p>Now we instantiate our models and attach them to the GPU. A few other preparatory objects are created like the loss criteria, epochs, the optimizer, and our optimizer scheduler.</p>
<div class="sourceCode" id="cb22"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb22-1"><a href="#cb22-1" aria-hidden="true"></a><span class="co"># instantiate BERT model with hidden states</span></span>
<span id="cb22-2"><a href="#cb22-2" aria-hidden="true"></a>model <span class="op">=</span> BertModel.from_pretrained(<span class="st">&#39;bert-base-uncased&#39;</span>, output_hidden_states<span class="op">=</span><span class="va">True</span>).cuda()</span>
<span id="cb22-3"><a href="#cb22-3" aria-hidden="true"></a></span>
<span id="cb22-4"><a href="#cb22-4" aria-hidden="true"></a><span class="co"># instantiate CNN config</span></span></code></pre></div>
<pre><code>## 
Downloading:   0%|          | 0.00/433 [00:00&lt;?, ?B/s]
Downloading: 100%|##########| 433/433 [00:00&lt;00:00, 432kB/s]
## 
Downloading:   0%|          | 0.00/440M [00:00&lt;?, ?B/s]
Downloading:   0%|          | 688k/440M [00:00&lt;01:04, 6.87MB/s]
Downloading:   1%|          | 2.25M/440M [00:00&lt;00:53, 8.25MB/s]
Downloading:   1%|          | 3.80M/440M [00:00&lt;00:45, 9.59MB/s]
Downloading:   1%|1         | 5.20M/440M [00:00&lt;00:41, 10.6MB/s]
Downloading:   1%|1         | 6.53M/440M [00:00&lt;00:38, 11.3MB/s]
Downloading:   2%|1         | 7.99M/440M [00:00&lt;00:35, 12.1MB/s]
Downloading:   2%|2         | 9.25M/440M [00:00&lt;00:35, 12.2MB/s]
Downloading:   2%|2         | 10.7M/440M [00:00&lt;00:33, 12.8MB/s]
Downloading:   3%|2         | 12.1M/440M [00:00&lt;00:32, 13.0MB/s]
Downloading:   3%|3         | 13.5M/440M [00:01&lt;00:32, 13.3MB/s]
Downloading:   3%|3         | 14.9M/440M [00:01&lt;00:31, 13.5MB/s]
Downloading:   4%|3         | 16.4M/440M [00:01&lt;00:30, 13.7MB/s]
Downloading:   4%|4         | 17.8M/440M [00:01&lt;00:30, 13.9MB/s]
Downloading:   4%|4         | 19.2M/440M [00:01&lt;00:30, 13.7MB/s]
Downloading:   5%|4         | 20.6M/440M [00:01&lt;00:30, 13.8MB/s]
Downloading:   5%|4         | 22.0M/440M [00:01&lt;00:30, 13.7MB/s]
Downloading:   5%|5         | 23.4M/440M [00:01&lt;00:30, 13.6MB/s]
Downloading:   6%|5         | 24.8M/440M [00:01&lt;00:29, 13.9MB/s]
Downloading:   6%|5         | 26.4M/440M [00:01&lt;00:29, 14.2MB/s]
Downloading:   6%|6         | 27.8M/440M [00:02&lt;00:32, 12.8MB/s]
Downloading:   7%|6         | 29.1M/440M [00:02&lt;00:35, 11.4MB/s]
Downloading:   7%|6         | 30.4M/440M [00:02&lt;00:34, 11.9MB/s]
Downloading:   7%|7         | 31.9M/440M [00:02&lt;00:32, 12.6MB/s]
Downloading:   8%|7         | 33.3M/440M [00:02&lt;00:30, 13.2MB/s]
Downloading:   8%|7         | 34.7M/440M [00:02&lt;00:30, 13.3MB/s]
Downloading:   8%|8         | 36.1M/440M [00:02&lt;00:30, 13.1MB/s]
Downloading:   8%|8         | 37.4M/440M [00:02&lt;00:33, 11.9MB/s]
Downloading:   9%|8         | 38.8M/440M [00:02&lt;00:32, 12.4MB/s]
Downloading:   9%|9         | 40.2M/440M [00:03&lt;00:31, 12.9MB/s]
Downloading:   9%|9         | 41.7M/440M [00:03&lt;00:30, 13.3MB/s]
Downloading:  10%|9         | 43.1M/440M [00:03&lt;00:29, 13.5MB/s]
Downloading:  10%|#         | 44.6M/440M [00:03&lt;00:28, 13.9MB/s]
Downloading:  10%|#         | 46.0M/440M [00:03&lt;00:28, 13.9MB/s]
Downloading:  11%|#         | 47.4M/440M [00:03&lt;00:28, 14.0MB/s]
Downloading:  11%|#1        | 48.9M/440M [00:03&lt;00:27, 14.2MB/s]
Downloading:  11%|#1        | 50.3M/440M [00:03&lt;00:27, 14.1MB/s]
Downloading:  12%|#1        | 51.7M/440M [00:03&lt;00:28, 13.7MB/s]
Downloading:  12%|#2        | 53.1M/440M [00:03&lt;00:28, 13.8MB/s]
Downloading:  12%|#2        | 54.5M/440M [00:04&lt;00:28, 13.7MB/s]
Downloading:  13%|#2        | 55.9M/440M [00:04&lt;00:37, 10.3MB/s]
Downloading:  13%|#2        | 57.3M/440M [00:04&lt;00:34, 11.1MB/s]
Downloading:  13%|#3        | 58.7M/440M [00:04&lt;00:32, 11.9MB/s]
Downloading:  14%|#3        | 60.0M/440M [00:04&lt;00:32, 11.6MB/s]
Downloading:  14%|#3        | 61.4M/440M [00:04&lt;00:31, 12.2MB/s]
Downloading:  14%|#4        | 62.6M/440M [00:04&lt;00:30, 12.3MB/s]
Downloading:  15%|#4        | 64.0M/440M [00:04&lt;00:30, 12.5MB/s]
Downloading:  15%|#4        | 65.4M/440M [00:05&lt;00:29, 12.7MB/s]
Downloading:  15%|#5        | 66.8M/440M [00:05&lt;00:28, 13.0MB/s]
Downloading:  16%|#5        | 68.3M/440M [00:05&lt;00:27, 13.5MB/s]
Downloading:  16%|#5        | 69.7M/440M [00:05&lt;00:27, 13.6MB/s]
Downloading:  16%|#6        | 71.1M/440M [00:05&lt;00:26, 13.8MB/s]
Downloading:  16%|#6        | 72.6M/440M [00:05&lt;00:26, 14.1MB/s]
Downloading:  17%|#6        | 74.0M/440M [00:05&lt;00:26, 14.1MB/s]
Downloading:  17%|#7        | 75.4M/440M [00:05&lt;00:26, 13.9MB/s]
Downloading:  17%|#7        | 76.8M/440M [00:05&lt;00:31, 11.5MB/s]
Downloading:  18%|#7        | 78.1M/440M [00:06&lt;00:32, 11.0MB/s]
Downloading:  18%|#8        | 79.6M/440M [00:06&lt;00:30, 11.9MB/s]
Downloading:  18%|#8        | 81.0M/440M [00:06&lt;00:28, 12.4MB/s]
Downloading:  19%|#8        | 82.3M/440M [00:06&lt;00:27, 12.8MB/s]
Downloading:  19%|#9        | 83.9M/440M [00:06&lt;00:26, 13.5MB/s]
Downloading:  19%|#9        | 85.3M/440M [00:06&lt;00:26, 13.4MB/s]
Downloading:  20%|#9        | 86.8M/440M [00:06&lt;00:25, 13.8MB/s]
Downloading:  20%|##        | 88.2M/440M [00:06&lt;00:25, 13.6MB/s]
Downloading:  20%|##        | 89.7M/440M [00:06&lt;00:24, 14.1MB/s]
Downloading:  21%|##        | 91.1M/440M [00:06&lt;00:28, 12.4MB/s]
Downloading:  21%|##1       | 92.6M/440M [00:07&lt;00:26, 13.0MB/s]
Downloading:  21%|##1       | 94.0M/440M [00:07&lt;00:26, 13.2MB/s]
Downloading:  22%|##1       | 95.4M/440M [00:07&lt;00:25, 13.5MB/s]
Downloading:  22%|##1       | 96.9M/440M [00:07&lt;00:24, 13.9MB/s]
Downloading:  22%|##2       | 98.3M/440M [00:07&lt;00:24, 13.9MB/s]
Downloading:  23%|##2       | 99.7M/440M [00:07&lt;00:24, 13.7MB/s]
Downloading:  23%|##2       | 101M/440M [00:07&lt;00:26, 13.0MB/s] 
Downloading:  23%|##3       | 102M/440M [00:07&lt;00:25, 13.2MB/s]
Downloading:  24%|##3       | 104M/440M [00:07&lt;00:24, 13.5MB/s]
Downloading:  24%|##3       | 105M/440M [00:08&lt;00:24, 13.5MB/s]
Downloading:  24%|##4       | 107M/440M [00:08&lt;00:24, 13.4MB/s]
Downloading:  25%|##4       | 108M/440M [00:08&lt;00:25, 13.1MB/s]
Downloading:  25%|##4       | 109M/440M [00:08&lt;00:25, 13.1MB/s]
Downloading:  25%|##5       | 111M/440M [00:08&lt;00:26, 12.7MB/s]
Downloading:  25%|##5       | 112M/440M [00:08&lt;00:24, 13.3MB/s]
Downloading:  26%|##5       | 114M/440M [00:08&lt;00:24, 13.3MB/s]
Downloading:  26%|##6       | 115M/440M [00:08&lt;00:23, 13.6MB/s]
Downloading:  26%|##6       | 116M/440M [00:08&lt;00:23, 13.9MB/s]
Downloading:  27%|##6       | 118M/440M [00:08&lt;00:23, 14.0MB/s]
Downloading:  27%|##7       | 119M/440M [00:09&lt;00:22, 14.0MB/s]
Downloading:  27%|##7       | 121M/440M [00:09&lt;00:22, 14.0MB/s]
Downloading:  28%|##7       | 122M/440M [00:09&lt;00:23, 13.8MB/s]
Downloading:  28%|##8       | 124M/440M [00:09&lt;00:22, 13.9MB/s]
Downloading:  28%|##8       | 125M/440M [00:09&lt;00:23, 13.7MB/s]
Downloading:  29%|##8       | 126M/440M [00:09&lt;00:22, 14.0MB/s]
Downloading:  29%|##9       | 128M/440M [00:09&lt;00:22, 14.1MB/s]
Downloading:  29%|##9       | 129M/440M [00:09&lt;00:22, 13.9MB/s]
Downloading:  30%|##9       | 131M/440M [00:09&lt;00:22, 14.1MB/s]
Downloading:  30%|###       | 132M/440M [00:09&lt;00:21, 14.2MB/s]
Downloading:  30%|###       | 134M/440M [00:10&lt;00:21, 14.3MB/s]
Downloading:  31%|###       | 135M/440M [00:10&lt;00:20, 14.7MB/s]
Downloading:  31%|###1      | 137M/440M [00:10&lt;00:21, 14.0MB/s]
Downloading:  31%|###1      | 138M/440M [00:10&lt;00:21, 13.8MB/s]
Downloading:  32%|###1      | 140M/440M [00:10&lt;00:21, 14.0MB/s]
Downloading:  32%|###2      | 141M/440M [00:10&lt;00:21, 13.8MB/s]
Downloading:  32%|###2      | 142M/440M [00:10&lt;00:21, 13.7MB/s]
Downloading:  33%|###2      | 144M/440M [00:10&lt;00:21, 13.9MB/s]
Downloading:  33%|###2      | 145M/440M [00:10&lt;00:21, 14.0MB/s]
Downloading:  33%|###3      | 147M/440M [00:10&lt;00:21, 13.8MB/s]
Downloading:  34%|###3      | 148M/440M [00:11&lt;00:20, 14.1MB/s]
Downloading:  34%|###3      | 150M/440M [00:11&lt;00:20, 14.1MB/s]
Downloading:  34%|###4      | 151M/440M [00:11&lt;00:20, 13.8MB/s]
Downloading:  35%|###4      | 152M/440M [00:11&lt;00:20, 13.9MB/s]
Downloading:  35%|###4      | 154M/440M [00:11&lt;00:20, 13.8MB/s]
Downloading:  35%|###5      | 155M/440M [00:11&lt;00:20, 13.7MB/s]
Downloading:  36%|###5      | 157M/440M [00:11&lt;00:20, 13.6MB/s]
Downloading:  36%|###5      | 158M/440M [00:11&lt;00:20, 13.7MB/s]
Downloading:  36%|###6      | 159M/440M [00:11&lt;00:20, 13.6MB/s]
Downloading:  37%|###6      | 161M/440M [00:12&lt;00:20, 13.7MB/s]
Downloading:  37%|###6      | 162M/440M [00:12&lt;00:19, 14.1MB/s]
Downloading:  37%|###7      | 164M/440M [00:12&lt;00:20, 13.7MB/s]
Downloading:  38%|###7      | 165M/440M [00:12&lt;00:19, 14.2MB/s]
Downloading:  38%|###7      | 167M/440M [00:12&lt;00:19, 14.2MB/s]
Downloading:  38%|###8      | 168M/440M [00:12&lt;00:18, 14.4MB/s]
Downloading:  39%|###8      | 170M/440M [00:12&lt;00:18, 14.4MB/s]
Downloading:  39%|###8      | 171M/440M [00:12&lt;00:18, 14.2MB/s]
Downloading:  39%|###9      | 173M/440M [00:12&lt;00:18, 14.3MB/s]
Downloading:  40%|###9      | 174M/440M [00:12&lt;00:18, 14.2MB/s]
Downloading:  40%|###9      | 175M/440M [00:13&lt;00:18, 14.1MB/s]
Downloading:  40%|####      | 177M/440M [00:13&lt;00:30, 8.50MB/s]
Downloading:  40%|####      | 178M/440M [00:13&lt;00:26, 9.72MB/s]
Downloading:  41%|####      | 180M/440M [00:13&lt;00:24, 10.8MB/s]
Downloading:  41%|####1     | 181M/440M [00:13&lt;00:22, 11.6MB/s]
Downloading:  41%|####1     | 183M/440M [00:13&lt;00:20, 12.3MB/s]
Downloading:  42%|####1     | 184M/440M [00:13&lt;00:19, 12.8MB/s]
Downloading:  42%|####2     | 185M/440M [00:13&lt;00:19, 13.1MB/s]
Downloading:  42%|####2     | 187M/440M [00:14&lt;00:18, 13.4MB/s]
Downloading:  43%|####2     | 188M/440M [00:14&lt;00:18, 13.4MB/s]
Downloading:  43%|####3     | 190M/440M [00:14&lt;00:18, 13.4MB/s]
Downloading:  43%|####3     | 191M/440M [00:14&lt;00:18, 13.4MB/s]
Downloading:  44%|####3     | 192M/440M [00:14&lt;00:18, 13.5MB/s]
Downloading:  44%|####3     | 194M/440M [00:14&lt;00:18, 13.4MB/s]
Downloading:  44%|####4     | 195M/440M [00:14&lt;00:17, 13.7MB/s]
Downloading:  45%|####4     | 197M/440M [00:14&lt;00:17, 13.7MB/s]
Downloading:  45%|####4     | 198M/440M [00:14&lt;00:17, 14.0MB/s]
Downloading:  45%|####5     | 199M/440M [00:14&lt;00:17, 14.0MB/s]
Downloading:  46%|####5     | 201M/440M [00:15&lt;00:17, 13.9MB/s]
Downloading:  46%|####5     | 202M/440M [00:15&lt;00:16, 14.2MB/s]
Downloading:  46%|####6     | 204M/440M [00:15&lt;00:17, 13.6MB/s]
Downloading:  47%|####6     | 205M/440M [00:15&lt;00:16, 13.9MB/s]
Downloading:  47%|####6     | 207M/440M [00:15&lt;00:17, 13.7MB/s]
Downloading:  47%|####7     | 208M/440M [00:15&lt;00:17, 13.6MB/s]
Downloading:  48%|####7     | 209M/440M [00:15&lt;00:17, 13.6MB/s]
Downloading:  48%|####7     | 211M/440M [00:15&lt;00:16, 13.7MB/s]
Downloading:  48%|####8     | 212M/440M [00:15&lt;00:16, 13.8MB/s]
Downloading:  49%|####8     | 214M/440M [00:16&lt;00:16, 14.0MB/s]
Downloading:  49%|####8     | 215M/440M [00:16&lt;00:15, 14.1MB/s]
Downloading:  49%|####9     | 217M/440M [00:16&lt;00:16, 13.8MB/s]
Downloading:  50%|####9     | 218M/440M [00:16&lt;00:15, 14.1MB/s]
Downloading:  50%|####9     | 220M/440M [00:16&lt;00:15, 14.0MB/s]
Downloading:  50%|#####     | 221M/440M [00:16&lt;00:15, 14.3MB/s]
Downloading:  51%|#####     | 223M/440M [00:16&lt;00:15, 14.3MB/s]
Downloading:  51%|#####     | 224M/440M [00:16&lt;00:15, 14.1MB/s]
Downloading:  51%|#####1    | 225M/440M [00:16&lt;00:15, 14.0MB/s]
Downloading:  51%|#####1    | 227M/440M [00:16&lt;00:15, 13.9MB/s]
Downloading:  52%|#####1    | 228M/440M [00:17&lt;00:15, 13.7MB/s]
Downloading:  52%|#####2    | 230M/440M [00:17&lt;00:15, 13.8MB/s]
Downloading:  52%|#####2    | 231M/440M [00:17&lt;00:15, 13.9MB/s]
Downloading:  53%|#####2    | 232M/440M [00:17&lt;00:14, 14.0MB/s]
Downloading:  53%|#####3    | 234M/440M [00:17&lt;00:14, 14.1MB/s]
Downloading:  53%|#####3    | 235M/440M [00:17&lt;00:14, 14.2MB/s]
Downloading:  54%|#####3    | 237M/440M [00:17&lt;00:14, 14.0MB/s]
Downloading:  54%|#####4    | 238M/440M [00:17&lt;00:14, 13.5MB/s]
Downloading:  54%|#####4    | 240M/440M [00:17&lt;00:14, 13.9MB/s]
Downloading:  55%|#####4    | 241M/440M [00:17&lt;00:14, 13.6MB/s]
Downloading:  55%|#####5    | 242M/440M [00:18&lt;00:14, 13.5MB/s]
Downloading:  55%|#####5    | 244M/440M [00:18&lt;00:14, 13.5MB/s]
Downloading:  56%|#####5    | 245M/440M [00:18&lt;00:13, 13.9MB/s]
Downloading:  56%|#####6    | 247M/440M [00:18&lt;00:13, 14.0MB/s]
Downloading:  56%|#####6    | 248M/440M [00:18&lt;00:13, 14.1MB/s]
Downloading:  57%|#####6    | 250M/440M [00:18&lt;00:13, 14.0MB/s]
Downloading:  57%|#####7    | 251M/440M [00:18&lt;00:13, 13.8MB/s]
Downloading:  57%|#####7    | 253M/440M [00:18&lt;00:13, 13.9MB/s]
Downloading:  58%|#####7    | 254M/440M [00:18&lt;00:13, 14.1MB/s]
Downloading:  58%|#####7    | 255M/440M [00:19&lt;00:13, 14.1MB/s]
Downloading:  58%|#####8    | 257M/440M [00:19&lt;00:12, 14.2MB/s]
Downloading:  59%|#####8    | 258M/440M [00:19&lt;00:12, 14.1MB/s]
Downloading:  59%|#####8    | 260M/440M [00:19&lt;00:12, 13.9MB/s]
Downloading:  59%|#####9    | 261M/440M [00:19&lt;00:12, 14.0MB/s]
Downloading:  60%|#####9    | 263M/440M [00:19&lt;00:13, 13.4MB/s]
Downloading:  60%|#####9    | 264M/440M [00:19&lt;00:13, 13.4MB/s]
Downloading:  60%|######    | 265M/440M [00:19&lt;00:12, 13.5MB/s]
Downloading:  61%|######    | 267M/440M [00:19&lt;00:12, 13.5MB/s]
Downloading:  61%|######    | 268M/440M [00:19&lt;00:12, 13.4MB/s]
Downloading:  61%|######1   | 270M/440M [00:20&lt;00:12, 13.7MB/s]
Downloading:  62%|######1   | 271M/440M [00:20&lt;00:12, 13.1MB/s]
Downloading:  62%|######1   | 272M/440M [00:20&lt;00:18, 9.17MB/s]
Downloading:  62%|######2   | 273M/440M [00:20&lt;00:19, 8.78MB/s]
Downloading:  62%|######2   | 274M/440M [00:20&lt;00:19, 8.55MB/s]
Downloading:  63%|######2   | 276M/440M [00:20&lt;00:17, 9.65MB/s]
Downloading:  63%|######2   | 277M/440M [00:20&lt;00:15, 10.7MB/s]
Downloading:  63%|######3   | 279M/440M [00:20&lt;00:13, 11.7MB/s]
Downloading:  64%|######3   | 280M/440M [00:21&lt;00:13, 12.0MB/s]
Downloading:  64%|######3   | 281M/440M [00:21&lt;00:13, 12.2MB/s]
Downloading:  64%|######4   | 283M/440M [00:21&lt;00:11, 13.2MB/s]
Downloading:  65%|######4   | 285M/440M [00:21&lt;00:11, 13.9MB/s]
Downloading:  65%|######4   | 286M/440M [00:21&lt;00:11, 14.0MB/s]
Downloading:  65%|######5   | 287M/440M [00:21&lt;00:11, 13.8MB/s]
Downloading:  66%|######5   | 289M/440M [00:21&lt;00:11, 13.7MB/s]
Downloading:  66%|######5   | 290M/440M [00:21&lt;00:10, 13.8MB/s]
Downloading:  66%|######6   | 292M/440M [00:21&lt;00:10, 13.6MB/s]
Downloading:  67%|######6   | 293M/440M [00:21&lt;00:10, 13.8MB/s]
Downloading:  67%|######6   | 295M/440M [00:22&lt;00:10, 13.7MB/s]
Downloading:  67%|######7   | 296M/440M [00:22&lt;00:10, 13.8MB/s]
Downloading:  67%|######7   | 297M/440M [00:22&lt;00:10, 13.7MB/s]
Downloading:  68%|######7   | 299M/440M [00:22&lt;00:10, 13.7MB/s]
Downloading:  68%|######8   | 300M/440M [00:22&lt;00:10, 13.8MB/s]
Downloading:  68%|######8   | 302M/440M [00:22&lt;00:09, 14.0MB/s]
Downloading:  69%|######8   | 303M/440M [00:22&lt;00:09, 13.9MB/s]
Downloading:  69%|######9   | 304M/440M [00:22&lt;00:09, 14.0MB/s]
Downloading:  69%|######9   | 306M/440M [00:22&lt;00:09, 13.6MB/s]
Downloading:  70%|######9   | 307M/440M [00:23&lt;00:09, 13.4MB/s]
Downloading:  70%|#######   | 309M/440M [00:23&lt;00:09, 13.7MB/s]
Downloading:  70%|#######   | 310M/440M [00:23&lt;00:09, 13.6MB/s]
Downloading:  71%|#######   | 311M/440M [00:23&lt;00:09, 13.4MB/s]
Downloading:  71%|#######1  | 313M/440M [00:23&lt;00:09, 13.6MB/s]
Downloading:  71%|#######1  | 314M/440M [00:23&lt;00:09, 13.4MB/s]
Downloading:  72%|#######1  | 315M/440M [00:23&lt;00:09, 13.4MB/s]
Downloading:  72%|#######1  | 317M/440M [00:23&lt;00:09, 13.7MB/s]
Downloading:  72%|#######2  | 318M/440M [00:23&lt;00:08, 14.1MB/s]
Downloading:  73%|#######2  | 320M/440M [00:23&lt;00:08, 13.9MB/s]
Downloading:  73%|#######2  | 321M/440M [00:24&lt;00:08, 13.9MB/s]
Downloading:  73%|#######3  | 323M/440M [00:24&lt;00:08, 14.0MB/s]
Downloading:  74%|#######3  | 324M/440M [00:24&lt;00:08, 14.2MB/s]
Downloading:  74%|#######3  | 326M/440M [00:24&lt;00:08, 13.7MB/s]
Downloading:  74%|#######4  | 327M/440M [00:24&lt;00:08, 13.9MB/s]
Downloading:  75%|#######4  | 329M/440M [00:24&lt;00:08, 13.5MB/s]
Downloading:  75%|#######4  | 330M/440M [00:24&lt;00:08, 13.5MB/s]
Downloading:  75%|#######5  | 331M/440M [00:24&lt;00:07, 13.7MB/s]
Downloading:  76%|#######5  | 333M/440M [00:24&lt;00:07, 13.7MB/s]
Downloading:  76%|#######5  | 334M/440M [00:24&lt;00:08, 13.0MB/s]
Downloading:  76%|#######6  | 335M/440M [00:25&lt;00:08, 12.3MB/s]
Downloading:  76%|#######6  | 337M/440M [00:25&lt;00:10, 10.2MB/s]
Downloading:  77%|#######6  | 338M/440M [00:25&lt;00:09, 11.2MB/s]
Downloading:  77%|#######7  | 339M/440M [00:25&lt;00:08, 11.5MB/s]
Downloading:  77%|#######7  | 341M/440M [00:25&lt;00:08, 12.2MB/s]
Downloading:  78%|#######7  | 342M/440M [00:25&lt;00:07, 13.0MB/s]
Downloading:  78%|#######8  | 344M/440M [00:25&lt;00:07, 13.4MB/s]
Downloading:  78%|#######8  | 345M/440M [00:25&lt;00:07, 13.5MB/s]
Downloading:  79%|#######8  | 347M/440M [00:25&lt;00:06, 13.4MB/s]
Downloading:  79%|#######9  | 348M/440M [00:26&lt;00:06, 13.5MB/s]
Downloading:  79%|#######9  | 349M/440M [00:26&lt;00:06, 13.5MB/s]
Downloading:  80%|#######9  | 351M/440M [00:26&lt;00:06, 13.6MB/s]
Downloading:  80%|#######9  | 352M/440M [00:26&lt;00:06, 13.6MB/s]
Downloading:  80%|########  | 354M/440M [00:26&lt;00:06, 13.9MB/s]
Downloading:  81%|########  | 355M/440M [00:26&lt;00:06, 13.8MB/s]
Downloading:  81%|########  | 356M/440M [00:26&lt;00:06, 13.5MB/s]
Downloading:  81%|########1 | 358M/440M [00:26&lt;00:05, 13.8MB/s]
Downloading:  82%|########1 | 359M/440M [00:26&lt;00:05, 14.0MB/s]
Downloading:  82%|########1 | 361M/440M [00:27&lt;00:05, 14.2MB/s]
Downloading:  82%|########2 | 362M/440M [00:27&lt;00:05, 14.5MB/s]
Downloading:  83%|########2 | 364M/440M [00:27&lt;00:05, 14.5MB/s]
Downloading:  83%|########2 | 365M/440M [00:27&lt;00:05, 13.9MB/s]
Downloading:  83%|########3 | 367M/440M [00:27&lt;00:05, 14.3MB/s]
Downloading:  84%|########3 | 368M/440M [00:27&lt;00:04, 14.5MB/s]
Downloading:  84%|########3 | 370M/440M [00:27&lt;00:04, 14.6MB/s]
Downloading:  84%|########4 | 371M/440M [00:27&lt;00:04, 14.6MB/s]
Downloading:  85%|########4 | 373M/440M [00:27&lt;00:04, 14.6MB/s]
Downloading:  85%|########4 | 374M/440M [00:27&lt;00:04, 14.2MB/s]
Downloading:  85%|########5 | 376M/440M [00:28&lt;00:04, 14.2MB/s]
Downloading:  86%|########5 | 377M/440M [00:28&lt;00:04, 14.4MB/s]
Downloading:  86%|########5 | 379M/440M [00:28&lt;00:04, 14.2MB/s]
Downloading:  86%|########6 | 380M/440M [00:28&lt;00:04, 14.2MB/s]
Downloading:  87%|########6 | 381M/440M [00:28&lt;00:04, 14.2MB/s]
Downloading:  87%|########6 | 383M/440M [00:28&lt;00:03, 14.4MB/s]
Downloading:  87%|########7 | 384M/440M [00:28&lt;00:03, 14.4MB/s]
Downloading:  88%|########7 | 386M/440M [00:28&lt;00:03, 14.2MB/s]
Downloading:  88%|########7 | 387M/440M [00:28&lt;00:03, 14.2MB/s]
Downloading:  88%|########8 | 389M/440M [00:28&lt;00:03, 14.1MB/s]
Downloading:  89%|########8 | 390M/440M [00:29&lt;00:03, 14.0MB/s]
Downloading:  89%|########8 | 392M/440M [00:29&lt;00:03, 14.0MB/s]
Downloading:  89%|########9 | 393M/440M [00:29&lt;00:03, 14.2MB/s]
Downloading:  90%|########9 | 394M/440M [00:29&lt;00:03, 14.2MB/s]
Downloading:  90%|########9 | 396M/440M [00:29&lt;00:03, 14.2MB/s]
Downloading:  90%|######### | 397M/440M [00:29&lt;00:03, 14.3MB/s]
Downloading:  91%|######### | 399M/440M [00:29&lt;00:02, 14.0MB/s]
Downloading:  91%|######### | 400M/440M [00:29&lt;00:02, 13.8MB/s]
Downloading:  91%|#########1| 402M/440M [00:29&lt;00:02, 14.0MB/s]
Downloading:  92%|#########1| 403M/440M [00:29&lt;00:02, 13.9MB/s]
Downloading:  92%|#########1| 404M/440M [00:30&lt;00:02, 13.9MB/s]
Downloading:  92%|#########2| 406M/440M [00:30&lt;00:02, 14.2MB/s]
Downloading:  92%|#########2| 407M/440M [00:30&lt;00:02, 14.0MB/s]
Downloading:  93%|#########2| 409M/440M [00:30&lt;00:02, 14.0MB/s]
Downloading:  93%|#########3| 410M/440M [00:30&lt;00:02, 14.2MB/s]
Downloading:  93%|#########3| 412M/440M [00:30&lt;00:02, 14.0MB/s]
Downloading:  94%|#########3| 413M/440M [00:30&lt;00:02, 13.4MB/s]
Downloading:  94%|#########4| 415M/440M [00:30&lt;00:01, 13.7MB/s]
Downloading:  94%|#########4| 416M/440M [00:30&lt;00:01, 14.0MB/s]
Downloading:  95%|#########4| 418M/440M [00:31&lt;00:01, 14.5MB/s]
Downloading:  95%|#########5| 419M/440M [00:31&lt;00:01, 14.4MB/s]
Downloading:  95%|#########5| 421M/440M [00:31&lt;00:01, 14.6MB/s]
Downloading:  96%|#########5| 422M/440M [00:31&lt;00:01, 14.1MB/s]
Downloading:  96%|#########6| 424M/440M [00:31&lt;00:01, 14.3MB/s]
Downloading:  97%|#########6| 425M/440M [00:31&lt;00:01, 14.5MB/s]
Downloading:  97%|#########6| 427M/440M [00:31&lt;00:00, 14.2MB/s]
Downloading:  97%|#########7| 428M/440M [00:31&lt;00:00, 14.0MB/s]
Downloading:  97%|#########7| 429M/440M [00:31&lt;00:00, 14.1MB/s]
Downloading:  98%|#########7| 431M/440M [00:31&lt;00:00, 14.0MB/s]
Downloading:  98%|#########8| 432M/440M [00:32&lt;00:00, 14.0MB/s]
Downloading:  98%|#########8| 434M/440M [00:32&lt;00:00, 13.9MB/s]
Downloading:  99%|#########8| 435M/440M [00:32&lt;00:00, 13.7MB/s]
Downloading:  99%|#########9| 436M/440M [00:32&lt;00:00, 13.6MB/s]
Downloading:  99%|#########9| 438M/440M [00:32&lt;00:00, 13.7MB/s]
Downloading: 100%|#########9| 439M/440M [00:32&lt;00:00, 13.4MB/s]
Downloading: 100%|##########| 440M/440M [00:32&lt;00:00, 13.5MB/s]</code></pre>
<div class="sourceCode" id="cb24"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb24-1"><a href="#cb24-1" aria-hidden="true"></a><span class="kw">class</span> config:</span>
<span id="cb24-2"><a href="#cb24-2" aria-hidden="true"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>):</span>
<span id="cb24-3"><a href="#cb24-3" aria-hidden="true"></a>        config.num_classes <span class="op">=</span> <span class="dv">2</span>  <span class="co"># binary</span></span>
<span id="cb24-4"><a href="#cb24-4" aria-hidden="true"></a>        config.output_channel <span class="op">=</span> <span class="dv">16</span>  <span class="co"># number of kernels</span></span>
<span id="cb24-5"><a href="#cb24-5" aria-hidden="true"></a>        config.embedding_dim <span class="op">=</span> <span class="dv">768</span>  <span class="co"># embed dimension</span></span>
<span id="cb24-6"><a href="#cb24-6" aria-hidden="true"></a>        config.dropout <span class="op">=</span> <span class="fl">0.4</span>  <span class="co"># dropout value</span></span>
<span id="cb24-7"><a href="#cb24-7" aria-hidden="true"></a>        <span class="cf">return</span> <span class="va">None</span></span>
<span id="cb24-8"><a href="#cb24-8" aria-hidden="true"></a></span>
<span id="cb24-9"><a href="#cb24-9" aria-hidden="true"></a></span>
<span id="cb24-10"><a href="#cb24-10" aria-hidden="true"></a><span class="co"># create config</span></span>
<span id="cb24-11"><a href="#cb24-11" aria-hidden="true"></a>config1 <span class="op">=</span> config()</span>
<span id="cb24-12"><a href="#cb24-12" aria-hidden="true"></a></span>
<span id="cb24-13"><a href="#cb24-13" aria-hidden="true"></a><span class="co"># instantiate CNN</span></span>
<span id="cb24-14"><a href="#cb24-14" aria-hidden="true"></a>kim_model <span class="op">=</span> KimCNN(config1).cuda()</span>
<span id="cb24-15"><a href="#cb24-15" aria-hidden="true"></a></span>
<span id="cb24-16"><a href="#cb24-16" aria-hidden="true"></a><span class="co"># set loss</span></span>
<span id="cb24-17"><a href="#cb24-17" aria-hidden="true"></a>criterion <span class="op">=</span> nn.CrossEntropyLoss()</span>
<span id="cb24-18"><a href="#cb24-18" aria-hidden="true"></a></span>
<span id="cb24-19"><a href="#cb24-19" aria-hidden="true"></a><span class="co"># set number of epochs</span></span>
<span id="cb24-20"><a href="#cb24-20" aria-hidden="true"></a>epochs <span class="op">=</span> <span class="dv">4</span></span>
<span id="cb24-21"><a href="#cb24-21" aria-hidden="true"></a></span>
<span id="cb24-22"><a href="#cb24-22" aria-hidden="true"></a><span class="co"># only train the last 4 layers; saves ~600mb of GPU mem and 30s of compute</span></span>
<span id="cb24-23"><a href="#cb24-23" aria-hidden="true"></a>BERT_parameters <span class="op">=</span> []</span>
<span id="cb24-24"><a href="#cb24-24" aria-hidden="true"></a>allowed_layers <span class="op">=</span> [<span class="dv">11</span>, <span class="dv">10</span>, <span class="dv">9</span>, <span class="dv">8</span>]</span>
<span id="cb24-25"><a href="#cb24-25" aria-hidden="true"></a></span>
<span id="cb24-26"><a href="#cb24-26" aria-hidden="true"></a><span class="cf">for</span> name, param <span class="kw">in</span> model.named_parameters():</span>
<span id="cb24-27"><a href="#cb24-27" aria-hidden="true"></a>    <span class="cf">for</span> layer_num <span class="kw">in</span> allowed_layers:</span>
<span id="cb24-28"><a href="#cb24-28" aria-hidden="true"></a>        layer_num <span class="op">=</span> <span class="bu">str</span>(layer_num)</span>
<span id="cb24-29"><a href="#cb24-29" aria-hidden="true"></a>        <span class="cf">if</span> <span class="st">&quot;.</span><span class="sc">{}</span><span class="st">.&quot;</span>.<span class="bu">format</span>(layer_num) <span class="kw">in</span> name:</span>
<span id="cb24-30"><a href="#cb24-30" aria-hidden="true"></a>            BERT_parameters.append(param)</span>
<span id="cb24-31"><a href="#cb24-31" aria-hidden="true"></a></span>
<span id="cb24-32"><a href="#cb24-32" aria-hidden="true"></a><span class="co"># set optimizer</span></span>
<span id="cb24-33"><a href="#cb24-33" aria-hidden="true"></a>optimizer <span class="op">=</span> AdamW([{<span class="st">&#39;params&#39;</span>: BERT_parameters, <span class="st">&#39;lr&#39;</span>: <span class="fl">2e-5</span>}], weight_decay<span class="op">=</span><span class="fl">1.0</span>)</span>
<span id="cb24-34"><a href="#cb24-34" aria-hidden="true"></a></span>
<span id="cb24-35"><a href="#cb24-35" aria-hidden="true"></a></span>
<span id="cb24-36"><a href="#cb24-36" aria-hidden="true"></a><span class="co"># set LR scheduler</span></span>
<span id="cb24-37"><a href="#cb24-37" aria-hidden="true"></a>total_steps <span class="op">=</span> <span class="bu">len</span>(train_dataloader) <span class="op">*</span> epochs</span>
<span id="cb24-38"><a href="#cb24-38" aria-hidden="true"></a>scheduler <span class="op">=</span> get_linear_schedule_with_warmup(optimizer,</span>
<span id="cb24-39"><a href="#cb24-39" aria-hidden="true"></a>                                            num_warmup_steps<span class="op">=</span><span class="dv">0</span>,</span>
<span id="cb24-40"><a href="#cb24-40" aria-hidden="true"></a>                                            num_training_steps<span class="op">=</span>total_steps)</span>
<span id="cb24-41"><a href="#cb24-41" aria-hidden="true"></a></span>
<span id="cb24-42"><a href="#cb24-42" aria-hidden="true"></a><span class="co"># create gradient scaler for mixed precision</span></span>
<span id="cb24-43"><a href="#cb24-43" aria-hidden="true"></a>scaler <span class="op">=</span> GradScaler()</span></code></pre></div>
</div>
<div id="train" class="section level2" number="1.9">
<h2 number="1.9"><span class="header-section-number">1.9</span> Train</h2>
<p>Finally we are ready to train. Two containers are created to store the results of each training and validation epoch</p>
<div class="sourceCode" id="cb25"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb25-1"><a href="#cb25-1" aria-hidden="true"></a><span class="co"># create training result storage</span></span>
<span id="cb25-2"><a href="#cb25-2" aria-hidden="true"></a>training_stats <span class="op">=</span> []</span>
<span id="cb25-3"><a href="#cb25-3" aria-hidden="true"></a>valid_stats <span class="op">=</span> []</span>
<span id="cb25-4"><a href="#cb25-4" aria-hidden="true"></a>best_valid_loss <span class="op">=</span> <span class="bu">float</span>(<span class="st">&#39;inf&#39;</span>)</span>
<span id="cb25-5"><a href="#cb25-5" aria-hidden="true"></a></span>
<span id="cb25-6"><a href="#cb25-6" aria-hidden="true"></a><span class="co"># for each epoch</span></span>
<span id="cb25-7"><a href="#cb25-7" aria-hidden="true"></a><span class="cf">for</span> epoch <span class="kw">in</span> <span class="bu">range</span>(epochs):</span>
<span id="cb25-8"><a href="#cb25-8" aria-hidden="true"></a>    <span class="co"># train</span></span>
<span id="cb25-9"><a href="#cb25-9" aria-hidden="true"></a>    train(model, train_dataloader, optimizer)</span>
<span id="cb25-10"><a href="#cb25-10" aria-hidden="true"></a>    <span class="co"># validate</span></span>
<span id="cb25-11"><a href="#cb25-11" aria-hidden="true"></a>    validating(model, valid_dataloader)</span>
<span id="cb25-12"><a href="#cb25-12" aria-hidden="true"></a>    <span class="co"># check validation loss</span></span>
<span id="cb25-13"><a href="#cb25-13" aria-hidden="true"></a>    <span class="cf">if</span> valid_stats[epoch][<span class="st">&#39;Val Loss&#39;</span>] <span class="op">&lt;</span> best_valid_loss:</span>
<span id="cb25-14"><a href="#cb25-14" aria-hidden="true"></a>        best_valid_loss <span class="op">=</span> valid_stats[epoch][<span class="st">&#39;Val Loss&#39;</span>]</span>
<span id="cb25-15"><a href="#cb25-15" aria-hidden="true"></a>        <span class="co"># save best model for use later</span></span>
<span id="cb25-16"><a href="#cb25-16" aria-hidden="true"></a>        torch.save(model.state_dict(), <span class="st">&#39;bert-cnn-model1.pt&#39;</span>)  <span class="co"># torch save</span></span>
<span id="cb25-17"><a href="#cb25-17" aria-hidden="true"></a>        model_to_save <span class="op">=</span> model.module <span class="cf">if</span> <span class="bu">hasattr</span>(model, <span class="st">&#39;module&#39;</span>) <span class="cf">else</span> model</span>
<span id="cb25-18"><a href="#cb25-18" aria-hidden="true"></a>        model_to_save.save_pretrained(<span class="st">&#39;./model_save/bert-cnn/&#39;</span>)  <span class="co"># transformers save</span></span>
<span id="cb25-19"><a href="#cb25-19" aria-hidden="true"></a>        tokenizer.save_pretrained(<span class="st">&#39;./model_save/bert-cnn/&#39;</span>)  <span class="co"># transformers save</span></span></code></pre></div>
<pre><code>## 
## ======== Epoch 1 / 4 ========
## Training...
##   Batch    40  of  1,005.
##   Batch    80  of  1,005.
##   Batch   120  of  1,005.
##   Batch   160  of  1,005.
##   Batch   200  of  1,005.
##   Batch   240  of  1,005.
##   Batch   280  of  1,005.
##   Batch   320  of  1,005.
##   Batch   360  of  1,005.
##   Batch   400  of  1,005.
##   Batch   440  of  1,005.
##   Batch   480  of  1,005.
##   Batch   520  of  1,005.
##   Batch   560  of  1,005.
##   Batch   600  of  1,005.
##   Batch   640  of  1,005.
##   Batch   680  of  1,005.
##   Batch   720  of  1,005.
##   Batch   760  of  1,005.
##   Batch   800  of  1,005.
##   Batch   840  of  1,005.
##   Batch   880  of  1,005.
##   Batch   920  of  1,005.
##   Batch   960  of  1,005.
##   Batch 1,000  of  1,005.
## 
## summary results
## epoch | trn loss | trn f1 | trn time 
##     1 | 0.39960 | 0.83102 | 0:12:21
## 
## Running Validation...
## 
## summary results
## epoch | val loss | val f1 | val time
##     1 | 0.27511 | 0.84472 | 0:00:17
## (&#39;./model_save/bert-cnn/vocab.txt&#39;, &#39;./model_save/bert-cnn/special_tokens_map.json&#39;, &#39;./model_save/bert-cnn/added_tokens.json&#39;)
## 
## ======== Epoch 2 / 4 ========
## Training...
##   Batch    40  of  1,005.
##   Batch    80  of  1,005.
##   Batch   120  of  1,005.
##   Batch   160  of  1,005.
##   Batch   200  of  1,005.
##   Batch   240  of  1,005.
##   Batch   280  of  1,005.
##   Batch   320  of  1,005.
##   Batch   360  of  1,005.
##   Batch   400  of  1,005.
##   Batch   440  of  1,005.
##   Batch   480  of  1,005.
##   Batch   520  of  1,005.
##   Batch   560  of  1,005.
##   Batch   600  of  1,005.
##   Batch   640  of  1,005.
##   Batch   680  of  1,005.
##   Batch   720  of  1,005.
##   Batch   760  of  1,005.
##   Batch   800  of  1,005.
##   Batch   840  of  1,005.
##   Batch   880  of  1,005.
##   Batch   920  of  1,005.
##   Batch   960  of  1,005.
##   Batch 1,000  of  1,005.
## 
## summary results
## epoch | trn loss | trn f1 | trn time 
##     2 | 0.29237 | 0.88316 | 0:12:24
## 
## Running Validation...
## 
## summary results
## epoch | val loss | val f1 | val time
##     2 | 0.30805 | 0.82737 | 0:00:17
## 
## ======== Epoch 3 / 4 ========
## Training...
##   Batch    40  of  1,005.
##   Batch    80  of  1,005.
##   Batch   120  of  1,005.
##   Batch   160  of  1,005.
##   Batch   200  of  1,005.
##   Batch   240  of  1,005.
##   Batch   280  of  1,005.
##   Batch   320  of  1,005.
##   Batch   360  of  1,005.
##   Batch   400  of  1,005.
##   Batch   440  of  1,005.
##   Batch   480  of  1,005.
##   Batch   520  of  1,005.
##   Batch   560  of  1,005.
##   Batch   600  of  1,005.
##   Batch   640  of  1,005.
##   Batch   680  of  1,005.
##   Batch   720  of  1,005.
##   Batch   760  of  1,005.
##   Batch   800  of  1,005.
##   Batch   840  of  1,005.
##   Batch   880  of  1,005.
##   Batch   920  of  1,005.
##   Batch   960  of  1,005.
##   Batch 1,000  of  1,005.
## 
## summary results
## epoch | trn loss | trn f1 | trn time 
##     3 | 0.27105 | 0.89014 | 0:12:11
## 
## Running Validation...
## 
## summary results
## epoch | val loss | val f1 | val time
##     3 | 0.30127 | 0.84481 | 0:00:17
## 
## ======== Epoch 4 / 4 ========
## Training...
##   Batch    40  of  1,005.
##   Batch    80  of  1,005.
##   Batch   120  of  1,005.
##   Batch   160  of  1,005.
##   Batch   200  of  1,005.
##   Batch   240  of  1,005.
##   Batch   280  of  1,005.
##   Batch   320  of  1,005.
##   Batch   360  of  1,005.
##   Batch   400  of  1,005.
##   Batch   440  of  1,005.
##   Batch   480  of  1,005.
##   Batch   520  of  1,005.
##   Batch   560  of  1,005.
##   Batch   600  of  1,005.
##   Batch   640  of  1,005.
##   Batch   680  of  1,005.
##   Batch   720  of  1,005.
##   Batch   760  of  1,005.
##   Batch   800  of  1,005.
##   Batch   840  of  1,005.
##   Batch   880  of  1,005.
##   Batch   920  of  1,005.
##   Batch   960  of  1,005.
##   Batch 1,000  of  1,005.
## 
## summary results
## epoch | trn loss | trn f1 | trn time 
##     4 | 0.25257 | 0.89834 | 0:11:23
## 
## Running Validation...
## 
## summary results
## epoch | val loss | val f1 | val time
##     4 | 0.27479 | 0.85058 | 0:00:16
## (&#39;./model_save/bert-cnn/vocab.txt&#39;, &#39;./model_save/bert-cnn/special_tokens_map.json&#39;, &#39;./model_save/bert-cnn/added_tokens.json&#39;)
## 
## C:\Users\Andrew\Anaconda3\envs\my_ml\lib\site-packages\sklearn\metrics\classification.py:1437: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
##   &#39;precision&#39;, &#39;predicted&#39;, average, warn_for)
## C:\Users\Andrew\Anaconda3\envs\my_ml\lib\site-packages\sklearn\metrics\classification.py:1437: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples.
##   &#39;precision&#39;, &#39;predicted&#39;, average, warn_for)</code></pre>
</div>
<div id="show-results" class="section level2" number="1.10">
<h2 number="1.10"><span class="header-section-number">1.10</span> Show Results</h2>
<p>After training, we organize the results nicely in <code>pandas</code>.</p>
<div class="sourceCode" id="cb27"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb27-1"><a href="#cb27-1" aria-hidden="true"></a><span class="co"># organize results</span></span>
<span id="cb27-2"><a href="#cb27-2" aria-hidden="true"></a>pd.set_option(<span class="st">&#39;precision&#39;</span>, <span class="dv">3</span>)</span>
<span id="cb27-3"><a href="#cb27-3" aria-hidden="true"></a>df_train_stats <span class="op">=</span> pd.DataFrame(data<span class="op">=</span>training_stats)</span>
<span id="cb27-4"><a href="#cb27-4" aria-hidden="true"></a>df_valid_stats <span class="op">=</span> pd.DataFrame(data<span class="op">=</span>valid_stats)</span>
<span id="cb27-5"><a href="#cb27-5" aria-hidden="true"></a>df_stats <span class="op">=</span> pd.concat([df_train_stats, df_valid_stats], axis<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb27-6"><a href="#cb27-6" aria-hidden="true"></a>df_stats.insert(<span class="dv">0</span>, <span class="st">&#39;Epoch&#39;</span>, <span class="bu">range</span>(<span class="dv">1</span>, <span class="bu">len</span>(df_stats)<span class="op">+</span><span class="dv">1</span>))</span>
<span id="cb27-7"><a href="#cb27-7" aria-hidden="true"></a>df_stats <span class="op">=</span> df_stats.set_index(<span class="st">&#39;Epoch&#39;</span>)</span>
<span id="cb27-8"><a href="#cb27-8" aria-hidden="true"></a>df_stats</span></code></pre></div>
<pre><code>##        Train Loss  Train F1 Train Time  ...  Val recall  Val F1  Val Time
## Epoch                                   ...                              
## 1           0.400     0.831    0:12:21  ...       0.861   0.845   0:00:17
## 2           0.292     0.883    0:12:24  ...       0.850   0.827   0:00:17
## 3           0.271     0.890    0:12:11  ...       0.862   0.845   0:00:17
## 4           0.253     0.898    0:11:23  ...       0.867   0.851   0:00:16
## 
## [4 rows x 9 columns]</code></pre>
</div>
<div id="test-the-model" class="section level2" number="1.11">
<h2 number="1.11"><span class="header-section-number">1.11</span> Test the Model</h2>
<p>And lastly we run our final test:</p>
<div class="sourceCode" id="cb29"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb29-1"><a href="#cb29-1" aria-hidden="true"></a><span class="co"># test the model</span></span>
<span id="cb29-2"><a href="#cb29-2" aria-hidden="true"></a>test_stats <span class="op">=</span> []</span>
<span id="cb29-3"><a href="#cb29-3" aria-hidden="true"></a>model.load_state_dict(torch.load(<span class="st">&#39;bert-cnn-model1.pt&#39;</span>))</span></code></pre></div>
<pre><code>## &lt;All keys matched successfully&gt;</code></pre>
<div class="sourceCode" id="cb31"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb31-1"><a href="#cb31-1" aria-hidden="true"></a>testing(model, test_dataloader)</span></code></pre></div>
<pre><code>## 
## Running Testing...
## 
## summary results
## epoch | test loss | test f1 | test time
##     4 | 0.31259 | 0.83967 | 0:00:16
## 
## C:\Users\Andrew\Anaconda3\envs\my_ml\lib\site-packages\sklearn\metrics\classification.py:1437: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
##   &#39;precision&#39;, &#39;predicted&#39;, average, warn_for)
## C:\Users\Andrew\Anaconda3\envs\my_ml\lib\site-packages\sklearn\metrics\classification.py:1437: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples.
##   &#39;precision&#39;, &#39;predicted&#39;, average, warn_for)</code></pre>
<div class="sourceCode" id="cb33"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb33-1"><a href="#cb33-1" aria-hidden="true"></a>df_test_stats <span class="op">=</span> pd.DataFrame(data<span class="op">=</span>test_stats)</span>
<span id="cb33-2"><a href="#cb33-2" aria-hidden="true"></a>df_test_stats</span></code></pre></div>
<pre><code>##    Test Loss  Test Accur.  Test precision  Test recall  Test F1 Test Time
## 0      0.313        0.854           0.862        0.854     0.84   0:00:16</code></pre>
<p>The results show a slight improvement over our standard BERT model at the cost of 4-5x the training time.</p>
</div>
</div>
<div id="sources" class="section level1" number="2">
<h1 number="2"><span class="header-section-number">2</span> Sources</h1>
<ul>
<li><p>Kim, Yoon. Convolutional neural networks for sentence classification. arXiv preprint arXiv:1408.5882 (2014).</p></li>
<li><p>Devlin, Jacob, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805 (2018).</p></li>
</ul>
</div>



<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>
