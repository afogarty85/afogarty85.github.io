<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml">

<head>

<meta charset="utf-8" />
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="generator" content="pandoc" />

<meta name="viewport" content="width=device-width, initial-scale=1">

<meta name="author" content="Andrew Fogarty" />

<meta name="date" content="2020-10-06" />

<title>Feature Normalization and Initialization</title>



<style type="text/css">code{white-space: pre;}</style>
<style type="text/css" data-origin="pandoc">
a.sourceLine { display: inline-block; line-height: 1.25; }
a.sourceLine { pointer-events: none; color: inherit; text-decoration: inherit; }
a.sourceLine:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
a.sourceLine { text-indent: -1em; padding-left: 1em; }
}
pre.numberSource a.sourceLine
  { position: relative; left: -4em; }
pre.numberSource a.sourceLine::before
  { content: attr(title);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; pointer-events: all; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {  }
@media screen {
a.sourceLine::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */

</style>
<script>
// apply pandoc div.sourceCode style to pre.sourceCode instead
(function() {
  var sheets = document.styleSheets;
  for (var i = 0; i < sheets.length; i++) {
    if (sheets[i].ownerNode.dataset["origin"] !== "pandoc") continue;
    try { var rules = sheets[i].cssRules; } catch (e) { continue; }
    for (var j = 0; j < rules.length; j++) {
      var rule = rules[j];
      // check if there is a div.sourceCode rule
      if (rule.type !== rule.STYLE_RULE || rule.selectorText !== "div.sourceCode") continue;
      var style = rule.style.cssText;
      // check if color or background-color is set
      if (rule.style.color === '' || rule.style.backgroundColor === '') continue;
      // replace div.sourceCode by a pre.sourceCode rule
      sheets[i].deleteRule(j);
      sheets[i].insertRule('pre.sourceCode{' + style + '}', j);
    }
  }
})();
</script>



<style type="text/css">body {
background-color: #fff;
margin: 1em auto;
max-width: 700px;
overflow: visible;
padding-left: 2em;
padding-right: 2em;
font-family: "Open Sans", "Helvetica Neue", Helvetica, Arial, sans-serif;
font-size: 14px;
line-height: 1.35;
}
#header {
text-align: center;
}
#TOC {
clear: both;
margin: 0 0 10px 10px;
padding: 4px;
width: 400px;
border: 1px solid #CCCCCC;
border-radius: 5px;
background-color: #f6f6f6;
font-size: 13px;
line-height: 1.3;
}
#TOC .toctitle {
font-weight: bold;
font-size: 15px;
margin-left: 5px;
}
#TOC ul {
padding-left: 40px;
margin-left: -1.5em;
margin-top: 5px;
margin-bottom: 5px;
}
#TOC ul ul {
margin-left: -2em;
}
#TOC li {
line-height: 16px;
}
table {
margin: 1em auto;
border-width: 1px;
border-color: #DDDDDD;
border-style: outset;
border-collapse: collapse;
}
table th {
border-width: 2px;
padding: 5px;
border-style: inset;
}
table td {
border-width: 1px;
border-style: inset;
line-height: 18px;
padding: 5px 5px;
}
table, table th, table td {
border-left-style: none;
border-right-style: none;
}
table thead, table tr.even {
background-color: #f7f7f7;
}
p {
margin: 0.5em 0;
}
blockquote {
background-color: #f6f6f6;
padding: 0.25em 0.75em;
}
hr {
border-style: solid;
border: none;
border-top: 1px solid #777;
margin: 28px 0;
}
dl {
margin-left: 0;
}
dl dd {
margin-bottom: 13px;
margin-left: 13px;
}
dl dt {
font-weight: bold;
}
ul {
margin-top: 0;
}
ul li {
list-style: circle outside;
}
ul ul {
margin-bottom: 0;
}
pre, code {
background-color: #f7f7f7;
border-radius: 3px;
color: #333;
white-space: pre-wrap; 
}
pre {
border-radius: 3px;
margin: 5px 0px 10px 0px;
padding: 10px;
}
pre:not([class]) {
background-color: #f7f7f7;
}
code {
font-family: Consolas, Monaco, 'Courier New', monospace;
font-size: 85%;
}
p > code, li > code {
padding: 2px 0px;
}
div.figure {
text-align: center;
}
img {
background-color: #FFFFFF;
padding: 2px;
border: 1px solid #DDDDDD;
border-radius: 3px;
border: 1px solid #CCCCCC;
margin: 0 5px;
}
h1 {
margin-top: 0;
font-size: 35px;
line-height: 40px;
}
h2 {
border-bottom: 4px solid #f7f7f7;
padding-top: 10px;
padding-bottom: 2px;
font-size: 145%;
}
h3 {
border-bottom: 2px solid #f7f7f7;
padding-top: 10px;
font-size: 120%;
}
h4 {
border-bottom: 1px solid #f7f7f7;
margin-left: 8px;
font-size: 105%;
}
h5, h6 {
border-bottom: 1px solid #ccc;
font-size: 105%;
}
a {
color: #0033dd;
text-decoration: none;
}
a:hover {
color: #6666ff; }
a:visited {
color: #800080; }
a:visited:hover {
color: #BB00BB; }
a[href^="http:"] {
text-decoration: underline; }
a[href^="https:"] {
text-decoration: underline; }

code > span.kw { color: #555; font-weight: bold; } 
code > span.dt { color: #902000; } 
code > span.dv { color: #40a070; } 
code > span.bn { color: #d14; } 
code > span.fl { color: #d14; } 
code > span.ch { color: #d14; } 
code > span.st { color: #d14; } 
code > span.co { color: #888888; font-style: italic; } 
code > span.ot { color: #007020; } 
code > span.al { color: #ff0000; font-weight: bold; } 
code > span.fu { color: #900; font-weight: bold; }  code > span.er { color: #a61717; background-color: #e3d2d2; } 
</style>




</head>

<body>




<h1 class="title toc-ignore">Feature Normalization and Initialization</h1>
<h4 class="author">Andrew Fogarty</h4>
<h4 class="date">10/06/2020</h4>


<div id="TOC">
<ul>
<li><a href="#introduction"><span class="toc-section-number">1</span> Introduction</a><ul>
<li><a href="#feature-normalization"><span class="toc-section-number">1.1</span> Feature Normalization</a><ul>
<li><a href="#batch-normalization"><span class="toc-section-number">1.1.1</span> Batch Normalization</a></li>
<li><a href="#batch-normalization-in-pytorch"><span class="toc-section-number">1.1.2</span> Batch Normalization in PyTorch</a></li>
</ul></li>
<li><a href="#weight-initialization"><span class="toc-section-number">1.2</span> Weight Initialization</a><ul>
<li><a href="#xavier-weight-initialization"><span class="toc-section-number">1.2.1</span> Xavier Weight Initialization</a></li>
</ul></li>
<li><a href="#feature-normalization-and-weight-initialization-in-practice"><span class="toc-section-number">1.3</span> Feature Normalization and Weight Initialization in Practice</a><ul>
<li><a href="#batch-normalization-1"><span class="toc-section-number">1.3.1</span> Batch Normalization</a></li>
<li><a href="#xavier-initialization"><span class="toc-section-number">1.3.2</span> Xavier Initialization</a></li>
</ul></li>
</ul></li>
<li><a href="#sources"><span class="toc-section-number">2</span> Sources</a></li>
</ul>
</div>

<div class="sourceCode" id="cb1"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1-1" title="1"><span class="co"># load python</span></a>
<a class="sourceLine" id="cb1-2" title="2"><span class="kw">library</span>(reticulate)</a>
<a class="sourceLine" id="cb1-3" title="3"><span class="kw">use_condaenv</span>(<span class="st">&quot;my_ml&quot;</span>)</a></code></pre></div>
<div class="sourceCode" id="cb2"><pre class="sourceCode python"><code class="sourceCode python"><a class="sourceLine" id="cb2-1" title="1"><span class="co"># load packages</span></a>
<a class="sourceLine" id="cb2-2" title="2"><span class="im">import</span> sys</a>
<a class="sourceLine" id="cb2-3" title="3">sys.path.append(<span class="st">&quot;C:/Users/Andrew/Desktop/Projects/Deep Learning/utils&quot;</span>)  <span class="co"># this is the folder with py files</span></a>
<a class="sourceLine" id="cb2-4" title="4"><span class="im">from</span> tools <span class="im">import</span> AverageMeter, ProgressBar <span class="co">#scriptName without .py extension; import each class</span></a>
<a class="sourceLine" id="cb2-5" title="5"><span class="im">from</span> radam <span class="im">import</span> RAdam</a>
<a class="sourceLine" id="cb2-6" title="6"><span class="im">import</span> torch</a>
<a class="sourceLine" id="cb2-7" title="7"><span class="im">import</span> torch.nn <span class="im">as</span> nn</a>
<a class="sourceLine" id="cb2-8" title="8"><span class="im">import</span> torch.nn.functional <span class="im">as</span> F</a>
<a class="sourceLine" id="cb2-9" title="9"><span class="im">from</span> transformers <span class="im">import</span> get_linear_schedule_with_warmup, AdamW</a>
<a class="sourceLine" id="cb2-10" title="10"><span class="im">from</span> torch.utils.data <span class="im">import</span> TensorDataset, random_split, DataLoader, RandomSampler, SequentialSampler</a>
<a class="sourceLine" id="cb2-11" title="11"><span class="im">import</span> time, datetime, random, re, os</a>
<a class="sourceLine" id="cb2-12" title="12"><span class="im">import</span> pandas <span class="im">as</span> pd</a>
<a class="sourceLine" id="cb2-13" title="13"><span class="im">import</span> numpy <span class="im">as</span> np</a>
<a class="sourceLine" id="cb2-14" title="14"><span class="im">from</span> sklearn.metrics <span class="im">import</span> accuracy_score, f1_score, precision_score, recall_score, confusion_matrix</a>
<a class="sourceLine" id="cb2-15" title="15"><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</a>
<a class="sourceLine" id="cb2-16" title="16"><span class="im">import</span> seaborn <span class="im">as</span> sns</a>
<a class="sourceLine" id="cb2-17" title="17"><span class="im">from</span> torch.cuda.amp <span class="im">import</span> autocast, GradScaler</a>
<a class="sourceLine" id="cb2-18" title="18"><span class="im">from</span> sklearn.model_selection <span class="im">import</span> train_test_split</a>
<a class="sourceLine" id="cb2-19" title="19"><span class="im">from</span> torch.utils.data <span class="im">import</span> Dataset, Subset</a>
<a class="sourceLine" id="cb2-20" title="20"><span class="im">from</span> sklearn.preprocessing <span class="im">import</span> LabelEncoder</a>
<a class="sourceLine" id="cb2-21" title="21"><span class="im">from</span> torchvision <span class="im">import</span> transforms</a>
<a class="sourceLine" id="cb2-22" title="22"></a>
<a class="sourceLine" id="cb2-23" title="23"><span class="co"># set seed and gpu requirements</span></a>
<a class="sourceLine" id="cb2-24" title="24">SEED <span class="op">=</span> <span class="dv">15</span></a>
<a class="sourceLine" id="cb2-25" title="25">random.seed(SEED)</a>
<a class="sourceLine" id="cb2-26" title="26">np.random.seed(SEED)</a>
<a class="sourceLine" id="cb2-27" title="27">torch.manual_seed(SEED)</a></code></pre></div>
<pre><code>## &lt;torch._C.Generator object at 0x000000001FB50370&gt;</code></pre>
<div class="sourceCode" id="cb4"><pre class="sourceCode python"><code class="sourceCode python"><a class="sourceLine" id="cb4-1" title="1">torch.backends.cudnn.deterministic <span class="op">=</span> <span class="va">True</span></a>
<a class="sourceLine" id="cb4-2" title="2">torch.cuda.amp.autocast(enabled<span class="op">=</span><span class="va">True</span>)</a>
<a class="sourceLine" id="cb4-3" title="3"></a>
<a class="sourceLine" id="cb4-4" title="4"><span class="co"># set gpu/cpu</span></a></code></pre></div>
<pre><code>## &lt;torch.cuda.amp.autocast_mode.autocast object at 0x0000000033ADCC48&gt;</code></pre>
<div class="sourceCode" id="cb6"><pre class="sourceCode python"><code class="sourceCode python"><a class="sourceLine" id="cb6-1" title="1">DEVICE <span class="op">=</span> torch.device(<span class="st">&#39;cuda:0&#39;</span> <span class="cf">if</span> torch.cuda.is_available() <span class="cf">else</span> <span class="st">&#39;cpu&#39;</span>)</a></code></pre></div>
<div id="introduction" class="section level1">
<h1><span class="header-section-number">1</span> Introduction</h1>
<p>In this guide, we will walk through feature normalization and weight initialization schemes in PyTorch. In short, we normalize our inputs for gradient descent because large weights will dominate our updates in our attempt to find global or local minima. Separately, we use custom weight initialization schemes to improve our ability to converge during optimization or to improve our ability to use certain activation functions.</p>
<div id="feature-normalization" class="section level2">
<h2><span class="header-section-number">1.1</span> Feature Normalization</h2>
<p>Normalization is also known as standardization which means that our features will be scaled to have zero mean and unit variance. However, normalizing our inputs only affects the first hidden layer of a neural network. To solve this problem, researchers invented batch normalization.</p>
<div id="batch-normalization" class="section level3">
<h3><span class="header-section-number">1.1.1</span> Batch Normalization</h3>
<p>Batch normalization normalizes the inputs of hidden layers which in turn: (1) reduce exploding/vanishing gradients, and (2) increases stability and convergence rate. Two parameters, <span class="math inline">\(\gamma\)</span> and <span class="math inline">\(\beta\)</span> are two parameters (variance and mean respectively) that can learn to perform standardization with zero mean and unit variance.</p>
<p>By using batch normalization, its <span class="math inline">\(\beta\)</span> parameter makes intercepts redundant.</p>
</div>
<div id="batch-normalization-in-pytorch" class="section level3">
<h3><span class="header-section-number">1.1.2</span> Batch Normalization in PyTorch</h3>
<p>Batch normalization can be applied in two ways. The first and most common way is this:</p>
<ul>
<li>Compute net inputs -&gt; BatchNorm -&gt; Activation Function -&gt; Compute next layer net inputs</li>
</ul>
<p>Another viable option, with some evidence suggesting better performance, is as follows:</p>
<ul>
<li>Compute net inputs -&gt; Activation Function -&gt; BatchNorm -&gt; Compute next layer net inputs</li>
</ul>
<div class="sourceCode" id="cb7"><pre class="sourceCode python"><code class="sourceCode python"><a class="sourceLine" id="cb7-1" title="1"><span class="kw">class</span> FF_NN(torch.nn.Module):</a>
<a class="sourceLine" id="cb7-2" title="2">    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, num_features, num_classes):</a>
<a class="sourceLine" id="cb7-3" title="3">        <span class="bu">super</span>(FF_NN, <span class="va">self</span>).<span class="fu">__init__</span>()</a>
<a class="sourceLine" id="cb7-4" title="4">        <span class="co"># initialize 3 layers</span></a>
<a class="sourceLine" id="cb7-5" title="5">        <span class="co"># first hidden layer</span></a>
<a class="sourceLine" id="cb7-6" title="6">        <span class="va">self</span>.linear_1 <span class="op">=</span> torch.nn.Linear(num_features, num_hidden_1)</a>
<a class="sourceLine" id="cb7-7" title="7">        <span class="va">self</span>.linear_1_bn <span class="op">=</span> torch.nn.BatchNorm1d(num_hidden_1)</a>
<a class="sourceLine" id="cb7-8" title="8">        <span class="co"># second hidden layer</span></a>
<a class="sourceLine" id="cb7-9" title="9">        <span class="va">self</span>.linear_2 <span class="op">=</span> torch.nn.Linear(num_hidden_1, num_hidden_2)</a>
<a class="sourceLine" id="cb7-10" title="10">        <span class="va">self</span>.linear_2_bn <span class="op">=</span> torch.nn.BatchNorm1d(num_hidden_2)</a>
<a class="sourceLine" id="cb7-11" title="11">        <span class="co"># output layer</span></a>
<a class="sourceLine" id="cb7-12" title="12">        <span class="va">self</span>.linear_out <span class="op">=</span> torch.nn.Linear(num_hidden_2, num_classes)</a>
<a class="sourceLine" id="cb7-13" title="13"></a>
<a class="sourceLine" id="cb7-14" title="14">    <span class="co"># define how and what order model parameters should be used in forward prop.</span></a>
<a class="sourceLine" id="cb7-15" title="15">    <span class="kw">def</span> forward(<span class="va">self</span>, x):</a>
<a class="sourceLine" id="cb7-16" title="16">        <span class="co"># run inputs through first layer</span></a>
<a class="sourceLine" id="cb7-17" title="17">        out <span class="op">=</span> <span class="va">self</span>.linear_1(x)</a>
<a class="sourceLine" id="cb7-18" title="18">        <span class="co"># apply relu</span></a>
<a class="sourceLine" id="cb7-19" title="19">        out <span class="op">=</span> F.relu(out)</a>
<a class="sourceLine" id="cb7-20" title="20">        <span class="co"># apply batchnorm</span></a>
<a class="sourceLine" id="cb7-21" title="21">        out <span class="op">=</span> <span class="va">self</span>.linear_1_bn(out)</a>
<a class="sourceLine" id="cb7-22" title="22">        <span class="co"># run inputs through second layer</span></a>
<a class="sourceLine" id="cb7-23" title="23">        out <span class="op">=</span> <span class="va">self</span>.linear_2(out)</a>
<a class="sourceLine" id="cb7-24" title="24">        <span class="co"># apply relu</span></a>
<a class="sourceLine" id="cb7-25" title="25">        out <span class="op">=</span> F.relu(out)</a>
<a class="sourceLine" id="cb7-26" title="26">        <span class="co"># apply batchnorm</span></a>
<a class="sourceLine" id="cb7-27" title="27">        out <span class="op">=</span> <span class="va">self</span>.linear_2_bn(out)</a>
<a class="sourceLine" id="cb7-28" title="28">        <span class="co"># run inputs through final classification layer</span></a>
<a class="sourceLine" id="cb7-29" title="29">        logits <span class="op">=</span> <span class="va">self</span>.linear_out(out)</a>
<a class="sourceLine" id="cb7-30" title="30">        probs <span class="op">=</span> F.log_softmax(logits, dim<span class="op">=</span><span class="dv">1</span>)</a>
<a class="sourceLine" id="cb7-31" title="31">        <span class="cf">return</span> logits, probs</a></code></pre></div>
</div>
</div>
<div id="weight-initialization" class="section level2">
<h2><span class="header-section-number">1.2</span> Weight Initialization</h2>
<p>Traditionally, we can initialize weights by sampling from a random uniform distribution in a range between [0, 1], or better, [-0.5, 0.5]. Alternatively, we could choose a Gaussian distribution with a mean of 0 and a small variance of 0.1 or 0.01. Separately, we can initialize all the intercepts to zeros.</p>
<p>In PyTorch, custom weight initialization looks like this:</p>
<div class="sourceCode" id="cb8"><pre class="sourceCode python"><code class="sourceCode python"><a class="sourceLine" id="cb8-1" title="1"><span class="co"># nn.Module tells PyTorch to do backward propagation</span></a>
<a class="sourceLine" id="cb8-2" title="2"><span class="kw">class</span> FF_NN(torch.nn.Module):</a>
<a class="sourceLine" id="cb8-3" title="3">    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, num_features, num_classes):</a>
<a class="sourceLine" id="cb8-4" title="4">        <span class="bu">super</span>(FF_NN, <span class="va">self</span>).<span class="fu">__init__</span>()</a>
<a class="sourceLine" id="cb8-5" title="5">        <span class="co"># initialize 3 layers</span></a>
<a class="sourceLine" id="cb8-6" title="6">        <span class="co"># first hidden layer</span></a>
<a class="sourceLine" id="cb8-7" title="7">        <span class="va">self</span>.linear_1 <span class="op">=</span> torch.nn.Linear(num_features, num_hidden_1)</a>
<a class="sourceLine" id="cb8-8" title="8">        <span class="va">self</span>.linear_1.weight.detach().normal_(<span class="fl">0.0</span>, <span class="fl">0.1</span>)</a>
<a class="sourceLine" id="cb8-9" title="9">        <span class="va">self</span>.linear_1.bias.detach().zero_()</a>
<a class="sourceLine" id="cb8-10" title="10">        <span class="co"># second hidden layer</span></a>
<a class="sourceLine" id="cb8-11" title="11">        <span class="va">self</span>.linear_2 <span class="op">=</span> torch.nn.Linear(num_hidden_1, num_hidden_2)</a>
<a class="sourceLine" id="cb8-12" title="12">        <span class="va">self</span>.linear_2.weight.detach().normal_(<span class="fl">0.0</span>, <span class="fl">0.1</span>)</a>
<a class="sourceLine" id="cb8-13" title="13">        <span class="va">self</span>.linear_2.bias.detach().zero_()</a>
<a class="sourceLine" id="cb8-14" title="14">        <span class="co"># output layer</span></a>
<a class="sourceLine" id="cb8-15" title="15">        <span class="va">self</span>.linear_out <span class="op">=</span> torch.nn.Linear(num_hidden_2, num_classes)</a>
<a class="sourceLine" id="cb8-16" title="16">        <span class="va">self</span>.linear_out.weight.detach().normal_(<span class="fl">0.0</span>, <span class="fl">0.1</span>)</a>
<a class="sourceLine" id="cb8-17" title="17">        <span class="va">self</span>.linear_out.bias.detach().zero_()        </a>
<a class="sourceLine" id="cb8-18" title="18"></a>
<a class="sourceLine" id="cb8-19" title="19">    <span class="co"># define how and what order model parameters should be used in forward prop.</span></a>
<a class="sourceLine" id="cb8-20" title="20">    <span class="kw">def</span> forward(<span class="va">self</span>, x):</a>
<a class="sourceLine" id="cb8-21" title="21">        <span class="co"># run inputs through first layer</span></a>
<a class="sourceLine" id="cb8-22" title="22">        out <span class="op">=</span> <span class="va">self</span>.linear_1(x)</a>
<a class="sourceLine" id="cb8-23" title="23">        <span class="co"># apply relu</span></a>
<a class="sourceLine" id="cb8-24" title="24">        out <span class="op">=</span> F.relu(out)</a>
<a class="sourceLine" id="cb8-25" title="25">        <span class="co"># run inputs through second layer</span></a>
<a class="sourceLine" id="cb8-26" title="26">        out <span class="op">=</span> <span class="va">self</span>.linear_2(out)</a>
<a class="sourceLine" id="cb8-27" title="27">        <span class="co"># apply relu</span></a>
<a class="sourceLine" id="cb8-28" title="28">        out <span class="op">=</span> F.relu(out)</a>
<a class="sourceLine" id="cb8-29" title="29">        <span class="co"># run inputs through final classification layer</span></a>
<a class="sourceLine" id="cb8-30" title="30">        logits <span class="op">=</span> <span class="va">self</span>.linear_out(out)</a>
<a class="sourceLine" id="cb8-31" title="31">        probs <span class="op">=</span> F.log_softmax(logits, dim<span class="op">=</span><span class="dv">1</span>)</a>
<a class="sourceLine" id="cb8-32" title="32">        <span class="cf">return</span> logits, probs</a></code></pre></div>
<div id="xavier-weight-initialization" class="section level3">
<h3><span class="header-section-number">1.2.1</span> Xavier Weight Initialization</h3>
<p>To perform Xavier initialization, we first initialize weights from a Gaussian or uniform distribution and then we scale the weights proportional to the number of inputs to the layer. This means for that the first hidden layer, the number of inputs is the number of features. For the second hidden layer, the number of inputs is the number of neurons in the first hidden layer.</p>
<p>To apply this automatically in PyTorch, we add the following to our model:</p>
<div class="sourceCode" id="cb9"><pre class="sourceCode python"><code class="sourceCode python"><a class="sourceLine" id="cb9-1" title="1"><span class="kw">def</span> weights_init(m):</a>
<a class="sourceLine" id="cb9-2" title="2">    <span class="cf">if</span> <span class="bu">isinstance</span>(m, nn.Linear):</a>
<a class="sourceLine" id="cb9-3" title="3">        torch.nn.init.xavier_uniform_(m.weight)</a>
<a class="sourceLine" id="cb9-4" title="4">        torch.nn.init.xavier_uniform_(m.bias)</a>
<a class="sourceLine" id="cb9-5" title="5">model.<span class="bu">apply</span>(weights_init)</a></code></pre></div>
<p>PyTorch uses a weight initialization scheme similar to Xavier which is is as follows:</p>
<div class="sourceCode" id="cb10"><pre class="sourceCode python"><code class="sourceCode python"><a class="sourceLine" id="cb10-1" title="1"><span class="kw">def</span> reset_parameters(<span class="va">self</span>):</a>
<a class="sourceLine" id="cb10-2" title="2">    stdv <span class="op">=</span> <span class="fl">1.</span> <span class="op">/</span> math.sqrt(<span class="va">self</span>.weight.size(<span class="dv">1</span>))</a>
<a class="sourceLine" id="cb10-3" title="3">    <span class="va">self</span>.weight.data.uniform_(<span class="op">-</span>stdv, stdv)</a>
<a class="sourceLine" id="cb10-4" title="4">    <span class="cf">if</span> <span class="va">self</span>.bias <span class="kw">is</span> <span class="kw">not</span> <span class="va">None</span>:</a>
<a class="sourceLine" id="cb10-5" title="5">        <span class="va">self</span>.bias.data.uniform_(<span class="op">-</span>stdv, stdv)</a></code></pre></div>
<p>It is important to note that if we choose to use batch normalization, this means that initial feature weight choice is less important.</p>
</div>
</div>
<div id="feature-normalization-and-weight-initialization-in-practice" class="section level2">
<h2><span class="header-section-number">1.3</span> Feature Normalization and Weight Initialization in Practice</h2>
<p>In this section, we will apply the feature normalization and weight initialization functions on the MNIST data set.</p>
<div id="batch-normalization-1" class="section level3">
<h3><span class="header-section-number">1.3.1</span> Batch Normalization</h3>
<div class="sourceCode" id="cb11"><pre class="sourceCode python"><code class="sourceCode python"><a class="sourceLine" id="cb11-1" title="1"><span class="co"># create Dataset</span></a>
<a class="sourceLine" id="cb11-2" title="2"><span class="kw">class</span> CSVDataset(Dataset):</a>
<a class="sourceLine" id="cb11-3" title="3">    <span class="co">&quot;&quot;&quot;MNIST dataset.&quot;&quot;&quot;</span></a>
<a class="sourceLine" id="cb11-4" title="4"></a>
<a class="sourceLine" id="cb11-5" title="5">    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, csv_file, transform<span class="op">=</span><span class="va">None</span>):</a>
<a class="sourceLine" id="cb11-6" title="6">        <span class="co">&quot;&quot;&quot;</span></a>
<a class="sourceLine" id="cb11-7" title="7"><span class="co">        Args:</span></a>
<a class="sourceLine" id="cb11-8" title="8"><span class="co">            csv_file (string): Path to the csv file.</span></a>
<a class="sourceLine" id="cb11-9" title="9"><span class="co">            transform (callable, optional): Optional transform to be applied</span></a>
<a class="sourceLine" id="cb11-10" title="10"><span class="co">                on a sample.</span></a>
<a class="sourceLine" id="cb11-11" title="11"><span class="co">        &quot;&quot;&quot;</span></a>
<a class="sourceLine" id="cb11-12" title="12">        <span class="co"># initialize</span></a>
<a class="sourceLine" id="cb11-13" title="13">        <span class="va">self</span>.data_frame <span class="op">=</span> pd.read_csv(csv_file)</a>
<a class="sourceLine" id="cb11-14" title="14">        <span class="co"># all columns but the last</span></a>
<a class="sourceLine" id="cb11-15" title="15">        <span class="va">self</span>.features <span class="op">=</span> <span class="va">self</span>.data_frame[<span class="va">self</span>.data_frame.columns[:<span class="op">-</span><span class="dv">1</span>]]</a>
<a class="sourceLine" id="cb11-16" title="16">        <span class="co"># the last column</span></a>
<a class="sourceLine" id="cb11-17" title="17">        <span class="va">self</span>.target <span class="op">=</span> <span class="va">self</span>.data_frame[<span class="va">self</span>.data_frame.columns[<span class="op">-</span><span class="dv">1</span>]]</a>
<a class="sourceLine" id="cb11-18" title="18">        <span class="co"># initialize the transform if specified</span></a>
<a class="sourceLine" id="cb11-19" title="19">        <span class="va">self</span>.transform <span class="op">=</span> transform</a>
<a class="sourceLine" id="cb11-20" title="20"></a>
<a class="sourceLine" id="cb11-21" title="21">        <span class="co"># get length of df</span></a>
<a class="sourceLine" id="cb11-22" title="22">    <span class="kw">def</span> <span class="fu">__len__</span>(<span class="va">self</span>):</a>
<a class="sourceLine" id="cb11-23" title="23">        <span class="cf">return</span> <span class="bu">len</span>(<span class="va">self</span>.data_frame)</a>
<a class="sourceLine" id="cb11-24" title="24"></a>
<a class="sourceLine" id="cb11-25" title="25">        <span class="co"># get sample target</span></a>
<a class="sourceLine" id="cb11-26" title="26">    <span class="kw">def</span> __get_target__(<span class="va">self</span>):</a>
<a class="sourceLine" id="cb11-27" title="27">        <span class="cf">return</span> <span class="va">self</span>.target</a>
<a class="sourceLine" id="cb11-28" title="28"></a>
<a class="sourceLine" id="cb11-29" title="29">        <span class="co"># get df filtered by indices</span></a>
<a class="sourceLine" id="cb11-30" title="30">    <span class="kw">def</span> __get_values__(<span class="va">self</span>, indices):</a>
<a class="sourceLine" id="cb11-31" title="31">        <span class="cf">return</span> <span class="va">self</span>.data_frame.iloc[indices]</a>
<a class="sourceLine" id="cb11-32" title="32"></a>
<a class="sourceLine" id="cb11-33" title="33">    <span class="kw">def</span> <span class="fu">__getitem__</span>(<span class="va">self</span>, idx):</a>
<a class="sourceLine" id="cb11-34" title="34">        <span class="cf">if</span> torch.is_tensor(idx):</a>
<a class="sourceLine" id="cb11-35" title="35">            idx <span class="op">=</span> idx.tolist()</a>
<a class="sourceLine" id="cb11-36" title="36"></a>
<a class="sourceLine" id="cb11-37" title="37">        <span class="co"># pull a sample in a dict</span></a>
<a class="sourceLine" id="cb11-38" title="38">        sample <span class="op">=</span> {<span class="st">&#39;features&#39;</span>: torch.tensor(<span class="va">self</span>.features.iloc[idx].values),</a>
<a class="sourceLine" id="cb11-39" title="39">                  <span class="st">&#39;target&#39;</span>: torch.tensor(<span class="va">self</span>.target.iloc[idx]),</a>
<a class="sourceLine" id="cb11-40" title="40">                  <span class="st">&#39;idx&#39;</span>: torch.tensor(idx)}</a>
<a class="sourceLine" id="cb11-41" title="41"></a>
<a class="sourceLine" id="cb11-42" title="42">        <span class="cf">if</span> <span class="va">self</span>.transform:</a>
<a class="sourceLine" id="cb11-43" title="43">            sample <span class="op">=</span> <span class="va">self</span>.transform(sample)</a>
<a class="sourceLine" id="cb11-44" title="44"></a>
<a class="sourceLine" id="cb11-45" title="45">        <span class="cf">return</span> sample</a>
<a class="sourceLine" id="cb11-46" title="46"></a>
<a class="sourceLine" id="cb11-47" title="47"></a>
<a class="sourceLine" id="cb11-48" title="48"><span class="kw">class</span> Pixel_Normalize():</a>
<a class="sourceLine" id="cb11-49" title="49"></a>
<a class="sourceLine" id="cb11-50" title="50">    <span class="co"># retrieve sample and unpack it</span></a>
<a class="sourceLine" id="cb11-51" title="51">    <span class="kw">def</span> <span class="fu">__call__</span>(<span class="va">self</span>, sample):</a>
<a class="sourceLine" id="cb11-52" title="52">        features, target, idx <span class="op">=</span> (sample[<span class="st">&#39;features&#39;</span>],</a>
<a class="sourceLine" id="cb11-53" title="53">                              sample[<span class="st">&#39;target&#39;</span>],</a>
<a class="sourceLine" id="cb11-54" title="54">                              sample[<span class="st">&#39;idx&#39;</span>])</a>
<a class="sourceLine" id="cb11-55" title="55"></a>
<a class="sourceLine" id="cb11-56" title="56">        <span class="co"># normalize each pixel</span></a>
<a class="sourceLine" id="cb11-57" title="57">        normalized_pixels <span class="op">=</span> torch.true_divide(sample[<span class="st">&#39;features&#39;</span>], <span class="dv">255</span>)</a>
<a class="sourceLine" id="cb11-58" title="58"></a>
<a class="sourceLine" id="cb11-59" title="59">        <span class="co"># yield another dict</span></a>
<a class="sourceLine" id="cb11-60" title="60">        <span class="cf">return</span> {<span class="st">&#39;features&#39;</span>: normalized_pixels,</a>
<a class="sourceLine" id="cb11-61" title="61">                <span class="st">&#39;target&#39;</span>: target,</a>
<a class="sourceLine" id="cb11-62" title="62">                <span class="st">&#39;idx&#39;</span>: idx}</a>
<a class="sourceLine" id="cb11-63" title="63"></a>
<a class="sourceLine" id="cb11-64" title="64"></a>
<a class="sourceLine" id="cb11-65" title="65"><span class="co"># instantiate the lazy data set</span></a>
<a class="sourceLine" id="cb11-66" title="66">csv_dataset <span class="op">=</span> CSVDataset(csv_file<span class="op">=</span><span class="st">&#39;https://datahub.io/machine-learning/mnist_784/r/mnist_784.csv&#39;</span>,</a>
<a class="sourceLine" id="cb11-67" title="67">                         transform<span class="op">=</span>Pixel_Normalize())</a>
<a class="sourceLine" id="cb11-68" title="68"></a>
<a class="sourceLine" id="cb11-69" title="69"><span class="co"># set train, valid, and test size</span></a>
<a class="sourceLine" id="cb11-70" title="70">train_size <span class="op">=</span> <span class="bu">int</span>(<span class="fl">0.8</span> <span class="op">*</span> <span class="bu">len</span>(csv_dataset))</a>
<a class="sourceLine" id="cb11-71" title="71">valid_size <span class="op">=</span> <span class="bu">int</span>(<span class="fl">0.1</span> <span class="op">*</span> <span class="bu">len</span>(csv_dataset))</a>
<a class="sourceLine" id="cb11-72" title="72"></a>
<a class="sourceLine" id="cb11-73" title="73"><span class="co"># use random split to create three data sets;</span></a>
<a class="sourceLine" id="cb11-74" title="74">train_ds, valid_ds, test_ds <span class="op">=</span> torch.utils.data.random_split(csv_dataset, [train_size, valid_size, valid_size])</a>
<a class="sourceLine" id="cb11-75" title="75"></a>
<a class="sourceLine" id="cb11-76" title="76"><span class="co"># check the output</span></a>
<a class="sourceLine" id="cb11-77" title="77"><span class="cf">for</span> i, batch <span class="kw">in</span> <span class="bu">enumerate</span>(train_ds):</a>
<a class="sourceLine" id="cb11-78" title="78">    <span class="cf">if</span> i <span class="op">==</span> <span class="dv">0</span>:</a>
<a class="sourceLine" id="cb11-79" title="79">        <span class="cf">break</span></a>
<a class="sourceLine" id="cb11-80" title="80"></a>
<a class="sourceLine" id="cb11-81" title="81"></a>
<a class="sourceLine" id="cb11-82" title="82"><span class="co"># check the distribution of dependent variable; some imbalance</span></a>
<a class="sourceLine" id="cb11-83" title="83">csv_dataset.__get_target__().value_counts()</a>
<a class="sourceLine" id="cb11-84" title="84"></a>
<a class="sourceLine" id="cb11-85" title="85"></a>
<a class="sourceLine" id="cb11-86" title="86"><span class="co"># prepare weighted sampling for imbalanced classification</span></a></code></pre></div>
<pre><code>## 1    7877
## 7    7293
## 3    7141
## 2    6990
## 9    6958
## 0    6903
## 6    6876
## 8    6825
## 4    6824
## 5    6313
## Name: class, dtype: int64</code></pre>
<div class="sourceCode" id="cb13"><pre class="sourceCode python"><code class="sourceCode python"><a class="sourceLine" id="cb13-1" title="1"><span class="kw">def</span> create_sampler(train_ds, csv_dataset):</a>
<a class="sourceLine" id="cb13-2" title="2">    <span class="co"># get indicies from train split</span></a>
<a class="sourceLine" id="cb13-3" title="3">    train_indices <span class="op">=</span> train_ds.indices</a>
<a class="sourceLine" id="cb13-4" title="4">    <span class="co"># generate class distributions [y1, y2, etc...]</span></a>
<a class="sourceLine" id="cb13-5" title="5">    bin_count <span class="op">=</span> np.bincount(csv_dataset.__get_target__()[train_indices])</a>
<a class="sourceLine" id="cb13-6" title="6">    <span class="co"># weight gen</span></a>
<a class="sourceLine" id="cb13-7" title="7">    weight <span class="op">=</span> <span class="fl">1.</span> <span class="op">/</span> bin_count.astype(np.float32)</a>
<a class="sourceLine" id="cb13-8" title="8">    <span class="co"># produce weights for each observation in the data set</span></a>
<a class="sourceLine" id="cb13-9" title="9">    samples_weight <span class="op">=</span> torch.tensor([weight[t] <span class="cf">for</span> t <span class="kw">in</span> csv_dataset.__get_target__()[train_indices]])</a>
<a class="sourceLine" id="cb13-10" title="10">    <span class="co"># prepare sampler</span></a>
<a class="sourceLine" id="cb13-11" title="11">    sampler <span class="op">=</span> torch.utils.data.WeightedRandomSampler(weights<span class="op">=</span>samples_weight,</a>
<a class="sourceLine" id="cb13-12" title="12">                                                     num_samples<span class="op">=</span><span class="bu">len</span>(samples_weight),</a>
<a class="sourceLine" id="cb13-13" title="13">                                                     replacement<span class="op">=</span><span class="va">True</span>)</a>
<a class="sourceLine" id="cb13-14" title="14">    <span class="cf">return</span> sampler</a>
<a class="sourceLine" id="cb13-15" title="15"></a>
<a class="sourceLine" id="cb13-16" title="16"></a>
<a class="sourceLine" id="cb13-17" title="17"><span class="co"># create sampler for the training ds</span></a>
<a class="sourceLine" id="cb13-18" title="18">train_sampler <span class="op">=</span> create_sampler(train_ds, csv_dataset)</a>
<a class="sourceLine" id="cb13-19" title="19"></a>
<a class="sourceLine" id="cb13-20" title="20"><span class="co"># create NN</span></a>
<a class="sourceLine" id="cb13-21" title="21"><span class="co"># nn.Module tells PyTorch to do backward propagation</span></a>
<a class="sourceLine" id="cb13-22" title="22"><span class="kw">class</span> FF_NN(torch.nn.Module):</a>
<a class="sourceLine" id="cb13-23" title="23">    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, num_features, num_classes):</a>
<a class="sourceLine" id="cb13-24" title="24">        <span class="bu">super</span>(FF_NN, <span class="va">self</span>).<span class="fu">__init__</span>()</a>
<a class="sourceLine" id="cb13-25" title="25">        <span class="co"># initialize 3 layers</span></a>
<a class="sourceLine" id="cb13-26" title="26">        <span class="co"># first hidden layer</span></a>
<a class="sourceLine" id="cb13-27" title="27">        <span class="va">self</span>.linear_1 <span class="op">=</span> torch.nn.Linear(num_features, num_hidden_1)</a>
<a class="sourceLine" id="cb13-28" title="28">        <span class="va">self</span>.linear_1_bn <span class="op">=</span> torch.nn.BatchNorm1d(num_hidden_1)</a>
<a class="sourceLine" id="cb13-29" title="29">        <span class="co"># second hidden layer</span></a>
<a class="sourceLine" id="cb13-30" title="30">        <span class="va">self</span>.linear_2 <span class="op">=</span> torch.nn.Linear(num_hidden_1, num_hidden_2)</a>
<a class="sourceLine" id="cb13-31" title="31">        <span class="va">self</span>.linear_2_bn <span class="op">=</span> torch.nn.BatchNorm1d(num_hidden_2)</a>
<a class="sourceLine" id="cb13-32" title="32">        <span class="co"># output layer</span></a>
<a class="sourceLine" id="cb13-33" title="33">        <span class="va">self</span>.linear_out <span class="op">=</span> torch.nn.Linear(num_hidden_2, num_classes)</a>
<a class="sourceLine" id="cb13-34" title="34"></a>
<a class="sourceLine" id="cb13-35" title="35">    <span class="co"># define how and what order model parameters should be used in forward prop.</span></a>
<a class="sourceLine" id="cb13-36" title="36">    <span class="kw">def</span> forward(<span class="va">self</span>, x):</a>
<a class="sourceLine" id="cb13-37" title="37">        <span class="co"># run inputs through first layer</span></a>
<a class="sourceLine" id="cb13-38" title="38">        out <span class="op">=</span> <span class="va">self</span>.linear_1(x)</a>
<a class="sourceLine" id="cb13-39" title="39">        <span class="co"># apply relu</span></a>
<a class="sourceLine" id="cb13-40" title="40">        out <span class="op">=</span> F.relu(out)</a>
<a class="sourceLine" id="cb13-41" title="41">        <span class="co"># apply batchnorm</span></a>
<a class="sourceLine" id="cb13-42" title="42">        out <span class="op">=</span> <span class="va">self</span>.linear_1_bn(out)</a>
<a class="sourceLine" id="cb13-43" title="43">        <span class="co"># run inputs through second layer</span></a>
<a class="sourceLine" id="cb13-44" title="44">        out <span class="op">=</span> <span class="va">self</span>.linear_2(out)</a>
<a class="sourceLine" id="cb13-45" title="45">        <span class="co"># apply relu</span></a>
<a class="sourceLine" id="cb13-46" title="46">        out <span class="op">=</span> F.relu(out)</a>
<a class="sourceLine" id="cb13-47" title="47">        <span class="co"># apply batchnorm</span></a>
<a class="sourceLine" id="cb13-48" title="48">        out <span class="op">=</span> <span class="va">self</span>.linear_2_bn(out)</a>
<a class="sourceLine" id="cb13-49" title="49">        <span class="co"># run inputs through final classification layer</span></a>
<a class="sourceLine" id="cb13-50" title="50">        logits <span class="op">=</span> <span class="va">self</span>.linear_out(out)</a>
<a class="sourceLine" id="cb13-51" title="51">        probs <span class="op">=</span> F.log_softmax(logits, dim<span class="op">=</span><span class="dv">1</span>)</a>
<a class="sourceLine" id="cb13-52" title="52">        <span class="cf">return</span> logits, probs</a>
<a class="sourceLine" id="cb13-53" title="53"></a>
<a class="sourceLine" id="cb13-54" title="54"></a>
<a class="sourceLine" id="cb13-55" title="55"><span class="co"># load the NN model</span></a>
<a class="sourceLine" id="cb13-56" title="56">num_features <span class="op">=</span> <span class="dv">784</span></a>
<a class="sourceLine" id="cb13-57" title="57">num_hidden_1 <span class="op">=</span> <span class="dv">128</span></a>
<a class="sourceLine" id="cb13-58" title="58">num_hidden_2 <span class="op">=</span> <span class="dv">256</span></a>
<a class="sourceLine" id="cb13-59" title="59">num_classes <span class="op">=</span> <span class="dv">10</span></a>
<a class="sourceLine" id="cb13-60" title="60">model <span class="op">=</span> FF_NN(num_features<span class="op">=</span>num_features, num_classes<span class="op">=</span>num_classes).to(DEVICE)</a>
<a class="sourceLine" id="cb13-61" title="61"></a>
<a class="sourceLine" id="cb13-62" title="62"></a>
<a class="sourceLine" id="cb13-63" title="63"><span class="co"># optimizer</span></a>
<a class="sourceLine" id="cb13-64" title="64">optimizer <span class="op">=</span> RAdam(model.parameters(), lr<span class="op">=</span><span class="fl">0.1</span>)</a>
<a class="sourceLine" id="cb13-65" title="65"></a>
<a class="sourceLine" id="cb13-66" title="66"><span class="co"># set number of epochs</span></a>
<a class="sourceLine" id="cb13-67" title="67">epochs <span class="op">=</span> <span class="dv">4</span></a>
<a class="sourceLine" id="cb13-68" title="68"></a>
<a class="sourceLine" id="cb13-69" title="69"></a>
<a class="sourceLine" id="cb13-70" title="70"><span class="co"># create DataLoaders with samplers</span></a>
<a class="sourceLine" id="cb13-71" title="71">train_dataloader <span class="op">=</span> DataLoader(train_ds,</a>
<a class="sourceLine" id="cb13-72" title="72">                              batch_size<span class="op">=</span><span class="dv">100</span>,</a>
<a class="sourceLine" id="cb13-73" title="73">                              sampler<span class="op">=</span>train_sampler,</a>
<a class="sourceLine" id="cb13-74" title="74">                              shuffle<span class="op">=</span><span class="va">False</span>)</a>
<a class="sourceLine" id="cb13-75" title="75"></a>
<a class="sourceLine" id="cb13-76" title="76">valid_dataloader <span class="op">=</span> DataLoader(valid_ds,</a>
<a class="sourceLine" id="cb13-77" title="77">                              batch_size<span class="op">=</span><span class="dv">100</span>,</a>
<a class="sourceLine" id="cb13-78" title="78">                              shuffle<span class="op">=</span><span class="va">True</span>)</a>
<a class="sourceLine" id="cb13-79" title="79"></a>
<a class="sourceLine" id="cb13-80" title="80">test_dataloader <span class="op">=</span> DataLoader(test_ds,</a>
<a class="sourceLine" id="cb13-81" title="81">                              batch_size<span class="op">=</span><span class="dv">100</span>,</a>
<a class="sourceLine" id="cb13-82" title="82">                              shuffle<span class="op">=</span><span class="va">True</span>)</a>
<a class="sourceLine" id="cb13-83" title="83"></a>
<a class="sourceLine" id="cb13-84" title="84"><span class="co"># set LR scheduler</span></a>
<a class="sourceLine" id="cb13-85" title="85">scheduler <span class="op">=</span> torch.optim.lr_scheduler.OneCycleLR(optimizer,</a>
<a class="sourceLine" id="cb13-86" title="86">                                                max_lr<span class="op">=</span><span class="fl">0.01</span>,</a>
<a class="sourceLine" id="cb13-87" title="87">                                                total_steps<span class="op">=</span><span class="bu">len</span>(train_dataloader)<span class="op">*</span>epochs)</a>
<a class="sourceLine" id="cb13-88" title="88"></a>
<a class="sourceLine" id="cb13-89" title="89"><span class="co"># create gradient scaler for mixed precision</span></a>
<a class="sourceLine" id="cb13-90" title="90">scaler <span class="op">=</span> GradScaler()</a>
<a class="sourceLine" id="cb13-91" title="91"></a>
<a class="sourceLine" id="cb13-92" title="92"><span class="co"># train function</span></a>
<a class="sourceLine" id="cb13-93" title="93"><span class="kw">def</span> train(dataloader):</a>
<a class="sourceLine" id="cb13-94" title="94">    <span class="co">#pbar = ProgressBar(n_total=len(dataloader), desc=&#39;Training&#39;)</span></a>
<a class="sourceLine" id="cb13-95" title="95">    train_loss <span class="op">=</span> AverageMeter()</a>
<a class="sourceLine" id="cb13-96" title="96">    model.train()</a>
<a class="sourceLine" id="cb13-97" title="97">    <span class="cf">for</span> batch_idx, batch <span class="kw">in</span> <span class="bu">enumerate</span>(dataloader):</a>
<a class="sourceLine" id="cb13-98" title="98">        b_features, b_target, b_idx <span class="op">=</span> batch[<span class="st">&#39;features&#39;</span>].to(DEVICE),  batch[<span class="st">&#39;target&#39;</span>].to(DEVICE), batch[<span class="st">&#39;idx&#39;</span>].to(DEVICE)</a>
<a class="sourceLine" id="cb13-99" title="99">        optimizer.zero_grad()</a>
<a class="sourceLine" id="cb13-100" title="100">        <span class="cf">with</span> autocast():</a>
<a class="sourceLine" id="cb13-101" title="101">            logits, probs <span class="op">=</span> model(b_features)</a>
<a class="sourceLine" id="cb13-102" title="102">            loss <span class="op">=</span> F.cross_entropy(logits, b_target)</a>
<a class="sourceLine" id="cb13-103" title="103">            <span class="co"># regularize loss -- but not the intercept</span></a>
<a class="sourceLine" id="cb13-104" title="104">            LAMBDA, L2 <span class="op">=</span> <span class="fl">0.5</span>, <span class="fl">0.</span></a>
<a class="sourceLine" id="cb13-105" title="105">            <span class="cf">for</span> name, p <span class="kw">in</span> model.named_parameters():</a>
<a class="sourceLine" id="cb13-106" title="106">                <span class="cf">if</span> <span class="st">&#39;weight&#39;</span> <span class="kw">in</span> name:</a>
<a class="sourceLine" id="cb13-107" title="107">                    L2 <span class="op">=</span> L2 <span class="op">+</span> (p<span class="op">**</span><span class="dv">2</span>).<span class="bu">sum</span>()</a>
<a class="sourceLine" id="cb13-108" title="108">            loss <span class="op">=</span> loss <span class="op">+</span> <span class="fl">2.</span><span class="op">/</span>b_target.size(<span class="dv">0</span>) <span class="op">*</span> LAMBDA <span class="op">*</span> L2</a>
<a class="sourceLine" id="cb13-109" title="109">        scaler.scale(loss).backward()</a>
<a class="sourceLine" id="cb13-110" title="110">        scaler.step(optimizer)</a>
<a class="sourceLine" id="cb13-111" title="111">        scaler.update()</a>
<a class="sourceLine" id="cb13-112" title="112">        scheduler.step()</a>
<a class="sourceLine" id="cb13-113" title="113">        <span class="co">#pbar(step=batch_idx, info={&#39;loss&#39;: loss.item()})</span></a>
<a class="sourceLine" id="cb13-114" title="114">        train_loss.update(loss.item(), n<span class="op">=</span><span class="dv">1</span>)</a>
<a class="sourceLine" id="cb13-115" title="115">    <span class="cf">return</span> {<span class="st">&#39;loss&#39;</span>: train_loss.avg}</a>
<a class="sourceLine" id="cb13-116" title="116"></a>
<a class="sourceLine" id="cb13-117" title="117"></a>
<a class="sourceLine" id="cb13-118" title="118"><span class="co"># valid/test function</span></a>
<a class="sourceLine" id="cb13-119" title="119"><span class="kw">def</span> test(dataloader):</a>
<a class="sourceLine" id="cb13-120" title="120">    <span class="co">#pbar = ProgressBar(n_total=len(dataloader), desc=&#39;Testing&#39;)</span></a>
<a class="sourceLine" id="cb13-121" title="121">    valid_loss <span class="op">=</span> AverageMeter()</a>
<a class="sourceLine" id="cb13-122" title="122">    valid_acc <span class="op">=</span> AverageMeter()</a>
<a class="sourceLine" id="cb13-123" title="123">    valid_f1 <span class="op">=</span> AverageMeter()</a>
<a class="sourceLine" id="cb13-124" title="124">    model.<span class="bu">eval</span>()</a>
<a class="sourceLine" id="cb13-125" title="125">    count <span class="op">=</span> <span class="dv">0</span></a>
<a class="sourceLine" id="cb13-126" title="126">    <span class="cf">with</span> torch.no_grad():</a>
<a class="sourceLine" id="cb13-127" title="127">        <span class="cf">for</span> batch_idx, batch <span class="kw">in</span> <span class="bu">enumerate</span>(dataloader):</a>
<a class="sourceLine" id="cb13-128" title="128">            b_features, b_target, b_idx <span class="op">=</span> batch[<span class="st">&#39;features&#39;</span>].to(DEVICE),  batch[<span class="st">&#39;target&#39;</span>].to(DEVICE), batch[<span class="st">&#39;idx&#39;</span>].to(DEVICE)</a>
<a class="sourceLine" id="cb13-129" title="129">            logits, probs <span class="op">=</span> model(b_features)</a>
<a class="sourceLine" id="cb13-130" title="130">            loss <span class="op">=</span> F.cross_entropy(logits, b_target).item()</a>
<a class="sourceLine" id="cb13-131" title="131">            pred <span class="op">=</span> probs.argmax(dim<span class="op">=</span><span class="dv">1</span>, keepdim<span class="op">=</span><span class="va">True</span>)  <span class="co"># get the index of the max log-probability</span></a>
<a class="sourceLine" id="cb13-132" title="132">            correct <span class="op">=</span> pred.eq(b_target.view_as(pred)).<span class="bu">sum</span>().item()</a>
<a class="sourceLine" id="cb13-133" title="133">            f1 <span class="op">=</span> f1_score(pred.to(<span class="st">&quot;cpu&quot;</span>).numpy(), b_target.to(<span class="st">&quot;cpu&quot;</span>).numpy(), average<span class="op">=</span><span class="st">&#39;macro&#39;</span>)</a>
<a class="sourceLine" id="cb13-134" title="134">            valid_f1.update(f1, n<span class="op">=</span>b_features.size(<span class="dv">0</span>))</a>
<a class="sourceLine" id="cb13-135" title="135">            valid_loss.update(loss, n<span class="op">=</span>b_features.size(<span class="dv">0</span>))</a>
<a class="sourceLine" id="cb13-136" title="136">            valid_acc.update(correct, n<span class="op">=</span><span class="dv">1</span>)</a>
<a class="sourceLine" id="cb13-137" title="137">            count <span class="op">+=</span> b_features.size(<span class="dv">0</span>)</a>
<a class="sourceLine" id="cb13-138" title="138">            <span class="co">#pbar(step=batch_idx)</span></a>
<a class="sourceLine" id="cb13-139" title="139">    <span class="cf">return</span> {<span class="st">&#39;valid_loss&#39;</span>: valid_loss.avg,</a>
<a class="sourceLine" id="cb13-140" title="140">            <span class="st">&#39;valid_acc&#39;</span>: valid_acc.<span class="bu">sum</span> <span class="op">/</span>count,</a>
<a class="sourceLine" id="cb13-141" title="141">            <span class="st">&#39;valid_f1&#39;</span>: valid_f1.avg}</a>
<a class="sourceLine" id="cb13-142" title="142"></a>
<a class="sourceLine" id="cb13-143" title="143"><span class="co"># training</span></a>
<a class="sourceLine" id="cb13-144" title="144"><span class="cf">for</span> epoch <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">1</span>, epochs <span class="op">+</span> <span class="dv">1</span>):</a>
<a class="sourceLine" id="cb13-145" title="145">    train_log <span class="op">=</span> train(train_dataloader)</a>
<a class="sourceLine" id="cb13-146" title="146">    valid_log <span class="op">=</span> test(valid_dataloader)</a>
<a class="sourceLine" id="cb13-147" title="147">    logs <span class="op">=</span> <span class="bu">dict</span>(train_log, <span class="op">**</span>valid_log)</a>
<a class="sourceLine" id="cb13-148" title="148">    show_info <span class="op">=</span> <span class="ss">f&#39;</span><span class="ch">\n</span><span class="ss">Epoch: </span><span class="sc">{</span>epoch<span class="sc">}</span><span class="ss"> - &#39;</span> <span class="op">+</span> <span class="st">&quot;-&quot;</span>.join([<span class="ss">f&#39; </span><span class="sc">{</span>key<span class="sc">}</span><span class="ss">: </span><span class="sc">{</span>value<span class="sc">:.4f}</span><span class="ss"> &#39;</span> <span class="cf">for</span> key, value <span class="kw">in</span> logs.items()])</a>
<a class="sourceLine" id="cb13-149" title="149">    <span class="bu">print</span>(show_info)</a></code></pre></div>
<pre><code>## 
## Epoch: 1 -  loss: 3.7701 - valid_loss: 0.4558 - valid_acc: 0.8626 - valid_f1: 0.8627 
## 
## Epoch: 2 -  loss: 0.9400 - valid_loss: 0.4367 - valid_acc: 0.8791 - valid_f1: 0.8729 
## 
## Epoch: 3 -  loss: 0.8238 - valid_loss: 0.2809 - valid_acc: 0.9397 - valid_f1: 0.9364 
## 
## Epoch: 4 -  loss: 0.6290 - valid_loss: 0.1779 - valid_acc: 0.9646 - valid_f1: 0.9622 
## 
## C:/Users/Andrew/Desktop/Projects/Deep Learning/utils\radam.py:60: UserWarning: This overload of add_ is deprecated:
##  add_(Number alpha, Tensor other)
## Consider using one of the following signatures instead:
##  add_(Tensor other, *, Number alpha) (Triggered internally at  ..\torch\csrc\utils\python_arg_parser.cpp:766.)
##   exp_avg.mul_(beta1).add_(1 - beta1, grad)</code></pre>
<div class="sourceCode" id="cb15"><pre class="sourceCode python"><code class="sourceCode python"><a class="sourceLine" id="cb15-1" title="1"><span class="co"># testing</span></a>
<a class="sourceLine" id="cb15-2" title="2">test_log <span class="op">=</span> test(test_dataloader)</a>
<a class="sourceLine" id="cb15-3" title="3"><span class="bu">print</span>(test_log)</a></code></pre></div>
<pre><code>## {&#39;valid_loss&#39;: 0.18130239312137877, &#39;valid_acc&#39;: 0.9631428571428572, &#39;valid_f1&#39;: 0.9605992527642337}</code></pre>
</div>
<div id="xavier-initialization" class="section level3">
<h3><span class="header-section-number">1.3.2</span> Xavier Initialization</h3>
<div class="sourceCode" id="cb17"><pre class="sourceCode python"><code class="sourceCode python"><a class="sourceLine" id="cb17-1" title="1"><span class="kw">class</span> FF_NN(torch.nn.Module):</a>
<a class="sourceLine" id="cb17-2" title="2">    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, num_features, num_classes):</a>
<a class="sourceLine" id="cb17-3" title="3">        <span class="bu">super</span>(FF_NN, <span class="va">self</span>).<span class="fu">__init__</span>()</a>
<a class="sourceLine" id="cb17-4" title="4">        <span class="co"># initialize 3 layers</span></a>
<a class="sourceLine" id="cb17-5" title="5">        <span class="co"># first hidden layer</span></a>
<a class="sourceLine" id="cb17-6" title="6">        <span class="va">self</span>.linear_1 <span class="op">=</span> torch.nn.Linear(num_features, num_hidden_1)</a>
<a class="sourceLine" id="cb17-7" title="7">        <span class="va">self</span>.linear_1.weight.detach().normal_(<span class="fl">0.0</span>, <span class="fl">0.1</span>)</a>
<a class="sourceLine" id="cb17-8" title="8">        <span class="va">self</span>.linear_1.bias.detach().zero_()</a>
<a class="sourceLine" id="cb17-9" title="9">        <span class="co"># second hidden layer</span></a>
<a class="sourceLine" id="cb17-10" title="10">        <span class="va">self</span>.linear_2 <span class="op">=</span> torch.nn.Linear(num_hidden_1, num_hidden_2)</a>
<a class="sourceLine" id="cb17-11" title="11">        <span class="va">self</span>.linear_2.weight.detach().normal_(<span class="fl">0.0</span>, <span class="fl">0.1</span>)</a>
<a class="sourceLine" id="cb17-12" title="12">        <span class="va">self</span>.linear_2.bias.detach().zero_()</a>
<a class="sourceLine" id="cb17-13" title="13">        <span class="co"># output layer</span></a>
<a class="sourceLine" id="cb17-14" title="14">        <span class="va">self</span>.linear_out <span class="op">=</span> torch.nn.Linear(num_hidden_2, num_classes)</a>
<a class="sourceLine" id="cb17-15" title="15">        <span class="va">self</span>.linear_out.weight.detach().normal_(<span class="fl">0.0</span>, <span class="fl">0.1</span>)</a>
<a class="sourceLine" id="cb17-16" title="16">        <span class="va">self</span>.linear_out.bias.detach().zero_()</a>
<a class="sourceLine" id="cb17-17" title="17"></a>
<a class="sourceLine" id="cb17-18" title="18">    <span class="co"># define how and what order model parameters should be used in forward prop.</span></a>
<a class="sourceLine" id="cb17-19" title="19">    <span class="kw">def</span> forward(<span class="va">self</span>, x):</a>
<a class="sourceLine" id="cb17-20" title="20">        <span class="co"># run inputs through first layer</span></a>
<a class="sourceLine" id="cb17-21" title="21">        out <span class="op">=</span> <span class="va">self</span>.linear_1(x)</a>
<a class="sourceLine" id="cb17-22" title="22">        <span class="co"># apply relu</span></a>
<a class="sourceLine" id="cb17-23" title="23">        out <span class="op">=</span> F.relu(out)</a>
<a class="sourceLine" id="cb17-24" title="24">        <span class="co"># run inputs through second layer</span></a>
<a class="sourceLine" id="cb17-25" title="25">        out <span class="op">=</span> <span class="va">self</span>.linear_2(out)</a>
<a class="sourceLine" id="cb17-26" title="26">        <span class="co"># apply relu</span></a>
<a class="sourceLine" id="cb17-27" title="27">        out <span class="op">=</span> F.relu(out)</a>
<a class="sourceLine" id="cb17-28" title="28">        <span class="co"># run inputs through final classification layer</span></a>
<a class="sourceLine" id="cb17-29" title="29">        logits <span class="op">=</span> <span class="va">self</span>.linear_out(out)</a>
<a class="sourceLine" id="cb17-30" title="30">        probs <span class="op">=</span> F.log_softmax(logits, dim<span class="op">=</span><span class="dv">1</span>)</a>
<a class="sourceLine" id="cb17-31" title="31">        <span class="cf">return</span> logits, probs</a></code></pre></div>
<div class="sourceCode" id="cb18"><pre class="sourceCode python"><code class="sourceCode python"><a class="sourceLine" id="cb18-1" title="1"><span class="co"># load the NN model</span></a>
<a class="sourceLine" id="cb18-2" title="2">num_features <span class="op">=</span> <span class="dv">784</span></a>
<a class="sourceLine" id="cb18-3" title="3">num_hidden_1 <span class="op">=</span> <span class="dv">128</span></a>
<a class="sourceLine" id="cb18-4" title="4">num_hidden_2 <span class="op">=</span> <span class="dv">256</span></a>
<a class="sourceLine" id="cb18-5" title="5">num_classes <span class="op">=</span> <span class="dv">10</span></a>
<a class="sourceLine" id="cb18-6" title="6">model <span class="op">=</span> FF_NN(num_features<span class="op">=</span>num_features, num_classes<span class="op">=</span>num_classes).to(DEVICE)</a>
<a class="sourceLine" id="cb18-7" title="7"></a>
<a class="sourceLine" id="cb18-8" title="8"></a>
<a class="sourceLine" id="cb18-9" title="9"><span class="co"># optimizer</span></a>
<a class="sourceLine" id="cb18-10" title="10">optimizer <span class="op">=</span> RAdam(model.parameters(), lr<span class="op">=</span><span class="fl">0.1</span>)</a>
<a class="sourceLine" id="cb18-11" title="11"></a>
<a class="sourceLine" id="cb18-12" title="12"><span class="co"># set number of epochs</span></a>
<a class="sourceLine" id="cb18-13" title="13">epochs <span class="op">=</span> <span class="dv">4</span></a>
<a class="sourceLine" id="cb18-14" title="14"></a>
<a class="sourceLine" id="cb18-15" title="15"></a>
<a class="sourceLine" id="cb18-16" title="16"><span class="co"># create DataLoaders with samplers</span></a>
<a class="sourceLine" id="cb18-17" title="17">train_dataloader <span class="op">=</span> DataLoader(train_ds,</a>
<a class="sourceLine" id="cb18-18" title="18">                              batch_size<span class="op">=</span><span class="dv">100</span>,</a>
<a class="sourceLine" id="cb18-19" title="19">                              sampler<span class="op">=</span>train_sampler,</a>
<a class="sourceLine" id="cb18-20" title="20">                              shuffle<span class="op">=</span><span class="va">False</span>)</a>
<a class="sourceLine" id="cb18-21" title="21"></a>
<a class="sourceLine" id="cb18-22" title="22">valid_dataloader <span class="op">=</span> DataLoader(valid_ds,</a>
<a class="sourceLine" id="cb18-23" title="23">                              batch_size<span class="op">=</span><span class="dv">100</span>,</a>
<a class="sourceLine" id="cb18-24" title="24">                              shuffle<span class="op">=</span><span class="va">True</span>)</a>
<a class="sourceLine" id="cb18-25" title="25"></a>
<a class="sourceLine" id="cb18-26" title="26">test_dataloader <span class="op">=</span> DataLoader(test_ds,</a>
<a class="sourceLine" id="cb18-27" title="27">                              batch_size<span class="op">=</span><span class="dv">100</span>,</a>
<a class="sourceLine" id="cb18-28" title="28">                              shuffle<span class="op">=</span><span class="va">True</span>)</a>
<a class="sourceLine" id="cb18-29" title="29"></a>
<a class="sourceLine" id="cb18-30" title="30"><span class="co"># set LR scheduler</span></a>
<a class="sourceLine" id="cb18-31" title="31">scheduler <span class="op">=</span> torch.optim.lr_scheduler.OneCycleLR(optimizer,</a>
<a class="sourceLine" id="cb18-32" title="32">                                                max_lr<span class="op">=</span><span class="fl">0.01</span>,</a>
<a class="sourceLine" id="cb18-33" title="33">                                                total_steps<span class="op">=</span><span class="bu">len</span>(train_dataloader)<span class="op">*</span>epochs)</a>
<a class="sourceLine" id="cb18-34" title="34"></a>
<a class="sourceLine" id="cb18-35" title="35"><span class="co"># create gradient scaler for mixed precision</span></a>
<a class="sourceLine" id="cb18-36" title="36">scaler <span class="op">=</span> GradScaler()</a>
<a class="sourceLine" id="cb18-37" title="37"></a>
<a class="sourceLine" id="cb18-38" title="38"><span class="co"># train function</span></a>
<a class="sourceLine" id="cb18-39" title="39"><span class="kw">def</span> train(dataloader):</a>
<a class="sourceLine" id="cb18-40" title="40">    <span class="co">#pbar = ProgressBar(n_total=len(dataloader), desc=&#39;Training&#39;)</span></a>
<a class="sourceLine" id="cb18-41" title="41">    train_loss <span class="op">=</span> AverageMeter()</a>
<a class="sourceLine" id="cb18-42" title="42">    model.train()</a>
<a class="sourceLine" id="cb18-43" title="43">    <span class="cf">for</span> batch_idx, batch <span class="kw">in</span> <span class="bu">enumerate</span>(dataloader):</a>
<a class="sourceLine" id="cb18-44" title="44">        b_features, b_target, b_idx <span class="op">=</span> batch[<span class="st">&#39;features&#39;</span>].to(DEVICE),  batch[<span class="st">&#39;target&#39;</span>].to(DEVICE), batch[<span class="st">&#39;idx&#39;</span>].to(DEVICE)</a>
<a class="sourceLine" id="cb18-45" title="45">        optimizer.zero_grad()</a>
<a class="sourceLine" id="cb18-46" title="46">        <span class="cf">with</span> autocast():</a>
<a class="sourceLine" id="cb18-47" title="47">            logits, probs <span class="op">=</span> model(b_features)</a>
<a class="sourceLine" id="cb18-48" title="48">            loss <span class="op">=</span> F.cross_entropy(logits, b_target)</a>
<a class="sourceLine" id="cb18-49" title="49">        scaler.scale(loss).backward()</a>
<a class="sourceLine" id="cb18-50" title="50">        scaler.step(optimizer)</a>
<a class="sourceLine" id="cb18-51" title="51">        scaler.update()</a>
<a class="sourceLine" id="cb18-52" title="52">        scheduler.step()</a>
<a class="sourceLine" id="cb18-53" title="53">        <span class="co">#pbar(step=batch_idx, info={&#39;loss&#39;: loss.item()})</span></a>
<a class="sourceLine" id="cb18-54" title="54">        train_loss.update(loss.item(), n<span class="op">=</span><span class="dv">1</span>)</a>
<a class="sourceLine" id="cb18-55" title="55">    <span class="cf">return</span> {<span class="st">&#39;loss&#39;</span>: train_loss.avg}</a>
<a class="sourceLine" id="cb18-56" title="56"></a>
<a class="sourceLine" id="cb18-57" title="57"></a>
<a class="sourceLine" id="cb18-58" title="58"><span class="co"># valid/test function</span></a>
<a class="sourceLine" id="cb18-59" title="59"><span class="kw">def</span> test(dataloader):</a>
<a class="sourceLine" id="cb18-60" title="60">    <span class="co">#pbar = ProgressBar(n_total=len(dataloader), desc=&#39;Testing&#39;)</span></a>
<a class="sourceLine" id="cb18-61" title="61">    valid_loss <span class="op">=</span> AverageMeter()</a>
<a class="sourceLine" id="cb18-62" title="62">    valid_acc <span class="op">=</span> AverageMeter()</a>
<a class="sourceLine" id="cb18-63" title="63">    valid_f1 <span class="op">=</span> AverageMeter()</a>
<a class="sourceLine" id="cb18-64" title="64">    model.<span class="bu">eval</span>()</a>
<a class="sourceLine" id="cb18-65" title="65">    count <span class="op">=</span> <span class="dv">0</span></a>
<a class="sourceLine" id="cb18-66" title="66">    <span class="cf">with</span> torch.no_grad():</a>
<a class="sourceLine" id="cb18-67" title="67">        <span class="cf">for</span> batch_idx, batch <span class="kw">in</span> <span class="bu">enumerate</span>(dataloader):</a>
<a class="sourceLine" id="cb18-68" title="68">            b_features, b_target, b_idx <span class="op">=</span> batch[<span class="st">&#39;features&#39;</span>].to(DEVICE),  batch[<span class="st">&#39;target&#39;</span>].to(DEVICE), batch[<span class="st">&#39;idx&#39;</span>].to(DEVICE)</a>
<a class="sourceLine" id="cb18-69" title="69">            logits, probs <span class="op">=</span> model(b_features)</a>
<a class="sourceLine" id="cb18-70" title="70">            loss <span class="op">=</span> F.cross_entropy(logits, b_target).item()</a>
<a class="sourceLine" id="cb18-71" title="71">            pred <span class="op">=</span> probs.argmax(dim<span class="op">=</span><span class="dv">1</span>, keepdim<span class="op">=</span><span class="va">True</span>)  <span class="co"># get the index of the max log-probability</span></a>
<a class="sourceLine" id="cb18-72" title="72">            correct <span class="op">=</span> pred.eq(b_target.view_as(pred)).<span class="bu">sum</span>().item()</a>
<a class="sourceLine" id="cb18-73" title="73">            f1 <span class="op">=</span> f1_score(pred.to(<span class="st">&quot;cpu&quot;</span>).numpy(), b_target.to(<span class="st">&quot;cpu&quot;</span>).numpy(), average<span class="op">=</span><span class="st">&#39;macro&#39;</span>)</a>
<a class="sourceLine" id="cb18-74" title="74">            valid_f1.update(f1, n<span class="op">=</span>b_features.size(<span class="dv">0</span>))</a>
<a class="sourceLine" id="cb18-75" title="75">            valid_loss.update(loss, n<span class="op">=</span>b_features.size(<span class="dv">0</span>))</a>
<a class="sourceLine" id="cb18-76" title="76">            valid_acc.update(correct, n<span class="op">=</span><span class="dv">1</span>)</a>
<a class="sourceLine" id="cb18-77" title="77">            count <span class="op">+=</span> b_features.size(<span class="dv">0</span>)</a>
<a class="sourceLine" id="cb18-78" title="78">            <span class="co">#pbar(step=batch_idx)</span></a>
<a class="sourceLine" id="cb18-79" title="79">    <span class="cf">return</span> {<span class="st">&#39;valid_loss&#39;</span>: valid_loss.avg,</a>
<a class="sourceLine" id="cb18-80" title="80">            <span class="st">&#39;valid_acc&#39;</span>: valid_acc.<span class="bu">sum</span> <span class="op">/</span>count,</a>
<a class="sourceLine" id="cb18-81" title="81">            <span class="st">&#39;valid_f1&#39;</span>: valid_f1.avg}</a>
<a class="sourceLine" id="cb18-82" title="82"></a>
<a class="sourceLine" id="cb18-83" title="83"><span class="co"># training</span></a>
<a class="sourceLine" id="cb18-84" title="84"><span class="cf">for</span> epoch <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">1</span>, epochs <span class="op">+</span> <span class="dv">1</span>):</a>
<a class="sourceLine" id="cb18-85" title="85">    train_log <span class="op">=</span> train(train_dataloader)</a>
<a class="sourceLine" id="cb18-86" title="86">    valid_log <span class="op">=</span> test(valid_dataloader)</a>
<a class="sourceLine" id="cb18-87" title="87">    logs <span class="op">=</span> <span class="bu">dict</span>(train_log, <span class="op">**</span>valid_log)</a>
<a class="sourceLine" id="cb18-88" title="88">    show_info <span class="op">=</span> <span class="ss">f&#39;</span><span class="ch">\n</span><span class="ss">Epoch: </span><span class="sc">{</span>epoch<span class="sc">}</span><span class="ss"> - &#39;</span> <span class="op">+</span> <span class="st">&quot;-&quot;</span>.join([<span class="ss">f&#39; </span><span class="sc">{</span>key<span class="sc">}</span><span class="ss">: </span><span class="sc">{</span>value<span class="sc">:.4f}</span><span class="ss"> &#39;</span> <span class="cf">for</span> key, value <span class="kw">in</span> logs.items()])</a>
<a class="sourceLine" id="cb18-89" title="89">    <span class="bu">print</span>(show_info)</a></code></pre></div>
<pre><code>## 
## Epoch: 1 -  loss: 0.6772 - valid_loss: 0.1668 - valid_acc: 0.9484 - valid_f1: 0.9455 
## 
## Epoch: 2 -  loss: 0.1306 - valid_loss: 0.1363 - valid_acc: 0.9583 - valid_f1: 0.9551 
## 
## Epoch: 3 -  loss: 0.0715 - valid_loss: 0.0803 - valid_acc: 0.9750 - valid_f1: 0.9732 
## 
## Epoch: 4 -  loss: 0.0297 - valid_loss: 0.0627 - valid_acc: 0.9814 - valid_f1: 0.9798</code></pre>
<div class="sourceCode" id="cb20"><pre class="sourceCode python"><code class="sourceCode python"><a class="sourceLine" id="cb20-1" title="1"><span class="co"># testing</span></a>
<a class="sourceLine" id="cb20-2" title="2">test_log <span class="op">=</span> test(test_dataloader)</a>
<a class="sourceLine" id="cb20-3" title="3"><span class="bu">print</span>(test_log)</a></code></pre></div>
<pre><code>## {&#39;valid_loss&#39;: 0.06501132133749447, &#39;valid_acc&#39;: 0.9835714285714285, &#39;valid_f1&#39;: 0.9821693332461051}</code></pre>
</div>
</div>
</div>
<div id="sources" class="section level1">
<h1><span class="header-section-number">2</span> Sources</h1>
<ul>
<li><a href="https://github.com/rasbt/stat479-deep-learning-ss19/blob/master/L11_weight-init/L11_weight-init_slides.pdf" class="uri">https://github.com/rasbt/stat479-deep-learning-ss19/blob/master/L11_weight-init/L11_weight-init_slides.pdf</a></li>
</ul>
</div>



<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>
