<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml">

<head>

<meta charset="utf-8" />
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="generator" content="pandoc" />

<meta name="viewport" content="width=device-width, initial-scale=1">

<meta name="author" content="Andrew Fogarty" />

<meta name="date" content="2020-10-06" />

<title>Multilayer Perceptrons in PyTorch</title>



<style type="text/css">code{white-space: pre;}</style>
<style type="text/css" data-origin="pandoc">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */

</style>
<script>
// apply pandoc div.sourceCode style to pre.sourceCode instead
(function() {
  var sheets = document.styleSheets;
  for (var i = 0; i < sheets.length; i++) {
    if (sheets[i].ownerNode.dataset["origin"] !== "pandoc") continue;
    try { var rules = sheets[i].cssRules; } catch (e) { continue; }
    for (var j = 0; j < rules.length; j++) {
      var rule = rules[j];
      // check if there is a div.sourceCode rule
      if (rule.type !== rule.STYLE_RULE || rule.selectorText !== "div.sourceCode") continue;
      var style = rule.style.cssText;
      // check if color or background-color is set
      if (rule.style.color === '' || rule.style.backgroundColor === '') continue;
      // replace div.sourceCode by a pre.sourceCode rule
      sheets[i].deleteRule(j);
      sheets[i].insertRule('pre.sourceCode{' + style + '}', j);
    }
  }
})();
</script>



<style type="text/css">body {
background-color: #fff;
margin: 1em auto;
max-width: 700px;
overflow: visible;
padding-left: 2em;
padding-right: 2em;
font-family: "Open Sans", "Helvetica Neue", Helvetica, Arial, sans-serif;
font-size: 14px;
line-height: 1.35;
}
#header {
text-align: center;
}
#TOC {
clear: both;
margin: 0 0 10px 10px;
padding: 4px;
width: 400px;
border: 1px solid #CCCCCC;
border-radius: 5px;
background-color: #f6f6f6;
font-size: 13px;
line-height: 1.3;
}
#TOC .toctitle {
font-weight: bold;
font-size: 15px;
margin-left: 5px;
}
#TOC ul {
padding-left: 40px;
margin-left: -1.5em;
margin-top: 5px;
margin-bottom: 5px;
}
#TOC ul ul {
margin-left: -2em;
}
#TOC li {
line-height: 16px;
}
table {
margin: 1em auto;
border-width: 1px;
border-color: #DDDDDD;
border-style: outset;
border-collapse: collapse;
}
table th {
border-width: 2px;
padding: 5px;
border-style: inset;
}
table td {
border-width: 1px;
border-style: inset;
line-height: 18px;
padding: 5px 5px;
}
table, table th, table td {
border-left-style: none;
border-right-style: none;
}
table thead, table tr.even {
background-color: #f7f7f7;
}
p {
margin: 0.5em 0;
}
blockquote {
background-color: #f6f6f6;
padding: 0.25em 0.75em;
}
hr {
border-style: solid;
border: none;
border-top: 1px solid #777;
margin: 28px 0;
}
dl {
margin-left: 0;
}
dl dd {
margin-bottom: 13px;
margin-left: 13px;
}
dl dt {
font-weight: bold;
}
ul {
margin-top: 0;
}
ul li {
list-style: circle outside;
}
ul ul {
margin-bottom: 0;
}
pre, code {
background-color: #f7f7f7;
border-radius: 3px;
color: #333;
white-space: pre-wrap; 
}
pre {
border-radius: 3px;
margin: 5px 0px 10px 0px;
padding: 10px;
}
pre:not([class]) {
background-color: #f7f7f7;
}
code {
font-family: Consolas, Monaco, 'Courier New', monospace;
font-size: 85%;
}
p > code, li > code {
padding: 2px 0px;
}
div.figure {
text-align: center;
}
img {
background-color: #FFFFFF;
padding: 2px;
border: 1px solid #DDDDDD;
border-radius: 3px;
border: 1px solid #CCCCCC;
margin: 0 5px;
}
h1 {
margin-top: 0;
font-size: 35px;
line-height: 40px;
}
h2 {
border-bottom: 4px solid #f7f7f7;
padding-top: 10px;
padding-bottom: 2px;
font-size: 145%;
}
h3 {
border-bottom: 2px solid #f7f7f7;
padding-top: 10px;
font-size: 120%;
}
h4 {
border-bottom: 1px solid #f7f7f7;
margin-left: 8px;
font-size: 105%;
}
h5, h6 {
border-bottom: 1px solid #ccc;
font-size: 105%;
}
a {
color: #0033dd;
text-decoration: none;
}
a:hover {
color: #6666ff; }
a:visited {
color: #800080; }
a:visited:hover {
color: #BB00BB; }
a[href^="http:"] {
text-decoration: underline; }
a[href^="https:"] {
text-decoration: underline; }

code > span.kw { color: #555; font-weight: bold; } 
code > span.dt { color: #902000; } 
code > span.dv { color: #40a070; } 
code > span.bn { color: #d14; } 
code > span.fl { color: #d14; } 
code > span.ch { color: #d14; } 
code > span.st { color: #d14; } 
code > span.co { color: #888888; font-style: italic; } 
code > span.ot { color: #007020; } 
code > span.al { color: #ff0000; font-weight: bold; } 
code > span.fu { color: #900; font-weight: bold; }  code > span.er { color: #a61717; background-color: #e3d2d2; } 
</style>




</head>

<body>




<h1 class="title toc-ignore">Multilayer Perceptrons in PyTorch</h1>
<h4 class="author">Andrew Fogarty</h4>
<h4 class="date">10/06/2020</h4>


<div id="TOC">
<ul>
<li><a href="#introduction"><span class="toc-section-number">1</span> Introduction</a>
<ul>
<li><a href="#preparing-a-custom-data-set"><span class="toc-section-number">1.1</span> Preparing a Custom Data Set</a></li>
<li><a href="#preparing-a-feed-forward-neural-network"><span class="toc-section-number">1.2</span> Preparing a Feed Forward Neural Network</a></li>
<li><a href="#training-helper-functions"><span class="toc-section-number">1.3</span> Training Helper Functions</a></li>
<li><a href="#data-loaders-and-misc"><span class="toc-section-number">1.4</span> Data Loaders and Misc</a></li>
<li><a href="#optimizer-and-scheduler"><span class="toc-section-number">1.5</span> Optimizer and Scheduler</a></li>
</ul></li>
<li><a href="#sources"><span class="toc-section-number">2</span> Sources</a></li>
</ul>
</div>

<div class="sourceCode" id="cb1"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1-1"><a href="#cb1-1"></a><span class="co"># load python</span></span>
<span id="cb1-2"><a href="#cb1-2"></a><span class="kw">library</span>(reticulate)</span>
<span id="cb1-3"><a href="#cb1-3"></a><span class="kw">use_condaenv</span>(<span class="st">&quot;my_ml&quot;</span>)</span></code></pre></div>
<div class="sourceCode" id="cb2"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1"></a><span class="co"># load packages</span></span>
<span id="cb2-2"><a href="#cb2-2"></a><span class="im">import</span> sys</span>
<span id="cb2-3"><a href="#cb2-3"></a>sys.path.append(<span class="st">&quot;C:/Users/Andrew/Desktop/Projects/Deep Learning/utils&quot;</span>)  <span class="co"># this is the folder with py files</span></span>
<span id="cb2-4"><a href="#cb2-4"></a><span class="im">from</span> tools <span class="im">import</span> AverageMeter, ProgressBar <span class="co">#scriptName without .py extension; import each class</span></span>
<span id="cb2-5"><a href="#cb2-5"></a><span class="im">from</span> radam <span class="im">import</span> RAdam</span>
<span id="cb2-6"><a href="#cb2-6"></a><span class="im">import</span> torch</span>
<span id="cb2-7"><a href="#cb2-7"></a><span class="im">import</span> torch.nn <span class="im">as</span> nn</span>
<span id="cb2-8"><a href="#cb2-8"></a><span class="im">import</span> torch.nn.functional <span class="im">as</span> F</span>
<span id="cb2-9"><a href="#cb2-9"></a><span class="im">from</span> transformers <span class="im">import</span> get_linear_schedule_with_warmup, AdamW</span>
<span id="cb2-10"><a href="#cb2-10"></a><span class="im">from</span> torch.utils.data <span class="im">import</span> TensorDataset, random_split, DataLoader, RandomSampler, SequentialSampler</span>
<span id="cb2-11"><a href="#cb2-11"></a><span class="im">import</span> time, datetime, random, re, os</span>
<span id="cb2-12"><a href="#cb2-12"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb2-13"><a href="#cb2-13"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb2-14"><a href="#cb2-14"></a><span class="im">from</span> sklearn.metrics <span class="im">import</span> accuracy_score, f1_score, precision_score, recall_score, confusion_matrix</span>
<span id="cb2-15"><a href="#cb2-15"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb2-16"><a href="#cb2-16"></a><span class="im">import</span> seaborn <span class="im">as</span> sns</span>
<span id="cb2-17"><a href="#cb2-17"></a><span class="im">from</span> torch.cuda.amp <span class="im">import</span> autocast, GradScaler</span>
<span id="cb2-18"><a href="#cb2-18"></a><span class="im">from</span> sklearn.model_selection <span class="im">import</span> train_test_split</span>
<span id="cb2-19"><a href="#cb2-19"></a><span class="im">from</span> torch.utils.data <span class="im">import</span> Dataset, Subset</span>
<span id="cb2-20"><a href="#cb2-20"></a><span class="im">from</span> sklearn.preprocessing <span class="im">import</span> LabelEncoder</span>
<span id="cb2-21"><a href="#cb2-21"></a><span class="im">from</span> torchvision <span class="im">import</span> transforms</span>
<span id="cb2-22"><a href="#cb2-22"></a></span>
<span id="cb2-23"><a href="#cb2-23"></a><span class="co"># set seed and gpu requirements</span></span>
<span id="cb2-24"><a href="#cb2-24"></a>SEED <span class="op">=</span> <span class="dv">15</span></span>
<span id="cb2-25"><a href="#cb2-25"></a>random.seed(SEED)</span>
<span id="cb2-26"><a href="#cb2-26"></a>np.random.seed(SEED)</span>
<span id="cb2-27"><a href="#cb2-27"></a>torch.manual_seed(SEED)</span></code></pre></div>
<pre><code>## &lt;torch._C.Generator object at 0x000000001FA40370&gt;</code></pre>
<div class="sourceCode" id="cb4"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1"></a>torch.backends.cudnn.deterministic <span class="op">=</span> <span class="va">True</span></span>
<span id="cb4-2"><a href="#cb4-2"></a>torch.cuda.amp.autocast(enabled<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb4-3"><a href="#cb4-3"></a></span>
<span id="cb4-4"><a href="#cb4-4"></a><span class="co"># set gpu/cpu</span></span></code></pre></div>
<pre><code>## &lt;torch.cuda.amp.autocast_mode.autocast object at 0x00000000339BCB48&gt;</code></pre>
<div class="sourceCode" id="cb6"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1"></a>DEVICE <span class="op">=</span> torch.device(<span class="st">&#39;cuda:0&#39;</span> <span class="cf">if</span> torch.cuda.is_available() <span class="cf">else</span> <span class="st">&#39;cpu&#39;</span>)</span></code></pre></div>
<div id="introduction" class="section level1" number="1">
<h1 number="1"><span class="header-section-number">1</span> Introduction</h1>
<p>Artificial Neural Networks (ANN) are powerful <em>classification</em> and <em>regression</em> algorithms that can solve simple and complex linear and non-linear modeling problems. In this post, we demonstrate the functionality of a basic deep learning multi-layer perceptron model on PyTorch using the famous MNIST data set.</p>
<div id="preparing-a-custom-data-set" class="section level2" number="1.1">
<h2 number="1.1"><span class="header-section-number">1.1</span> Preparing a Custom Data Set</h2>
<p>In the chunk below, we begin by loading our data using <code>PyTorch</code>’s custom Dataset class – ideal for lazy evaluation, advanced modeling, and big data.</p>
<div class="sourceCode" id="cb7"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1"></a><span class="co"># create Dataset</span></span>
<span id="cb7-2"><a href="#cb7-2"></a><span class="kw">class</span> CSVDataset(Dataset):</span>
<span id="cb7-3"><a href="#cb7-3"></a>    <span class="co">&quot;&quot;&quot;MNIST dataset.&quot;&quot;&quot;</span></span>
<span id="cb7-4"><a href="#cb7-4"></a></span>
<span id="cb7-5"><a href="#cb7-5"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, csv_file, transform<span class="op">=</span><span class="va">None</span>):</span>
<span id="cb7-6"><a href="#cb7-6"></a>        <span class="co">&quot;&quot;&quot;</span></span>
<span id="cb7-7"><a href="#cb7-7"></a><span class="co">        Args:</span></span>
<span id="cb7-8"><a href="#cb7-8"></a><span class="co">            csv_file (string): Path to the csv file.</span></span>
<span id="cb7-9"><a href="#cb7-9"></a><span class="co">            transform (callable, optional): Optional transform to be applied</span></span>
<span id="cb7-10"><a href="#cb7-10"></a><span class="co">                on a sample.</span></span>
<span id="cb7-11"><a href="#cb7-11"></a><span class="co">        &quot;&quot;&quot;</span></span>
<span id="cb7-12"><a href="#cb7-12"></a>        <span class="co"># initialize</span></span>
<span id="cb7-13"><a href="#cb7-13"></a>        <span class="va">self</span>.data_frame <span class="op">=</span> pd.read_csv(csv_file)</span>
<span id="cb7-14"><a href="#cb7-14"></a>        <span class="co"># all columns but the last</span></span>
<span id="cb7-15"><a href="#cb7-15"></a>        <span class="va">self</span>.features <span class="op">=</span> <span class="va">self</span>.data_frame[<span class="va">self</span>.data_frame.columns[:<span class="op">-</span><span class="dv">1</span>]]</span>
<span id="cb7-16"><a href="#cb7-16"></a>        <span class="co"># the last column</span></span>
<span id="cb7-17"><a href="#cb7-17"></a>        <span class="va">self</span>.target <span class="op">=</span> <span class="va">self</span>.data_frame[<span class="va">self</span>.data_frame.columns[<span class="op">-</span><span class="dv">1</span>]]</span>
<span id="cb7-18"><a href="#cb7-18"></a>        <span class="co"># initialize the transform if specified</span></span>
<span id="cb7-19"><a href="#cb7-19"></a>        <span class="va">self</span>.transform <span class="op">=</span> transform</span>
<span id="cb7-20"><a href="#cb7-20"></a></span>
<span id="cb7-21"><a href="#cb7-21"></a>        <span class="co"># get length of df</span></span>
<span id="cb7-22"><a href="#cb7-22"></a>    <span class="kw">def</span> <span class="fu">__len__</span>(<span class="va">self</span>):</span>
<span id="cb7-23"><a href="#cb7-23"></a>        <span class="cf">return</span> <span class="bu">len</span>(<span class="va">self</span>.data_frame)</span>
<span id="cb7-24"><a href="#cb7-24"></a></span>
<span id="cb7-25"><a href="#cb7-25"></a>        <span class="co"># get sample target</span></span>
<span id="cb7-26"><a href="#cb7-26"></a>    <span class="kw">def</span> __get_target__(<span class="va">self</span>):</span>
<span id="cb7-27"><a href="#cb7-27"></a>        <span class="cf">return</span> <span class="va">self</span>.target</span>
<span id="cb7-28"><a href="#cb7-28"></a></span>
<span id="cb7-29"><a href="#cb7-29"></a>        <span class="co"># get df filtered by indices</span></span>
<span id="cb7-30"><a href="#cb7-30"></a>    <span class="kw">def</span> __get_values__(<span class="va">self</span>, indices):</span>
<span id="cb7-31"><a href="#cb7-31"></a>        <span class="cf">return</span> <span class="va">self</span>.data_frame.iloc[indices]</span>
<span id="cb7-32"><a href="#cb7-32"></a></span>
<span id="cb7-33"><a href="#cb7-33"></a>    <span class="kw">def</span> <span class="fu">__getitem__</span>(<span class="va">self</span>, idx):</span>
<span id="cb7-34"><a href="#cb7-34"></a>        <span class="cf">if</span> torch.is_tensor(idx):</span>
<span id="cb7-35"><a href="#cb7-35"></a>            idx <span class="op">=</span> idx.tolist()</span>
<span id="cb7-36"><a href="#cb7-36"></a></span>
<span id="cb7-37"><a href="#cb7-37"></a>        <span class="co"># pull a sample in a dict</span></span>
<span id="cb7-38"><a href="#cb7-38"></a>        sample <span class="op">=</span> {<span class="st">&#39;features&#39;</span>: torch.tensor(<span class="va">self</span>.features.iloc[idx].values),</span>
<span id="cb7-39"><a href="#cb7-39"></a>                  <span class="st">&#39;target&#39;</span>: torch.tensor(<span class="va">self</span>.target.iloc[idx]),</span>
<span id="cb7-40"><a href="#cb7-40"></a>                  <span class="st">&#39;idx&#39;</span>: torch.tensor(idx)}</span>
<span id="cb7-41"><a href="#cb7-41"></a></span>
<span id="cb7-42"><a href="#cb7-42"></a>        <span class="cf">if</span> <span class="va">self</span>.transform:</span>
<span id="cb7-43"><a href="#cb7-43"></a>            sample <span class="op">=</span> <span class="va">self</span>.transform(sample)</span>
<span id="cb7-44"><a href="#cb7-44"></a></span>
<span id="cb7-45"><a href="#cb7-45"></a>        <span class="cf">return</span> sample</span></code></pre></div>
<p>Next, a custom transform is created which applies a normalization transformation on-the-fly.</p>
<div class="sourceCode" id="cb8"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1"></a><span class="kw">class</span> Pixel_Normalize():</span>
<span id="cb8-2"><a href="#cb8-2"></a></span>
<span id="cb8-3"><a href="#cb8-3"></a>    <span class="co"># retrieve sample and unpack it</span></span>
<span id="cb8-4"><a href="#cb8-4"></a>    <span class="kw">def</span> <span class="fu">__call__</span>(<span class="va">self</span>, sample):</span>
<span id="cb8-5"><a href="#cb8-5"></a>        features, target, idx <span class="op">=</span> (sample[<span class="st">&#39;features&#39;</span>],</span>
<span id="cb8-6"><a href="#cb8-6"></a>                              sample[<span class="st">&#39;target&#39;</span>],</span>
<span id="cb8-7"><a href="#cb8-7"></a>                              sample[<span class="st">&#39;idx&#39;</span>])</span>
<span id="cb8-8"><a href="#cb8-8"></a></span>
<span id="cb8-9"><a href="#cb8-9"></a>        <span class="co"># normalize each pixel</span></span>
<span id="cb8-10"><a href="#cb8-10"></a>        normalized_pixels <span class="op">=</span> torch.true_divide(sample[<span class="st">&#39;features&#39;</span>], <span class="dv">255</span>)</span>
<span id="cb8-11"><a href="#cb8-11"></a></span>
<span id="cb8-12"><a href="#cb8-12"></a>        <span class="co"># yield another dict</span></span>
<span id="cb8-13"><a href="#cb8-13"></a>        <span class="cf">return</span> {<span class="st">&#39;features&#39;</span>: normalized_pixels,</span>
<span id="cb8-14"><a href="#cb8-14"></a>                <span class="st">&#39;target&#39;</span>: target,</span>
<span id="cb8-15"><a href="#cb8-15"></a>                <span class="st">&#39;idx&#39;</span>: idx}</span></code></pre></div>
<p>Lastly, we instantiate the custom data set and split it into train, validation, and test data sets as well as check its output.</p>
<div class="sourceCode" id="cb9"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1"></a><span class="co"># instantiate the lazy data set</span></span>
<span id="cb9-2"><a href="#cb9-2"></a>csv_dataset <span class="op">=</span> CSVDataset(csv_file<span class="op">=</span><span class="st">&#39;https://datahub.io/machine-learning/mnist_784/r/mnist_784.csv&#39;</span>,</span>
<span id="cb9-3"><a href="#cb9-3"></a>                         transform<span class="op">=</span>Pixel_Normalize())</span>
<span id="cb9-4"><a href="#cb9-4"></a></span>
<span id="cb9-5"><a href="#cb9-5"></a><span class="co"># set train, valid, and test size</span></span>
<span id="cb9-6"><a href="#cb9-6"></a>train_size <span class="op">=</span> <span class="bu">int</span>(<span class="fl">0.8</span> <span class="op">*</span> <span class="bu">len</span>(csv_dataset))</span>
<span id="cb9-7"><a href="#cb9-7"></a>valid_size <span class="op">=</span> <span class="bu">int</span>(<span class="fl">0.1</span> <span class="op">*</span> <span class="bu">len</span>(csv_dataset))</span>
<span id="cb9-8"><a href="#cb9-8"></a></span>
<span id="cb9-9"><a href="#cb9-9"></a><span class="co"># use random split to create three data sets;</span></span>
<span id="cb9-10"><a href="#cb9-10"></a>train_ds, valid_ds, test_ds <span class="op">=</span> torch.utils.data.random_split(csv_dataset, [train_size, valid_size, valid_size])</span>
<span id="cb9-11"><a href="#cb9-11"></a></span>
<span id="cb9-12"><a href="#cb9-12"></a><span class="co"># check the output</span></span>
<span id="cb9-13"><a href="#cb9-13"></a><span class="cf">for</span> i, batch <span class="kw">in</span> <span class="bu">enumerate</span>(train_ds):</span>
<span id="cb9-14"><a href="#cb9-14"></a>    <span class="cf">if</span> i <span class="op">==</span> <span class="dv">0</span>:</span>
<span id="cb9-15"><a href="#cb9-15"></a>        <span class="cf">break</span></span></code></pre></div>
<p>By preparing the <code>__get_target__()</code> method in the data set class, we can check the distribution of our dependent variable easily.</p>
<div class="sourceCode" id="cb10"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb10-1"><a href="#cb10-1"></a><span class="co"># check the distribution of dependent variable; some imbalance</span></span>
<span id="cb10-2"><a href="#cb10-2"></a>csv_dataset.__get_target__().value_counts()</span></code></pre></div>
<pre><code>## 1    7877
## 7    7293
## 3    7141
## 2    6990
## 9    6958
## 0    6903
## 6    6876
## 8    6825
## 4    6824
## 5    6313
## Name: class, dtype: int64</code></pre>
<p>To deal with imbalance, we have two options: (1) prepare a stratified-split data set, or (2) use a weighted sampler. For small data sets, option #1 is ideal because we prepare (sample) data sets in such a way that they are ideally representative of the true population. If we split them haphazardly, we do two things: (1) we violate the i.i.d assumption of machine learning, and (2) we break our sample’s underlying distribution and thereby underestimate or overestimate the population’s true distribution.</p>
<p>Since our data set is large, we will use a weighted sampler:</p>
<div class="sourceCode" id="cb12"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb12-1"><a href="#cb12-1"></a><span class="co"># prepare weighted sampling for imbalanced classification</span></span>
<span id="cb12-2"><a href="#cb12-2"></a><span class="kw">def</span> create_sampler(train_ds, csv_dataset):</span>
<span id="cb12-3"><a href="#cb12-3"></a>    <span class="co"># get indicies from train split</span></span>
<span id="cb12-4"><a href="#cb12-4"></a>    train_indices <span class="op">=</span> train_ds.indices</span>
<span id="cb12-5"><a href="#cb12-5"></a>    <span class="co"># generate class distributions [y1, y2, etc...]</span></span>
<span id="cb12-6"><a href="#cb12-6"></a>    bin_count <span class="op">=</span> np.bincount(csv_dataset.__get_target__()[train_indices])</span>
<span id="cb12-7"><a href="#cb12-7"></a>    <span class="co"># weight gen</span></span>
<span id="cb12-8"><a href="#cb12-8"></a>    weight <span class="op">=</span> <span class="fl">1.</span> <span class="op">/</span> bin_count.astype(np.float32)</span>
<span id="cb12-9"><a href="#cb12-9"></a>    <span class="co"># produce weights for each observation in the data set</span></span>
<span id="cb12-10"><a href="#cb12-10"></a>    samples_weight <span class="op">=</span> torch.tensor([weight[t] <span class="cf">for</span> t <span class="kw">in</span> csv_dataset.__get_target__()[train_indices]])</span>
<span id="cb12-11"><a href="#cb12-11"></a>    <span class="co"># prepare sampler</span></span>
<span id="cb12-12"><a href="#cb12-12"></a>    sampler <span class="op">=</span> torch.utils.data.WeightedRandomSampler(weights<span class="op">=</span>samples_weight,</span>
<span id="cb12-13"><a href="#cb12-13"></a>                                                     num_samples<span class="op">=</span><span class="bu">len</span>(samples_weight),</span>
<span id="cb12-14"><a href="#cb12-14"></a>                                                     replacement<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb12-15"><a href="#cb12-15"></a>    <span class="cf">return</span> sampler</span>
<span id="cb12-16"><a href="#cb12-16"></a></span>
<span id="cb12-17"><a href="#cb12-17"></a><span class="co"># create sampler for the training ds</span></span>
<span id="cb12-18"><a href="#cb12-18"></a>train_sampler <span class="op">=</span> create_sampler(train_ds, csv_dataset)    </span></code></pre></div>
</div>
<div id="preparing-a-feed-forward-neural-network" class="section level2" number="1.2">
<h2 number="1.2"><span class="header-section-number">1.2</span> Preparing a Feed Forward Neural Network</h2>
<p>With some of the data preparation out of the way, the next task is to build a feed forward neural network. ReLU is probably the most prominent activation function, owing to its speed and reliability. However, ReLU neurons may die during training, a form of regularization, which can happen if the input is large/small leading to a zero gradient. ReLU suffers less from a vanishing gradient problem as compared to sigmoid or Tanh, but can explode easily.</p>
<p>We generally use multiple layers because:</p>
<ol style="list-style-type: decimal">
<li><p>We can achieve the same expressiveness with more layers with fewer parameters. With fewer parameters we are less likely to overfit.</p></li>
<li><p>More layers provides a form of regularization because later layers are constrained on the behavior of earlier layers.</p></li>
<li><p>We can use different layers for different levels of feature abstraction.</p></li>
<li><p>However, more layers leads to an increased risk of vanishing or exploding gradients.</p></li>
</ol>
<div class="sourceCode" id="cb13"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb13-1"><a href="#cb13-1"></a><span class="co"># create NN</span></span>
<span id="cb13-2"><a href="#cb13-2"></a><span class="co"># nn.Module tells PyTorch to do backward propagation</span></span>
<span id="cb13-3"><a href="#cb13-3"></a><span class="kw">class</span> FF_NN(torch.nn.Module):</span>
<span id="cb13-4"><a href="#cb13-4"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, num_features, num_classes):</span>
<span id="cb13-5"><a href="#cb13-5"></a>        <span class="bu">super</span>(FF_NN, <span class="va">self</span>).<span class="fu">__init__</span>()</span>
<span id="cb13-6"><a href="#cb13-6"></a>        <span class="co"># initialize 3 layers</span></span>
<span id="cb13-7"><a href="#cb13-7"></a>        <span class="co"># first hidden layer</span></span>
<span id="cb13-8"><a href="#cb13-8"></a>        <span class="va">self</span>.linear_1 <span class="op">=</span> torch.nn.Linear(num_features, num_hidden_1)</span>
<span id="cb13-9"><a href="#cb13-9"></a>        <span class="co"># second hidden layer</span></span>
<span id="cb13-10"><a href="#cb13-10"></a>        <span class="va">self</span>.linear_2 <span class="op">=</span> torch.nn.Linear(num_hidden_1, num_hidden_2)</span>
<span id="cb13-11"><a href="#cb13-11"></a>        <span class="co"># output layer</span></span>
<span id="cb13-12"><a href="#cb13-12"></a>        <span class="va">self</span>.linear_out <span class="op">=</span> torch.nn.Linear(num_hidden_2, num_classes)</span>
<span id="cb13-13"><a href="#cb13-13"></a></span>
<span id="cb13-14"><a href="#cb13-14"></a>    <span class="co"># define how and what order model parameters should be used in forward prop.</span></span>
<span id="cb13-15"><a href="#cb13-15"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb13-16"><a href="#cb13-16"></a>        <span class="co"># run inputs through first layer</span></span>
<span id="cb13-17"><a href="#cb13-17"></a>        out <span class="op">=</span> <span class="va">self</span>.linear_1(x)</span>
<span id="cb13-18"><a href="#cb13-18"></a>        <span class="co"># apply relu</span></span>
<span id="cb13-19"><a href="#cb13-19"></a>        out <span class="op">=</span> F.relu(out)</span>
<span id="cb13-20"><a href="#cb13-20"></a>        <span class="co"># run inputs through second layer</span></span>
<span id="cb13-21"><a href="#cb13-21"></a>        out <span class="op">=</span> <span class="va">self</span>.linear_2(out)</span>
<span id="cb13-22"><a href="#cb13-22"></a>        <span class="co"># apply relu</span></span>
<span id="cb13-23"><a href="#cb13-23"></a>        out <span class="op">=</span> F.relu(out)</span>
<span id="cb13-24"><a href="#cb13-24"></a>        <span class="co"># run inputs through final classification layer</span></span>
<span id="cb13-25"><a href="#cb13-25"></a>        logits <span class="op">=</span> <span class="va">self</span>.linear_out(out)</span>
<span id="cb13-26"><a href="#cb13-26"></a>        probs <span class="op">=</span> F.log_softmax(logits, dim<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb13-27"><a href="#cb13-27"></a>        <span class="cf">return</span> logits, probs</span>
<span id="cb13-28"><a href="#cb13-28"></a>        </span>
<span id="cb13-29"><a href="#cb13-29"></a><span class="co"># load the NN model</span></span>
<span id="cb13-30"><a href="#cb13-30"></a>num_features <span class="op">=</span> <span class="dv">784</span></span>
<span id="cb13-31"><a href="#cb13-31"></a>num_hidden_1 <span class="op">=</span> <span class="dv">128</span></span>
<span id="cb13-32"><a href="#cb13-32"></a>num_hidden_2 <span class="op">=</span> <span class="dv">256</span></span>
<span id="cb13-33"><a href="#cb13-33"></a>num_classes <span class="op">=</span> <span class="dv">10</span></span>
<span id="cb13-34"><a href="#cb13-34"></a>model <span class="op">=</span> FF_NN(num_features<span class="op">=</span>num_features, num_classes<span class="op">=</span>num_classes).to(DEVICE)</span></code></pre></div>
</div>
<div id="training-helper-functions" class="section level2" number="1.3">
<h2 number="1.3"><span class="header-section-number">1.3</span> Training Helper Functions</h2>
<p>There are a few things worth pointing out in terms of what is happening in the training function below.</p>
<ol style="list-style-type: decimal">
<li><p>When invoking <code>scaler.scale(loss).backward()</code>, we are computing the gradients. <code>Backward()</code> is automatically constructed by torch’s <code>autograd</code> based on the <code>forward()</code> method and the loss function.</p></li>
<li><p>When invoking <code>scaler.step(optimizer)</code>, we use the gradients to update the weights according to the optimization method. For example, given SGD: weight = weight + learning_rate <span class="math inline">\(\times\)</span> gradient.</p></li>
<li><p><code>F.cross_entropy</code> is our loss function which takes logits as inputs and performs <code>log_softmax</code> internally. It is desirable to use this loss function over something like <code>nll_loss</code> because it is more numerically stable.</p></li>
</ol>
<p>A PyTorch cheat sheet for loss functions is as follows:</p>
<ul>
<li><code>torch.nn.functional.binary_cross_entropy</code> takes logistic sigmoid values as inputs</li>
<li><code>torch.nn.functional.binary_cross_entropy_with_logits</code> takes logits as inputs</li>
<li><code>torch.nn.functional.cross_entropy</code> takes logits as inputs (performs <code>log_softmax</code> internally)</li>
<li><code>torch.nn.functional.nll_loss</code> is like cross_entropy but takes log-probabilities (<code>log-softmax</code>) values as inputs</li>
</ul>
<div class="sourceCode" id="cb14"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb14-1"><a href="#cb14-1"></a><span class="co"># train function</span></span>
<span id="cb14-2"><a href="#cb14-2"></a><span class="kw">def</span> train(dataloader):</span>
<span id="cb14-3"><a href="#cb14-3"></a>    <span class="co">#pbar = ProgressBar(n_total=len(dataloader), desc=&#39;Training&#39;)</span></span>
<span id="cb14-4"><a href="#cb14-4"></a>    train_loss <span class="op">=</span> AverageMeter()</span>
<span id="cb14-5"><a href="#cb14-5"></a>    model.train()</span>
<span id="cb14-6"><a href="#cb14-6"></a>    <span class="cf">for</span> batch_idx, batch <span class="kw">in</span> <span class="bu">enumerate</span>(dataloader):</span>
<span id="cb14-7"><a href="#cb14-7"></a>        b_features, b_target, b_idx <span class="op">=</span> batch[<span class="st">&#39;features&#39;</span>].to(DEVICE),  batch[<span class="st">&#39;target&#39;</span>].to(DEVICE), batch[<span class="st">&#39;idx&#39;</span>].to(DEVICE)</span>
<span id="cb14-8"><a href="#cb14-8"></a>        optimizer.zero_grad()</span>
<span id="cb14-9"><a href="#cb14-9"></a>        <span class="cf">with</span> autocast():</span>
<span id="cb14-10"><a href="#cb14-10"></a>            logits, probs <span class="op">=</span> model(b_features)</span>
<span id="cb14-11"><a href="#cb14-11"></a>            loss <span class="op">=</span> F.cross_entropy(logits, b_target)</span>
<span id="cb14-12"><a href="#cb14-12"></a>        scaler.scale(loss).backward()</span>
<span id="cb14-13"><a href="#cb14-13"></a>        scaler.step(optimizer)</span>
<span id="cb14-14"><a href="#cb14-14"></a>        scaler.update()</span>
<span id="cb14-15"><a href="#cb14-15"></a>        scheduler.step()</span>
<span id="cb14-16"><a href="#cb14-16"></a>        <span class="co"># pbar(step=batch_idx, info={&#39;loss&#39;: loss.item()})  # removed for markdown</span></span>
<span id="cb14-17"><a href="#cb14-17"></a>        train_loss.update(loss.item(), n<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb14-18"><a href="#cb14-18"></a>    <span class="cf">return</span> {<span class="st">&#39;loss&#39;</span>: train_loss.avg}</span>
<span id="cb14-19"><a href="#cb14-19"></a></span>
<span id="cb14-20"><a href="#cb14-20"></a></span>
<span id="cb14-21"><a href="#cb14-21"></a><span class="co"># valid/test function</span></span>
<span id="cb14-22"><a href="#cb14-22"></a><span class="kw">def</span> test(dataloader):</span>
<span id="cb14-23"><a href="#cb14-23"></a>    <span class="co">#pbar = ProgressBar(n_total=len(dataloader), desc=&#39;Testing&#39;)</span></span>
<span id="cb14-24"><a href="#cb14-24"></a>    valid_loss <span class="op">=</span> AverageMeter()</span>
<span id="cb14-25"><a href="#cb14-25"></a>    valid_acc <span class="op">=</span> AverageMeter()</span>
<span id="cb14-26"><a href="#cb14-26"></a>    valid_f1 <span class="op">=</span> AverageMeter()</span>
<span id="cb14-27"><a href="#cb14-27"></a>    model.<span class="bu">eval</span>()</span>
<span id="cb14-28"><a href="#cb14-28"></a>    count <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb14-29"><a href="#cb14-29"></a>    <span class="cf">with</span> torch.no_grad():</span>
<span id="cb14-30"><a href="#cb14-30"></a>        <span class="cf">for</span> batch_idx, batch <span class="kw">in</span> <span class="bu">enumerate</span>(dataloader):</span>
<span id="cb14-31"><a href="#cb14-31"></a>            b_features, b_target, b_idx <span class="op">=</span> batch[<span class="st">&#39;features&#39;</span>].to(DEVICE),  batch[<span class="st">&#39;target&#39;</span>].to(DEVICE), batch[<span class="st">&#39;idx&#39;</span>].to(DEVICE)</span>
<span id="cb14-32"><a href="#cb14-32"></a>            logits, probs <span class="op">=</span> model(b_features)</span>
<span id="cb14-33"><a href="#cb14-33"></a>            loss <span class="op">=</span> F.cross_entropy(logits, b_target).item()</span>
<span id="cb14-34"><a href="#cb14-34"></a>            pred <span class="op">=</span> probs.argmax(dim<span class="op">=</span><span class="dv">1</span>, keepdim<span class="op">=</span><span class="va">True</span>)  <span class="co"># get the index of the max log-probability</span></span>
<span id="cb14-35"><a href="#cb14-35"></a>            correct <span class="op">=</span> pred.eq(b_target.view_as(pred)).<span class="bu">sum</span>().item()</span>
<span id="cb14-36"><a href="#cb14-36"></a>            f1 <span class="op">=</span> f1_score(pred.to(<span class="st">&quot;cpu&quot;</span>).numpy(), b_target.to(<span class="st">&quot;cpu&quot;</span>).numpy(), average<span class="op">=</span><span class="st">&#39;macro&#39;</span>)</span>
<span id="cb14-37"><a href="#cb14-37"></a>            valid_f1.update(f1, n<span class="op">=</span>b_features.size(<span class="dv">0</span>))</span>
<span id="cb14-38"><a href="#cb14-38"></a>            valid_loss.update(loss, n<span class="op">=</span>b_features.size(<span class="dv">0</span>))</span>
<span id="cb14-39"><a href="#cb14-39"></a>            valid_acc.update(correct, n<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb14-40"><a href="#cb14-40"></a>            count <span class="op">+=</span> b_features.size(<span class="dv">0</span>)</span>
<span id="cb14-41"><a href="#cb14-41"></a>            <span class="co"># pbar(step=batch_idx)  # removed for markdown</span></span>
<span id="cb14-42"><a href="#cb14-42"></a>    <span class="cf">return</span> {<span class="st">&#39;valid_loss&#39;</span>: valid_loss.avg,</span>
<span id="cb14-43"><a href="#cb14-43"></a>            <span class="st">&#39;valid_acc&#39;</span>: valid_acc.<span class="bu">sum</span> <span class="op">/</span>count,</span>
<span id="cb14-44"><a href="#cb14-44"></a>            <span class="st">&#39;valid_f1&#39;</span>: valid_f1.avg}</span></code></pre></div>
</div>
<div id="data-loaders-and-misc" class="section level2" number="1.4">
<h2 number="1.4"><span class="header-section-number">1.4</span> Data Loaders and Misc</h2>
<div class="sourceCode" id="cb15"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb15-1"><a href="#cb15-1"></a><span class="co"># set number of epochs</span></span>
<span id="cb15-2"><a href="#cb15-2"></a>epochs <span class="op">=</span> <span class="dv">5</span></span>
<span id="cb15-3"><a href="#cb15-3"></a></span>
<span id="cb15-4"><a href="#cb15-4"></a><span class="co"># create DataLoaders with samplers</span></span>
<span id="cb15-5"><a href="#cb15-5"></a>train_dataloader <span class="op">=</span> DataLoader(train_ds,</span>
<span id="cb15-6"><a href="#cb15-6"></a>                              batch_size<span class="op">=</span><span class="dv">100</span>,</span>
<span id="cb15-7"><a href="#cb15-7"></a>                              sampler<span class="op">=</span>train_sampler,</span>
<span id="cb15-8"><a href="#cb15-8"></a>                              shuffle<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb15-9"><a href="#cb15-9"></a></span>
<span id="cb15-10"><a href="#cb15-10"></a>valid_dataloader <span class="op">=</span> DataLoader(valid_ds,</span>
<span id="cb15-11"><a href="#cb15-11"></a>                              batch_size<span class="op">=</span><span class="dv">100</span>,</span>
<span id="cb15-12"><a href="#cb15-12"></a>                              shuffle<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb15-13"><a href="#cb15-13"></a></span>
<span id="cb15-14"><a href="#cb15-14"></a>test_dataloader <span class="op">=</span> DataLoader(test_ds,</span>
<span id="cb15-15"><a href="#cb15-15"></a>                              batch_size<span class="op">=</span><span class="dv">100</span>,</span>
<span id="cb15-16"><a href="#cb15-16"></a>                              shuffle<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb15-17"><a href="#cb15-17"></a></span>
<span id="cb15-18"><a href="#cb15-18"></a><span class="co"># create gradient scaler for mixed precision</span></span>
<span id="cb15-19"><a href="#cb15-19"></a>scaler <span class="op">=</span> GradScaler()</span></code></pre></div>
</div>
<div id="optimizer-and-scheduler" class="section level2" number="1.5">
<h2 number="1.5"><span class="header-section-number">1.5</span> Optimizer and Scheduler</h2>
<p>Next, Rectified Adam (<code>RAdam</code>) is implemented along with a <code>OneCycleLR</code> scheduler.</p>
<div class="sourceCode" id="cb16"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb16-1"><a href="#cb16-1"></a><span class="co"># create optimizer</span></span>
<span id="cb16-2"><a href="#cb16-2"></a>optimizer <span class="op">=</span> RAdam(model.parameters(), lr<span class="op">=</span><span class="fl">0.1</span>)</span>
<span id="cb16-3"><a href="#cb16-3"></a></span>
<span id="cb16-4"><a href="#cb16-4"></a><span class="co"># set LR scheduler</span></span>
<span id="cb16-5"><a href="#cb16-5"></a>scheduler <span class="op">=</span> torch.optim.lr_scheduler.OneCycleLR(optimizer,</span>
<span id="cb16-6"><a href="#cb16-6"></a>                                                max_lr<span class="op">=</span><span class="fl">0.01</span>,</span>
<span id="cb16-7"><a href="#cb16-7"></a>                                                total_steps<span class="op">=</span><span class="bu">len</span>(train_dataloader)<span class="op">*</span>epochs)</span></code></pre></div>
<div class="sourceCode" id="cb17"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb17-1"><a href="#cb17-1"></a><span class="co"># training</span></span>
<span id="cb17-2"><a href="#cb17-2"></a><span class="cf">for</span> epoch <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">1</span>, epochs <span class="op">+</span> <span class="dv">1</span>):</span>
<span id="cb17-3"><a href="#cb17-3"></a>    train_log <span class="op">=</span> train(train_dataloader)</span>
<span id="cb17-4"><a href="#cb17-4"></a>    valid_log <span class="op">=</span> test(valid_dataloader)</span>
<span id="cb17-5"><a href="#cb17-5"></a>    logs <span class="op">=</span> <span class="bu">dict</span>(train_log, <span class="op">**</span>valid_log)</span>
<span id="cb17-6"><a href="#cb17-6"></a>    show_info <span class="op">=</span> <span class="ss">f&#39;</span><span class="ch">\n</span><span class="ss">Epoch: </span><span class="sc">{</span>epoch<span class="sc">}</span><span class="ss"> - &#39;</span> <span class="op">+</span> <span class="st">&quot;-&quot;</span>.join([<span class="ss">f&#39; </span><span class="sc">{</span>key<span class="sc">}</span><span class="ss">: </span><span class="sc">{</span>value<span class="sc">:.4f}</span><span class="ss"> &#39;</span> <span class="cf">for</span> key, value <span class="kw">in</span> logs.items()])</span>
<span id="cb17-7"><a href="#cb17-7"></a>    <span class="bu">print</span>(show_info)</span></code></pre></div>
<pre><code>## 
## Epoch: 1 -  loss: 0.7573 - valid_loss: 0.1846 - valid_acc: 0.9439 - valid_f1: 0.9404 
## 
## Epoch: 2 -  loss: 0.1576 - valid_loss: 0.1325 - valid_acc: 0.9599 - valid_f1: 0.9586 
## 
## Epoch: 3 -  loss: 0.0956 - valid_loss: 0.1134 - valid_acc: 0.9660 - valid_f1: 0.9636 
## 
## Epoch: 4 -  loss: 0.0485 - valid_loss: 0.0689 - valid_acc: 0.9809 - valid_f1: 0.9799 
## 
## Epoch: 5 -  loss: 0.0245 - valid_loss: 0.0670 - valid_acc: 0.9806 - valid_f1: 0.9790 
## 
## C:/Users/Andrew/Desktop/Projects/Deep Learning/utils\radam.py:60: UserWarning: This overload of add_ is deprecated:
##  add_(Number alpha, Tensor other)
## Consider using one of the following signatures instead:
##  add_(Tensor other, *, Number alpha) (Triggered internally at  ..\torch\csrc\utils\python_arg_parser.cpp:766.)
##   exp_avg.mul_(beta1).add_(1 - beta1, grad)</code></pre>
<div class="sourceCode" id="cb19"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb19-1"><a href="#cb19-1"></a><span class="co"># testing</span></span>
<span id="cb19-2"><a href="#cb19-2"></a>test_log <span class="op">=</span> test(test_dataloader)</span>
<span id="cb19-3"><a href="#cb19-3"></a><span class="bu">print</span>(test_log)</span></code></pre></div>
<pre><code>## {&#39;valid_loss&#39;: 0.06291491132163043, &#39;valid_acc&#39;: 0.9818571428571429, &#39;valid_f1&#39;: 0.9804316277722123}</code></pre>
</div>
</div>
<div id="sources" class="section level1" number="2">
<h1 number="2"><span class="header-section-number">2</span> Sources</h1>
<ul>
<li><p><a href="https://github.com/rasbt/stat479-deep-learning-ss19/blob/master/L09_mlp/L09_mlp_slides.pdf" class="uri">https://github.com/rasbt/stat479-deep-learning-ss19/blob/master/L09_mlp/L09_mlp_slides.pdf</a></p></li>
<li><p><a href="https://github.com/LiyuanLucasLiu/RAdam" class="uri">https://github.com/LiyuanLucasLiu/RAdam</a></p></li>
<li><p><a href="https://arxiv.org/pdf/1708.07120.pdf" class="uri">https://arxiv.org/pdf/1708.07120.pdf</a></p></li>
</ul>
</div>



<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>
