<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml">

<head>

<meta charset="utf-8" />
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="generator" content="pandoc" />

<meta name="viewport" content="width=device-width, initial-scale=1">

<meta name="author" content="Andrew Fogarty" />

<meta name="date" content="2020-10-14" />

<title>Optimization</title>



<style type="text/css">code{white-space: pre;}</style>
<style type="text/css" data-origin="pandoc">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */

</style>
<script>
// apply pandoc div.sourceCode style to pre.sourceCode instead
(function() {
  var sheets = document.styleSheets;
  for (var i = 0; i < sheets.length; i++) {
    if (sheets[i].ownerNode.dataset["origin"] !== "pandoc") continue;
    try { var rules = sheets[i].cssRules; } catch (e) { continue; }
    for (var j = 0; j < rules.length; j++) {
      var rule = rules[j];
      // check if there is a div.sourceCode rule
      if (rule.type !== rule.STYLE_RULE || rule.selectorText !== "div.sourceCode") continue;
      var style = rule.style.cssText;
      // check if color or background-color is set
      if (rule.style.color === '' || rule.style.backgroundColor === '') continue;
      // replace div.sourceCode by a pre.sourceCode rule
      sheets[i].deleteRule(j);
      sheets[i].insertRule('pre.sourceCode{' + style + '}', j);
    }
  }
})();
</script>



<style type="text/css">body {
background-color: #fff;
margin: 1em auto;
max-width: 700px;
overflow: visible;
padding-left: 2em;
padding-right: 2em;
font-family: "Open Sans", "Helvetica Neue", Helvetica, Arial, sans-serif;
font-size: 14px;
line-height: 1.35;
}
#header {
text-align: center;
}
#TOC {
clear: both;
margin: 0 0 10px 10px;
padding: 4px;
width: 400px;
border: 1px solid #CCCCCC;
border-radius: 5px;
background-color: #f6f6f6;
font-size: 13px;
line-height: 1.3;
}
#TOC .toctitle {
font-weight: bold;
font-size: 15px;
margin-left: 5px;
}
#TOC ul {
padding-left: 40px;
margin-left: -1.5em;
margin-top: 5px;
margin-bottom: 5px;
}
#TOC ul ul {
margin-left: -2em;
}
#TOC li {
line-height: 16px;
}
table {
margin: 1em auto;
border-width: 1px;
border-color: #DDDDDD;
border-style: outset;
border-collapse: collapse;
}
table th {
border-width: 2px;
padding: 5px;
border-style: inset;
}
table td {
border-width: 1px;
border-style: inset;
line-height: 18px;
padding: 5px 5px;
}
table, table th, table td {
border-left-style: none;
border-right-style: none;
}
table thead, table tr.even {
background-color: #f7f7f7;
}
p {
margin: 0.5em 0;
}
blockquote {
background-color: #f6f6f6;
padding: 0.25em 0.75em;
}
hr {
border-style: solid;
border: none;
border-top: 1px solid #777;
margin: 28px 0;
}
dl {
margin-left: 0;
}
dl dd {
margin-bottom: 13px;
margin-left: 13px;
}
dl dt {
font-weight: bold;
}
ul {
margin-top: 0;
}
ul li {
list-style: circle outside;
}
ul ul {
margin-bottom: 0;
}
pre, code {
background-color: #f7f7f7;
border-radius: 3px;
color: #333;
white-space: pre-wrap; 
}
pre {
border-radius: 3px;
margin: 5px 0px 10px 0px;
padding: 10px;
}
pre:not([class]) {
background-color: #f7f7f7;
}
code {
font-family: Consolas, Monaco, 'Courier New', monospace;
font-size: 85%;
}
p > code, li > code {
padding: 2px 0px;
}
div.figure {
text-align: center;
}
img {
background-color: #FFFFFF;
padding: 2px;
border: 1px solid #DDDDDD;
border-radius: 3px;
border: 1px solid #CCCCCC;
margin: 0 5px;
}
h1 {
margin-top: 0;
font-size: 35px;
line-height: 40px;
}
h2 {
border-bottom: 4px solid #f7f7f7;
padding-top: 10px;
padding-bottom: 2px;
font-size: 145%;
}
h3 {
border-bottom: 2px solid #f7f7f7;
padding-top: 10px;
font-size: 120%;
}
h4 {
border-bottom: 1px solid #f7f7f7;
margin-left: 8px;
font-size: 105%;
}
h5, h6 {
border-bottom: 1px solid #ccc;
font-size: 105%;
}
a {
color: #0033dd;
text-decoration: none;
}
a:hover {
color: #6666ff; }
a:visited {
color: #800080; }
a:visited:hover {
color: #BB00BB; }
a[href^="http:"] {
text-decoration: underline; }
a[href^="https:"] {
text-decoration: underline; }

code > span.kw { color: #555; font-weight: bold; } 
code > span.dt { color: #902000; } 
code > span.dv { color: #40a070; } 
code > span.bn { color: #d14; } 
code > span.fl { color: #d14; } 
code > span.ch { color: #d14; } 
code > span.st { color: #d14; } 
code > span.co { color: #888888; font-style: italic; } 
code > span.ot { color: #007020; } 
code > span.al { color: #ff0000; font-weight: bold; } 
code > span.fu { color: #900; font-weight: bold; }  code > span.er { color: #a61717; background-color: #e3d2d2; } 
</style>




</head>

<body>




<h1 class="title toc-ignore">Optimization</h1>
<h4 class="author">Andrew Fogarty</h4>
<h4 class="date">10/14/2020</h4>


<div id="TOC">
<ul>
<li><a href="#introduction"><span class="toc-section-number">1</span> Introduction</a></li>
<li><a href="#generating-data-and-loading-on-the-fly"><span class="toc-section-number">2</span> Generating Data and Loading On-The-Fly</a></li>
<li><a href="#by-hand-logistic-regression-with-l2-regularization"><span class="toc-section-number">3</span> By Hand: Logistic Regression with L2 Regularization</a></li>
<li><a href="#data-loaders"><span class="toc-section-number">4</span> Data Loaders</a></li>
<li><a href="#training"><span class="toc-section-number">5</span> Training</a>
<ul>
<li><a href="#training-coefficients"><span class="toc-section-number">5.1</span> Training Coefficients</a></li>
</ul></li>
<li><a href="#testing"><span class="toc-section-number">6</span> Testing</a>
<ul>
<li><a href="#testing-coefficients"><span class="toc-section-number">6.1</span> Testing Coefficients</a></li>
</ul></li>
<li><a href="#verification"><span class="toc-section-number">7</span> Verification</a></li>
<li><a href="#by-hand-linear-regression-with-l2-regularization"><span class="toc-section-number">8</span> By Hand: Linear Regression with L2 Regularization</a>
<ul>
<li><a href="#prepare-data"><span class="toc-section-number">8.1</span> Prepare Data</a></li>
<li><a href="#linear-model"><span class="toc-section-number">8.2</span> Linear Model</a></li>
<li><a href="#check-output"><span class="toc-section-number">8.3</span> Check Output</a></li>
<li><a href="#compare-with-sklearn"><span class="toc-section-number">8.4</span> Compare with Sklearn</a></li>
</ul></li>
</ul>
</div>

<div class="sourceCode" id="cb1"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1-1"><a href="#cb1-1"></a><span class="co"># load python</span></span>
<span id="cb1-2"><a href="#cb1-2"></a><span class="kw">library</span>(reticulate)</span>
<span id="cb1-3"><a href="#cb1-3"></a><span class="kw">use_condaenv</span>(<span class="st">&quot;my_ml&quot;</span>)</span></code></pre></div>
<div class="sourceCode" id="cb2"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1"></a><span class="co"># import packages</span></span>
<span id="cb2-2"><a href="#cb2-2"></a><span class="im">import</span> torch</span>
<span id="cb2-3"><a href="#cb2-3"></a><span class="im">from</span> torch.utils.data <span class="im">import</span> Dataset, random_split, DataLoader, RandomSampler</span>
<span id="cb2-4"><a href="#cb2-4"></a><span class="im">import</span> torch.nn.functional <span class="im">as</span> F</span>
<span id="cb2-5"><a href="#cb2-5"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb2-6"><a href="#cb2-6"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb2-7"><a href="#cb2-7"></a><span class="im">from</span> sklearn <span class="im">import</span> datasets</span>
<span id="cb2-8"><a href="#cb2-8"></a><span class="im">from</span> sklearn.linear_model <span class="im">import</span> LogisticRegression</span>
<span id="cb2-9"><a href="#cb2-9"></a><span class="im">import</span> sys</span>
<span id="cb2-10"><a href="#cb2-10"></a>sys.path.append(<span class="st">&quot;C:/Users/Andrew/Desktop/Projects/Deep Learning/utils&quot;</span>)</span>
<span id="cb2-11"><a href="#cb2-11"></a><span class="im">from</span> tools <span class="im">import</span> AverageMeter, ProgressBar</span>
<span id="cb2-12"><a href="#cb2-12"></a></span>
<span id="cb2-13"><a href="#cb2-13"></a></span>
<span id="cb2-14"><a href="#cb2-14"></a>SEED <span class="op">=</span> <span class="dv">123</span></span>
<span id="cb2-15"><a href="#cb2-15"></a>torch.manual_seed(SEED)</span></code></pre></div>
<pre><code>## &lt;torch._C.Generator object at 0x000000002058C050&gt;</code></pre>
<div class="sourceCode" id="cb4"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1"></a>torch.backends.cudnn.deterministic <span class="op">=</span> <span class="va">True</span></span>
<span id="cb4-2"><a href="#cb4-2"></a>torch.cuda.amp.autocast(enabled<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb4-3"><a href="#cb4-3"></a></span>
<span id="cb4-4"><a href="#cb4-4"></a><span class="co"># set torch device</span></span></code></pre></div>
<pre><code>## &lt;torch.cuda.amp.autocast_mode.autocast object at 0x000000001F86B808&gt;</code></pre>
<div class="sourceCode" id="cb6"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1"></a>device <span class="op">=</span> torch.device(<span class="st">&quot;cuda:0&quot;</span> <span class="cf">if</span> torch.cuda.is_available() <span class="cf">else</span> <span class="st">&quot;cpu&quot;</span>)</span></code></pre></div>
<div id="introduction" class="section level1" number="1">
<h1 number="1"><span class="header-section-number">1</span> Introduction</h1>
<p>Numerical optimizers provide the means to estimate our parameters by finding the values that maximize the likelihood of generating our data. This guide helps us understand how optimization algorithms find the best estimates for our coefficients in a step-by-step process.</p>
<p>This guide uses Gradient Descent and regularization techniques in a completely manual approach to finding parameters that most likely generated our data. Another way of thinking about gradient descent is that we are inevitably asking the algorithm the following: What parameter values will push our error to zero?</p>
</div>
<div id="generating-data-and-loading-on-the-fly" class="section level1" number="2">
<h1 number="2"><span class="header-section-number">2</span> Generating Data and Loading On-The-Fly</h1>
<p>In this section, we will create a torch data set that reads a csv file and processes (normalizes) our data on the fly via the <code>standardize</code> class.</p>
<div class="sourceCode" id="cb7"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1"></a><span class="co"># create Dataset</span></span>
<span id="cb7-2"><a href="#cb7-2"></a><span class="kw">class</span> CSVDataset(Dataset):</span>
<span id="cb7-3"><a href="#cb7-3"></a>    <span class="co">&quot;&quot;&quot;LM dataset.&quot;&quot;&quot;</span></span>
<span id="cb7-4"><a href="#cb7-4"></a></span>
<span id="cb7-5"><a href="#cb7-5"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, csv_file, transform<span class="op">=</span><span class="va">None</span>):</span>
<span id="cb7-6"><a href="#cb7-6"></a>        <span class="co">&quot;&quot;&quot;</span></span>
<span id="cb7-7"><a href="#cb7-7"></a><span class="co">        Args:</span></span>
<span id="cb7-8"><a href="#cb7-8"></a><span class="co">            csv_file (string): Path to the csv file.</span></span>
<span id="cb7-9"><a href="#cb7-9"></a><span class="co">            transform (callable, optional): Optional transform to be applied</span></span>
<span id="cb7-10"><a href="#cb7-10"></a><span class="co">                on a sample.</span></span>
<span id="cb7-11"><a href="#cb7-11"></a><span class="co">        &quot;&quot;&quot;</span></span>
<span id="cb7-12"><a href="#cb7-12"></a>        <span class="co"># initialize</span></span>
<span id="cb7-13"><a href="#cb7-13"></a>        <span class="va">self</span>.data_frame <span class="op">=</span> pd.read_csv(csv_file)</span>
<span id="cb7-14"><a href="#cb7-14"></a>        <span class="co"># all columns but the last</span></span>
<span id="cb7-15"><a href="#cb7-15"></a>        <span class="va">self</span>.features <span class="op">=</span> <span class="va">self</span>.data_frame[<span class="va">self</span>.data_frame.columns[:<span class="op">-</span><span class="dv">1</span>]]</span>
<span id="cb7-16"><a href="#cb7-16"></a>        <span class="co"># the last column</span></span>
<span id="cb7-17"><a href="#cb7-17"></a>        <span class="va">self</span>.target <span class="op">=</span> <span class="va">self</span>.data_frame[<span class="va">self</span>.data_frame.columns[<span class="op">-</span><span class="dv">1</span>]]</span>
<span id="cb7-18"><a href="#cb7-18"></a>        <span class="co"># initialize the transform if specified</span></span>
<span id="cb7-19"><a href="#cb7-19"></a>        <span class="va">self</span>.transform <span class="op">=</span> transform</span>
<span id="cb7-20"><a href="#cb7-20"></a></span>
<span id="cb7-21"><a href="#cb7-21"></a>        <span class="co"># get length of df</span></span>
<span id="cb7-22"><a href="#cb7-22"></a>    <span class="kw">def</span> <span class="fu">__len__</span>(<span class="va">self</span>):</span>
<span id="cb7-23"><a href="#cb7-23"></a>        <span class="cf">return</span> <span class="bu">len</span>(<span class="va">self</span>.data_frame)</span>
<span id="cb7-24"><a href="#cb7-24"></a></span>
<span id="cb7-25"><a href="#cb7-25"></a>        <span class="co"># get df mean and std</span></span>
<span id="cb7-26"><a href="#cb7-26"></a>    <span class="kw">def</span> __get_norm__(<span class="va">self</span>):</span>
<span id="cb7-27"><a href="#cb7-27"></a>        <span class="va">self</span>.mu, <span class="va">self</span>.sigma <span class="op">=</span> np.mean(<span class="va">self</span>.features.values, axis<span class="op">=</span><span class="dv">0</span>), np.std(<span class="va">self</span>.features.values, axis<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb7-28"><a href="#cb7-28"></a>        <span class="cf">return</span> <span class="va">self</span>.mu, <span class="va">self</span>.sigma</span>
<span id="cb7-29"><a href="#cb7-29"></a></span>
<span id="cb7-30"><a href="#cb7-30"></a>        <span class="co"># get sample target</span></span>
<span id="cb7-31"><a href="#cb7-31"></a>    <span class="kw">def</span> __get_target__(<span class="va">self</span>):</span>
<span id="cb7-32"><a href="#cb7-32"></a>        <span class="cf">return</span> <span class="va">self</span>.target</span>
<span id="cb7-33"><a href="#cb7-33"></a></span>
<span id="cb7-34"><a href="#cb7-34"></a>        <span class="co"># get df filtered by indices</span></span>
<span id="cb7-35"><a href="#cb7-35"></a>    <span class="kw">def</span> __get_values__(<span class="va">self</span>, indices):</span>
<span id="cb7-36"><a href="#cb7-36"></a>        <span class="cf">return</span> <span class="va">self</span>.data_frame.iloc[indices]</span>
<span id="cb7-37"><a href="#cb7-37"></a></span>
<span id="cb7-38"><a href="#cb7-38"></a>    <span class="kw">def</span> <span class="fu">__getitem__</span>(<span class="va">self</span>, idx):</span>
<span id="cb7-39"><a href="#cb7-39"></a>        <span class="cf">if</span> torch.is_tensor(idx):</span>
<span id="cb7-40"><a href="#cb7-40"></a>            idx <span class="op">=</span> idx.tolist()</span>
<span id="cb7-41"><a href="#cb7-41"></a></span>
<span id="cb7-42"><a href="#cb7-42"></a>        <span class="co"># pull a sample in a dict</span></span>
<span id="cb7-43"><a href="#cb7-43"></a>        sample <span class="op">=</span> {<span class="st">&#39;features&#39;</span>: torch.tensor(<span class="va">self</span>.features.iloc[idx].values),</span>
<span id="cb7-44"><a href="#cb7-44"></a>                  <span class="st">&#39;target&#39;</span>: torch.tensor(<span class="va">self</span>.target.iloc[idx]),</span>
<span id="cb7-45"><a href="#cb7-45"></a>                  <span class="st">&#39;idx&#39;</span>: torch.tensor(idx)}</span>
<span id="cb7-46"><a href="#cb7-46"></a></span>
<span id="cb7-47"><a href="#cb7-47"></a>        <span class="cf">if</span> <span class="va">self</span>.transform:</span>
<span id="cb7-48"><a href="#cb7-48"></a>            sample <span class="op">=</span> <span class="va">self</span>.transform(sample)</span>
<span id="cb7-49"><a href="#cb7-49"></a></span>
<span id="cb7-50"><a href="#cb7-50"></a>        <span class="cf">return</span> sample</span>
<span id="cb7-51"><a href="#cb7-51"></a></span>
<span id="cb7-52"><a href="#cb7-52"></a></span>
<span id="cb7-53"><a href="#cb7-53"></a><span class="kw">class</span> Standardize():</span>
<span id="cb7-54"><a href="#cb7-54"></a>    <span class="co"># retrieve sample and unpack it</span></span>
<span id="cb7-55"><a href="#cb7-55"></a>    <span class="kw">def</span> <span class="fu">__call__</span>(<span class="va">self</span>, sample):</span>
<span id="cb7-56"><a href="#cb7-56"></a>        features, target, idx <span class="op">=</span> (sample[<span class="st">&#39;features&#39;</span>],</span>
<span id="cb7-57"><a href="#cb7-57"></a>                              sample[<span class="st">&#39;target&#39;</span>],</span>
<span id="cb7-58"><a href="#cb7-58"></a>                              sample[<span class="st">&#39;idx&#39;</span>])</span>
<span id="cb7-59"><a href="#cb7-59"></a></span>
<span id="cb7-60"><a href="#cb7-60"></a>        <span class="co"># normalize each value; zero mean, unit variance</span></span>
<span id="cb7-61"><a href="#cb7-61"></a>        normalized_features <span class="op">=</span> (features <span class="op">-</span> csv_dataset.__get_norm__()[<span class="dv">0</span>]) <span class="op">/</span> csv_dataset.__get_norm__()[<span class="dv">1</span>]</span>
<span id="cb7-62"><a href="#cb7-62"></a></span>
<span id="cb7-63"><a href="#cb7-63"></a>        <span class="co"># yield another dict</span></span>
<span id="cb7-64"><a href="#cb7-64"></a>        <span class="cf">return</span> {<span class="st">&#39;features&#39;</span>: torch.as_tensor(normalized_features,</span>
<span id="cb7-65"><a href="#cb7-65"></a>                                         dtype<span class="op">=</span>torch.float32,</span>
<span id="cb7-66"><a href="#cb7-66"></a>                                         device<span class="op">=</span>device),</span>
<span id="cb7-67"><a href="#cb7-67"></a>                <span class="st">&#39;target&#39;</span>: torch.as_tensor(target,</span>
<span id="cb7-68"><a href="#cb7-68"></a>                                          dtype<span class="op">=</span>torch.float32,</span>
<span id="cb7-69"><a href="#cb7-69"></a>                                          device<span class="op">=</span>device),</span>
<span id="cb7-70"><a href="#cb7-70"></a>                <span class="st">&#39;idx&#39;</span>: torch.as_tensor(idx,</span>
<span id="cb7-71"><a href="#cb7-71"></a>                                       dtype<span class="op">=</span>torch.<span class="bu">int</span>,</span>
<span id="cb7-72"><a href="#cb7-72"></a>                                       device<span class="op">=</span>device)}</span></code></pre></div>
<p>We will use <code>sklearn</code>’s <code>make_classification</code> method to generate some data useful for logistic regression.</p>
<div class="sourceCode" id="cb8"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1"></a>X, y <span class="op">=</span> datasets.make_classification(n_samples<span class="op">=</span><span class="dv">1000</span>,</span>
<span id="cb8-2"><a href="#cb8-2"></a>                                         n_features<span class="op">=</span><span class="dv">2</span>,</span>
<span id="cb8-3"><a href="#cb8-3"></a>                                         n_informative<span class="op">=</span><span class="dv">2</span>,</span>
<span id="cb8-4"><a href="#cb8-4"></a>                                         n_redundant<span class="op">=</span><span class="dv">0</span>,</span>
<span id="cb8-5"><a href="#cb8-5"></a>                                         n_classes<span class="op">=</span><span class="dv">2</span>,</span>
<span id="cb8-6"><a href="#cb8-6"></a>                                         random_state<span class="op">=</span><span class="dv">15</span>)</span>
<span id="cb8-7"><a href="#cb8-7"></a><span class="co"># place data into df</span></span>
<span id="cb8-8"><a href="#cb8-8"></a>df <span class="op">=</span> pd.DataFrame({<span class="st">&#39;x1&#39;</span>: X[:, <span class="dv">0</span>], <span class="st">&#39;x2&#39;</span>: X[:, <span class="dv">1</span>], <span class="st">&#39;y&#39;</span>: y})</span>
<span id="cb8-9"><a href="#cb8-9"></a>df.to_csv(<span class="st">&#39;classification_demo.csv&#39;</span>, index<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb8-10"><a href="#cb8-10"></a></span>
<span id="cb8-11"><a href="#cb8-11"></a><span class="co"># instantiate the lazy data set</span></span>
<span id="cb8-12"><a href="#cb8-12"></a>csv_dataset <span class="op">=</span> CSVDataset(csv_file<span class="op">=</span><span class="st">&#39;classification_demo.csv&#39;</span>, transform<span class="op">=</span>Standardize())</span>
<span id="cb8-13"><a href="#cb8-13"></a></span>
<span id="cb8-14"><a href="#cb8-14"></a><span class="co"># check normalization unit variance values</span></span>
<span id="cb8-15"><a href="#cb8-15"></a>csv_dataset.__get_norm__()[<span class="dv">1</span>]</span></code></pre></div>
<pre><code>## array([1.39204168, 1.26812651])</code></pre>
<div class="sourceCode" id="cb10"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb10-1"><a href="#cb10-1"></a>csv_dataset.__get_norm__()[<span class="dv">0</span>]</span></code></pre></div>
<pre><code>## array([-0.02077113,  0.01676885])</code></pre>
<p>Next, we will split the data into training and test sets.</p>
<div class="sourceCode" id="cb12"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb12-1"><a href="#cb12-1"></a><span class="co"># check some data</span></span>
<span id="cb12-2"><a href="#cb12-2"></a><span class="cf">for</span> i, batch <span class="kw">in</span> <span class="bu">enumerate</span>(csv_dataset):</span>
<span id="cb12-3"><a href="#cb12-3"></a>    <span class="cf">if</span> i <span class="op">==</span> <span class="dv">0</span>:</span>
<span id="cb12-4"><a href="#cb12-4"></a>        <span class="cf">break</span></span>
<span id="cb12-5"><a href="#cb12-5"></a></span>
<span id="cb12-6"><a href="#cb12-6"></a><span class="co"># set train and test size</span></span>
<span id="cb12-7"><a href="#cb12-7"></a>train_size <span class="op">=</span> <span class="bu">int</span>(<span class="fl">0.8</span> <span class="op">*</span> <span class="bu">len</span>(csv_dataset))</span>
<span id="cb12-8"><a href="#cb12-8"></a>test_size <span class="op">=</span> <span class="bu">int</span>(<span class="fl">0.2</span> <span class="op">*</span> <span class="bu">len</span>(csv_dataset))</span>
<span id="cb12-9"><a href="#cb12-9"></a></span>
<span id="cb12-10"><a href="#cb12-10"></a><span class="co"># split data sets</span></span>
<span id="cb12-11"><a href="#cb12-11"></a>train_ds, test_ds <span class="op">=</span> torch.utils.data.random_split(csv_dataset, [train_size, test_size])</span></code></pre></div>
</div>
<div id="by-hand-logistic-regression-with-l2-regularization" class="section level1" number="3">
<h1 number="3"><span class="header-section-number">3</span> By Hand: Logistic Regression with L2 Regularization</h1>
<p>The class below finds the weights and bias terms for a logistic regression model through stochastic gradient descent. Through numerical optimization, we find our coefficients via backward propagation by:</p>
<ol style="list-style-type: decimal">
<li><p>Instantiating guesses for our weights and intercept at 0. Alternatively, we could initialize our guesses based on a distribution of our choice.</p></li>
<li><p>Matrix multiplying our features against our errors to find our new weight increment and summing our errors to find our new intercept increment.</p></li>
<li><p>Update our weights and intercept by multiplying the learning rate (<span class="math inline">\(\eta\)</span>) against our respective increments and then finally subtracting these values from the instantiated guesses for our weights and bias.</p></li>
</ol>
<p>To avoid calculating and figuring out backward propagation, PyTorch allows us to skip this process by simply wrapping class model with <code>torch.nn.Module</code> and then relying on <code>torch.optim</code> to handle backward propagation and to update our model’s weights.</p>
<div class="sourceCode" id="cb13"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb13-1"><a href="#cb13-1"></a><span class="co"># create manual logistic regression model</span></span>
<span id="cb13-2"><a href="#cb13-2"></a><span class="kw">class</span> LR():</span>
<span id="cb13-3"><a href="#cb13-3"></a>    <span class="co">&#39;&#39;&#39; Stochastic Gradient Descent Logistic Regression Model &#39;&#39;&#39;</span></span>
<span id="cb13-4"><a href="#cb13-4"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, num_features, LAMBDA<span class="op">=</span><span class="fl">0.0</span>):</span>
<span id="cb13-5"><a href="#cb13-5"></a>        <span class="co">&quot;&quot;&quot;</span></span>
<span id="cb13-6"><a href="#cb13-6"></a><span class="co">        Args:</span></span>
<span id="cb13-7"><a href="#cb13-7"></a><span class="co">            num_features (int): Number of independent variables</span></span>
<span id="cb13-8"><a href="#cb13-8"></a><span class="co">            LAMBDA (float): L2 Regularization (Ridge)</span></span>
<span id="cb13-9"><a href="#cb13-9"></a><span class="co">        &quot;&quot;&quot;</span></span>
<span id="cb13-10"><a href="#cb13-10"></a>        <span class="va">self</span>.num_features <span class="op">=</span> num_features</span>
<span id="cb13-11"><a href="#cb13-11"></a>        <span class="va">self</span>.weights <span class="op">=</span> torch.zeros(<span class="dv">1</span>, num_features,</span>
<span id="cb13-12"><a href="#cb13-12"></a>                                   dtype<span class="op">=</span>torch.float32, device<span class="op">=</span>device).normal_(<span class="fl">0.0</span>, <span class="fl">0.1</span>)</span>
<span id="cb13-13"><a href="#cb13-13"></a>        <span class="va">self</span>.bias <span class="op">=</span> torch.zeros(<span class="dv">1</span>, dtype<span class="op">=</span>torch.float32, device<span class="op">=</span>device)</span>
<span id="cb13-14"><a href="#cb13-14"></a>        <span class="va">self</span>.LAMBDA <span class="op">=</span> LAMBDA</span>
<span id="cb13-15"><a href="#cb13-15"></a></span>
<span id="cb13-16"><a href="#cb13-16"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb13-17"><a href="#cb13-17"></a>        <span class="co"># linear combination</span></span>
<span id="cb13-18"><a href="#cb13-18"></a>        linear <span class="op">=</span> torch.add(torch.mm(x, <span class="va">self</span>.weights.t()), <span class="va">self</span>.bias).view(<span class="op">-</span><span class="dv">1</span>)</span>
<span id="cb13-19"><a href="#cb13-19"></a>        <span class="co"># activation function</span></span>
<span id="cb13-20"><a href="#cb13-20"></a>        probas <span class="op">=</span> torch.sigmoid(linear)</span>
<span id="cb13-21"><a href="#cb13-21"></a>        <span class="cf">return</span> probas</span>
<span id="cb13-22"><a href="#cb13-22"></a></span>
<span id="cb13-23"><a href="#cb13-23"></a>    <span class="kw">def</span> backward(<span class="va">self</span>, x, y, probas):</span>
<span id="cb13-24"><a href="#cb13-24"></a>        <span class="co"># compute error</span></span>
<span id="cb13-25"><a href="#cb13-25"></a>        grad_loss_out <span class="op">=</span> y <span class="op">-</span> probas.view(<span class="op">-</span><span class="dv">1</span>)</span>
<span id="cb13-26"><a href="#cb13-26"></a>        <span class="co"># compute gradient loss for each weight</span></span>
<span id="cb13-27"><a href="#cb13-27"></a>        grad_loss_w <span class="op">=</span> <span class="op">-</span>torch.mm(x.t(), grad_loss_out.view(<span class="op">-</span><span class="dv">1</span>, <span class="dv">1</span>)).t()</span>
<span id="cb13-28"><a href="#cb13-28"></a>        <span class="co"># compute gradient loss for intercept</span></span>
<span id="cb13-29"><a href="#cb13-29"></a>        grad_loss_b <span class="op">=</span> <span class="op">-</span>torch.<span class="bu">sum</span>(grad_loss_out)</span>
<span id="cb13-30"><a href="#cb13-30"></a>        <span class="cf">return</span> grad_loss_w, grad_loss_b</span>
<span id="cb13-31"><a href="#cb13-31"></a></span>
<span id="cb13-32"><a href="#cb13-32"></a>    <span class="kw">def</span> _logit_cost(<span class="va">self</span>, y, probas):</span>
<span id="cb13-33"><a href="#cb13-33"></a>        tmp1 <span class="op">=</span> torch.mm(<span class="op">-</span>y.view(<span class="dv">1</span>, <span class="op">-</span><span class="dv">1</span>), torch.log(probas.view(<span class="op">-</span><span class="dv">1</span>, <span class="dv">1</span>)))</span>
<span id="cb13-34"><a href="#cb13-34"></a>        tmp2 <span class="op">=</span> torch.mm((<span class="dv">1</span> <span class="op">-</span> y).view(<span class="dv">1</span>, <span class="op">-</span><span class="dv">1</span>), torch.log(<span class="dv">1</span> <span class="op">-</span> probas.view(<span class="op">-</span><span class="dv">1</span>, <span class="dv">1</span>)))</span>
<span id="cb13-35"><a href="#cb13-35"></a>        l2 <span class="op">=</span> <span class="va">self</span>.LAMBDA <span class="op">/</span> <span class="fl">2.0</span> <span class="op">*</span> torch.<span class="bu">sum</span>(<span class="va">self</span>.weights<span class="op">**</span><span class="dv">2</span>)</span>
<span id="cb13-36"><a href="#cb13-36"></a>        <span class="cf">return</span> (tmp1 <span class="op">-</span> tmp2) <span class="op">+</span> l2</span></code></pre></div>
</div>
<div id="data-loaders" class="section level1" number="4">
<h1 number="4"><span class="header-section-number">4</span> Data Loaders</h1>
<p>Next we prepare our data loaders and check our data again.</p>
<div class="sourceCode" id="cb14"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb14-1"><a href="#cb14-1"></a><span class="co"># create DataLoaders</span></span>
<span id="cb14-2"><a href="#cb14-2"></a>train_dataloader <span class="op">=</span> DataLoader(train_ds,</span>
<span id="cb14-3"><a href="#cb14-3"></a>                              batch_size<span class="op">=</span><span class="dv">100</span>,</span>
<span id="cb14-4"><a href="#cb14-4"></a>                              sampler<span class="op">=</span><span class="va">None</span>,</span>
<span id="cb14-5"><a href="#cb14-5"></a>                              shuffle<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb14-6"><a href="#cb14-6"></a></span>
<span id="cb14-7"><a href="#cb14-7"></a>test_dataloader <span class="op">=</span> DataLoader(test_ds,</span>
<span id="cb14-8"><a href="#cb14-8"></a>                              batch_size<span class="op">=</span><span class="dv">100</span>,</span>
<span id="cb14-9"><a href="#cb14-9"></a>                              sampler<span class="op">=</span><span class="va">None</span>,</span>
<span id="cb14-10"><a href="#cb14-10"></a>                              shuffle<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb14-11"><a href="#cb14-11"></a></span>
<span id="cb14-12"><a href="#cb14-12"></a><span class="co"># check data</span></span>
<span id="cb14-13"><a href="#cb14-13"></a><span class="cf">for</span> i, batch <span class="kw">in</span> <span class="bu">enumerate</span>(train_dataloader):</span>
<span id="cb14-14"><a href="#cb14-14"></a>    <span class="cf">if</span> i <span class="op">==</span> <span class="dv">0</span>:</span>
<span id="cb14-15"><a href="#cb14-15"></a>        <span class="cf">break</span></span></code></pre></div>
</div>
<div id="training" class="section level1" number="5">
<h1 number="5"><span class="header-section-number">5</span> Training</h1>
<div class="sourceCode" id="cb15"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb15-1"><a href="#cb15-1"></a><span class="co"># set epochs</span></span>
<span id="cb15-2"><a href="#cb15-2"></a>epochs <span class="op">=</span> <span class="dv">20</span></span>
<span id="cb15-3"><a href="#cb15-3"></a></span>
<span id="cb15-4"><a href="#cb15-4"></a><span class="co"># set lr</span></span>
<span id="cb15-5"><a href="#cb15-5"></a>learning_rate <span class="op">=</span> <span class="fl">0.01</span></span>
<span id="cb15-6"><a href="#cb15-6"></a></span>
<span id="cb15-7"><a href="#cb15-7"></a><span class="co"># instantiate model</span></span>
<span id="cb15-8"><a href="#cb15-8"></a>model <span class="op">=</span> LR(num_features<span class="op">=</span><span class="dv">2</span>, LAMBDA<span class="op">=</span><span class="fl">25.0</span>)</span>
<span id="cb15-9"><a href="#cb15-9"></a></span>
<span id="cb15-10"><a href="#cb15-10"></a></span>
<span id="cb15-11"><a href="#cb15-11"></a><span class="co"># prepare training function</span></span>
<span id="cb15-12"><a href="#cb15-12"></a><span class="kw">def</span> train(dataloader):</span>
<span id="cb15-13"><a href="#cb15-13"></a>    <span class="co">#pbar = ProgressBar(n_total=len(dataloader), desc=&#39;Training&#39;)</span></span>
<span id="cb15-14"><a href="#cb15-14"></a>    train_loss <span class="op">=</span> AverageMeter()</span>
<span id="cb15-15"><a href="#cb15-15"></a>    <span class="cf">for</span> batch_idx, batch <span class="kw">in</span> <span class="bu">enumerate</span>(dataloader):</span>
<span id="cb15-16"><a href="#cb15-16"></a>        <span class="co"># forward</span></span>
<span id="cb15-17"><a href="#cb15-17"></a>        probas <span class="op">=</span> model.forward(batch[<span class="st">&#39;features&#39;</span>])</span>
<span id="cb15-18"><a href="#cb15-18"></a>        <span class="co"># backward</span></span>
<span id="cb15-19"><a href="#cb15-19"></a>        grad_w, grad_b <span class="op">=</span> model.backward(batch[<span class="st">&#39;features&#39;</span>],</span>
<span id="cb15-20"><a href="#cb15-20"></a>                                        batch[<span class="st">&#39;target&#39;</span>],</span>
<span id="cb15-21"><a href="#cb15-21"></a>                                        probas)</span>
<span id="cb15-22"><a href="#cb15-22"></a>        <span class="co"># manual regularization -- account for mini-batches</span></span>
<span id="cb15-23"><a href="#cb15-23"></a>        l2_reg <span class="op">=</span> model.LAMBDA <span class="op">*</span> model.weights <span class="op">/</span> <span class="bu">len</span>(dataloader)</span>
<span id="cb15-24"><a href="#cb15-24"></a>        <span class="co"># update weights</span></span>
<span id="cb15-25"><a href="#cb15-25"></a>        model.weights <span class="op">-=</span> learning_rate <span class="op">*</span> (grad_w <span class="op">+</span> l2_reg)</span>
<span id="cb15-26"><a href="#cb15-26"></a>        model.bias <span class="op">-=</span> learning_rate <span class="op">*</span> grad_b</span>
<span id="cb15-27"><a href="#cb15-27"></a>        <span class="co"># record loss</span></span>
<span id="cb15-28"><a href="#cb15-28"></a>        loss <span class="op">=</span> model._logit_cost(batch[<span class="st">&#39;target&#39;</span>], probas)</span>
<span id="cb15-29"><a href="#cb15-29"></a>        <span class="co"># update meter</span></span>
<span id="cb15-30"><a href="#cb15-30"></a>        train_loss.update(loss.item(), n<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb15-31"><a href="#cb15-31"></a>        <span class="co"># update progress bar</span></span>
<span id="cb15-32"><a href="#cb15-32"></a>        <span class="co">#pbar(step=batch_idx, info={&#39;batch_loss&#39;: loss.item()})</span></span>
<span id="cb15-33"><a href="#cb15-33"></a>    <span class="cf">return</span> {<span class="st">&#39;train_loss&#39;</span>: train_loss.avg}</span>
<span id="cb15-34"><a href="#cb15-34"></a></span>
<span id="cb15-35"><a href="#cb15-35"></a></span>
<span id="cb15-36"><a href="#cb15-36"></a><span class="co"># training</span></span>
<span id="cb15-37"><a href="#cb15-37"></a><span class="cf">for</span> epoch <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">1</span>, epochs <span class="op">+</span> <span class="dv">1</span>):</span>
<span id="cb15-38"><a href="#cb15-38"></a>    train_log <span class="op">=</span> train(train_dataloader)</span>
<span id="cb15-39"><a href="#cb15-39"></a>    logs <span class="op">=</span> <span class="bu">dict</span>(train_log)</span>
<span id="cb15-40"><a href="#cb15-40"></a>    train_logs <span class="op">=</span> <span class="ss">f&#39;</span><span class="ch">\n</span><span class="ss">Epoch: </span><span class="sc">{</span>epoch<span class="sc">}</span><span class="ss"> - &#39;</span> <span class="op">+</span> <span class="st">&quot;-&quot;</span>.join([<span class="ss">f&#39; </span><span class="sc">{</span>key<span class="sc">}</span><span class="ss">: </span><span class="sc">{</span>value<span class="sc">:.4f}</span><span class="ss"> &#39;</span> <span class="cf">for</span> key, value <span class="kw">in</span> logs.items()])</span>
<span id="cb15-41"><a href="#cb15-41"></a>    <span class="bu">print</span>(train_logs)</span></code></pre></div>
<pre><code>## 
## Epoch: 1 -  train_loss: 61.5763 
## 
## Epoch: 2 -  train_loss: 68.8976 
## 
## Epoch: 3 -  train_loss: 75.1295 
## 
## Epoch: 4 -  train_loss: 79.3965 
## 
## Epoch: 5 -  train_loss: 80.2331 
## 
## Epoch: 6 -  train_loss: 81.3295 
## 
## Epoch: 7 -  train_loss: 81.1134 
## 
## Epoch: 8 -  train_loss: 82.3650 
## 
## Epoch: 9 -  train_loss: 81.3603 
## 
## Epoch: 10 -  train_loss: 81.6685 
## 
## Epoch: 11 -  train_loss: 80.6160 
## 
## Epoch: 12 -  train_loss: 81.1265 
## 
## Epoch: 13 -  train_loss: 82.3104 
## 
## Epoch: 14 -  train_loss: 81.0290 
## 
## Epoch: 15 -  train_loss: 82.2829 
## 
## Epoch: 16 -  train_loss: 81.7126 
## 
## Epoch: 17 -  train_loss: 81.8430 
## 
## Epoch: 18 -  train_loss: 81.8037 
## 
## Epoch: 19 -  train_loss: 81.6940 
## 
## Epoch: 20 -  train_loss: 80.5625</code></pre>
<div id="training-coefficients" class="section level2" number="5.1">
<h2 number="5.1"><span class="header-section-number">5.1</span> Training Coefficients</h2>
<div class="sourceCode" id="cb17"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb17-1"><a href="#cb17-1"></a><span class="bu">print</span>(<span class="st">&#39;Weights&#39;</span>, model.weights)</span></code></pre></div>
<pre><code>## Weights tensor([[-0.0359,  2.0143]], device=&#39;cuda:0&#39;)</code></pre>
<div class="sourceCode" id="cb19"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb19-1"><a href="#cb19-1"></a><span class="bu">print</span>(<span class="st">&#39;Bias&#39;</span>, model.bias)</span></code></pre></div>
<pre><code>## Bias tensor([0.1083], device=&#39;cuda:0&#39;)</code></pre>
</div>
</div>
<div id="testing" class="section level1" number="6">
<h1 number="6"><span class="header-section-number">6</span> Testing</h1>
<div class="sourceCode" id="cb21"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb21-1"><a href="#cb21-1"></a><span class="co"># valid/test function</span></span>
<span id="cb21-2"><a href="#cb21-2"></a><span class="kw">def</span> test(dataloader):</span>
<span id="cb21-3"><a href="#cb21-3"></a>    <span class="co">#pbar = ProgressBar(n_total=len(dataloader), desc=&#39;Testing&#39;)</span></span>
<span id="cb21-4"><a href="#cb21-4"></a>    valid_loss <span class="op">=</span> AverageMeter()</span>
<span id="cb21-5"><a href="#cb21-5"></a>    valid_acc <span class="op">=</span> AverageMeter()</span>
<span id="cb21-6"><a href="#cb21-6"></a>    count <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb21-7"><a href="#cb21-7"></a>    <span class="cf">for</span> batch_idx, batch <span class="kw">in</span> <span class="bu">enumerate</span>(dataloader):</span>
<span id="cb21-8"><a href="#cb21-8"></a>        <span class="co"># forward -- skip backward prop</span></span>
<span id="cb21-9"><a href="#cb21-9"></a>        probas <span class="op">=</span> model.forward(batch[<span class="st">&#39;features&#39;</span>])</span>
<span id="cb21-10"><a href="#cb21-10"></a>        <span class="co"># record loss</span></span>
<span id="cb21-11"><a href="#cb21-11"></a>        loss <span class="op">=</span> model._logit_cost(batch[<span class="st">&#39;target&#39;</span>], probas)</span>
<span id="cb21-12"><a href="#cb21-12"></a>        <span class="co"># get predictions</span></span>
<span id="cb21-13"><a href="#cb21-13"></a>        prediction <span class="op">=</span> torch.where(probas <span class="op">&gt;</span> <span class="fl">0.5</span>, torch.tensor(<span class="dv">1</span>, device<span class="op">=</span>device), torch.tensor(<span class="dv">0</span>, device<span class="op">=</span>device)).view(<span class="op">-</span><span class="dv">1</span>)</span>
<span id="cb21-14"><a href="#cb21-14"></a>        <span class="co"># compare</span></span>
<span id="cb21-15"><a href="#cb21-15"></a>        correct <span class="op">=</span> prediction.eq(batch[<span class="st">&#39;target&#39;</span>]).<span class="bu">sum</span>().item()</span>
<span id="cb21-16"><a href="#cb21-16"></a>        valid_loss.update(loss, n<span class="op">=</span>batch[<span class="st">&#39;features&#39;</span>].size(<span class="dv">0</span>))</span>
<span id="cb21-17"><a href="#cb21-17"></a>        valid_acc.update(correct, n<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb21-18"><a href="#cb21-18"></a>        count <span class="op">+=</span> batch[<span class="st">&#39;features&#39;</span>].size(<span class="dv">0</span>)</span>
<span id="cb21-19"><a href="#cb21-19"></a>        <span class="co">#pbar(step=batch_idx)</span></span>
<span id="cb21-20"><a href="#cb21-20"></a>    <span class="cf">return</span> {<span class="st">&#39;valid_loss&#39;</span>: valid_loss.avg,</span>
<span id="cb21-21"><a href="#cb21-21"></a>            <span class="st">&#39;valid_acc&#39;</span>: valid_acc.<span class="bu">sum</span> <span class="op">/</span> count}</span></code></pre></div>
<div id="testing-coefficients" class="section level2" number="6.1">
<h2 number="6.1"><span class="header-section-number">6.1</span> Testing Coefficients</h2>
<div class="sourceCode" id="cb22"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb22-1"><a href="#cb22-1"></a><span class="co"># testing</span></span>
<span id="cb22-2"><a href="#cb22-2"></a>test_log <span class="op">=</span> test(test_dataloader)</span>
<span id="cb22-3"><a href="#cb22-3"></a><span class="bu">print</span>(test_log)</span></code></pre></div>
<pre><code>## {&#39;valid_loss&#39;: tensor([[77.3259]], device=&#39;cuda:0&#39;), &#39;valid_acc&#39;: 0.955}</code></pre>
</div>
</div>
<div id="verification" class="section level1" number="7">
<h1 number="7"><span class="header-section-number">7</span> Verification</h1>
<p>We can see that our work above maps closely an sklearn implementation that uses all of the data for training.</p>
<div class="sourceCode" id="cb24"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb24-1"><a href="#cb24-1"></a><span class="co">## sklearn verify</span></span>
<span id="cb24-2"><a href="#cb24-2"></a><span class="co"># C = inverse of lambda</span></span>
<span id="cb24-3"><a href="#cb24-3"></a>LAMBDA <span class="op">=</span> <span class="fl">25.0</span></span>
<span id="cb24-4"><a href="#cb24-4"></a>C <span class="op">=</span> <span class="dv">1</span> <span class="op">/</span> LAMBDA</span>
<span id="cb24-5"><a href="#cb24-5"></a>clf <span class="op">=</span> LogisticRegression(solver<span class="op">=</span><span class="st">&#39;lbfgs&#39;</span>, penalty<span class="op">=</span><span class="st">&#39;l2&#39;</span>, C<span class="op">=</span>C).fit(X, y)</span>
<span id="cb24-6"><a href="#cb24-6"></a><span class="bu">print</span>(clf.coef_)</span></code></pre></div>
<pre><code>## [[-0.03849518  2.00255362]]</code></pre>
</div>
<div id="by-hand-linear-regression-with-l2-regularization" class="section level1" number="8">
<h1 number="8"><span class="header-section-number">8</span> By Hand: Linear Regression with L2 Regularization</h1>
<p>In a fashion very similar to logistic regression, the class below finds the weights and bias terms for a linear regression model through stochastic gradient descent.</p>
<div id="prepare-data" class="section level2" number="8.1">
<h2 number="8.1"><span class="header-section-number">8.1</span> Prepare Data</h2>
<div class="sourceCode" id="cb26"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb26-1"><a href="#cb26-1"></a>df2 <span class="op">=</span> pd.read_csv(<span class="st">&#39;https://raw.githubusercontent.com/rasbt/stat479-deep-learning-ss19/master/L05_grad-descent/code/datasets/linreg-data.csv&#39;</span>, index_col<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb26-2"><a href="#cb26-2"></a>X <span class="op">=</span> df[[<span class="st">&#39;x1&#39;</span>, <span class="st">&#39;x2&#39;</span>]].values</span>
<span id="cb26-3"><a href="#cb26-3"></a>y <span class="op">=</span> df[<span class="st">&#39;y&#39;</span>].values</span>
<span id="cb26-4"><a href="#cb26-4"></a>df2 <span class="op">=</span> pd.DataFrame({<span class="st">&#39;x1&#39;</span>: X[:, <span class="dv">0</span>], <span class="st">&#39;x2&#39;</span>: X[:, <span class="dv">1</span>], <span class="st">&#39;y&#39;</span>: y})</span>
<span id="cb26-5"><a href="#cb26-5"></a></span>
<span id="cb26-6"><a href="#cb26-6"></a><span class="co"># save df</span></span>
<span id="cb26-7"><a href="#cb26-7"></a>df2.to_csv(<span class="st">&#39;regression_demo.csv&#39;</span>, index<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb26-8"><a href="#cb26-8"></a></span>
<span id="cb26-9"><a href="#cb26-9"></a></span>
<span id="cb26-10"><a href="#cb26-10"></a><span class="kw">class</span> ToTensor():</span>
<span id="cb26-11"><a href="#cb26-11"></a>    <span class="co"># retrieve sample and unpack it</span></span>
<span id="cb26-12"><a href="#cb26-12"></a>    <span class="kw">def</span> <span class="fu">__call__</span>(<span class="va">self</span>, sample):</span>
<span id="cb26-13"><a href="#cb26-13"></a>        features, target, idx <span class="op">=</span> (sample[<span class="st">&#39;features&#39;</span>],</span>
<span id="cb26-14"><a href="#cb26-14"></a>                              sample[<span class="st">&#39;target&#39;</span>],</span>
<span id="cb26-15"><a href="#cb26-15"></a>                              sample[<span class="st">&#39;idx&#39;</span>])</span>
<span id="cb26-16"><a href="#cb26-16"></a></span>
<span id="cb26-17"><a href="#cb26-17"></a>        <span class="co"># yield another dict</span></span>
<span id="cb26-18"><a href="#cb26-18"></a>        <span class="cf">return</span> {<span class="st">&#39;features&#39;</span>: torch.as_tensor(features,</span>
<span id="cb26-19"><a href="#cb26-19"></a>                                         dtype<span class="op">=</span>torch.float32,</span>
<span id="cb26-20"><a href="#cb26-20"></a>                                         device<span class="op">=</span>device),</span>
<span id="cb26-21"><a href="#cb26-21"></a>                <span class="st">&#39;target&#39;</span>: torch.as_tensor(target,</span>
<span id="cb26-22"><a href="#cb26-22"></a>                                          dtype<span class="op">=</span>torch.float32,</span>
<span id="cb26-23"><a href="#cb26-23"></a>                                          device<span class="op">=</span>device),</span>
<span id="cb26-24"><a href="#cb26-24"></a>                <span class="st">&#39;idx&#39;</span>: torch.as_tensor(idx,</span>
<span id="cb26-25"><a href="#cb26-25"></a>                                       dtype<span class="op">=</span>torch.<span class="bu">int</span>,</span>
<span id="cb26-26"><a href="#cb26-26"></a>                                       device<span class="op">=</span>device)}</span>
<span id="cb26-27"><a href="#cb26-27"></a></span>
<span id="cb26-28"><a href="#cb26-28"></a><span class="co"># instantiate the lazy data set</span></span>
<span id="cb26-29"><a href="#cb26-29"></a>csv_dataset2 <span class="op">=</span> CSVDataset(csv_file<span class="op">=</span><span class="st">&#39;regression_demo.csv&#39;</span>, transform<span class="op">=</span>ToTensor())</span></code></pre></div>
</div>
<div id="linear-model" class="section level2" number="8.2">
<h2 number="8.2"><span class="header-section-number">8.2</span> Linear Model</h2>
<p>The linear model is created below. Notice that <code>forward</code> models a linear relationship of the weights with respect to the output. <code>Backward</code> follows the general premise of backward propagation taking place in the following steps:</p>
<ol style="list-style-type: decimal">
<li><p>Take the results of forward propagation and compute <span class="math inline">\(\hat{Y}\)</span></p></li>
<li><p>Compute the loss <span class="math inline">\(Y\)</span>-<span class="math inline">\(\hat{Y}\)</span> to get the gradient of the loss with respect to each weight using the chain rule.</p></li>
</ol>
<p>Outside of the <code>backward</code> function, we multiply the results of step 2 by the learning rate and update each weight.</p>
<div class="sourceCode" id="cb27"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb27-1"><a href="#cb27-1"></a><span class="co"># create manual ols model</span></span>
<span id="cb27-2"><a href="#cb27-2"></a><span class="kw">class</span> LM():</span>
<span id="cb27-3"><a href="#cb27-3"></a>    <span class="co">&#39;&#39;&#39; Stochastic Gradient Descent Linear Model &#39;&#39;&#39;</span></span>
<span id="cb27-4"><a href="#cb27-4"></a></span>
<span id="cb27-5"><a href="#cb27-5"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, num_features, LAMBDA<span class="op">=</span><span class="fl">0.0</span>):</span>
<span id="cb27-6"><a href="#cb27-6"></a>        <span class="co">&quot;&quot;&quot;</span></span>
<span id="cb27-7"><a href="#cb27-7"></a><span class="co">        Args:</span></span>
<span id="cb27-8"><a href="#cb27-8"></a><span class="co">            num_features (int): Number of independent variables</span></span>
<span id="cb27-9"><a href="#cb27-9"></a><span class="co">            LAMBDA (float): L2 Regularization (Ridge)</span></span>
<span id="cb27-10"><a href="#cb27-10"></a><span class="co">        &quot;&quot;&quot;</span></span>
<span id="cb27-11"><a href="#cb27-11"></a>        <span class="co"># set num. dimensions</span></span>
<span id="cb27-12"><a href="#cb27-12"></a>        <span class="va">self</span>.num_features <span class="op">=</span> num_features</span>
<span id="cb27-13"><a href="#cb27-13"></a>        <span class="co"># initialize weights as zeros</span></span>
<span id="cb27-14"><a href="#cb27-14"></a>        <span class="va">self</span>.weights <span class="op">=</span> torch.zeros(<span class="dv">1</span>, num_features,</span>
<span id="cb27-15"><a href="#cb27-15"></a>                                   dtype<span class="op">=</span>torch.float32, device<span class="op">=</span>device).normal_(<span class="fl">0.0</span>, <span class="fl">0.1</span>)</span>
<span id="cb27-16"><a href="#cb27-16"></a>        <span class="co"># initialize bias as zeros</span></span>
<span id="cb27-17"><a href="#cb27-17"></a>        <span class="va">self</span>.bias <span class="op">=</span> torch.zeros(<span class="dv">1</span>, dtype<span class="op">=</span>torch.float32, device<span class="op">=</span>device)</span>
<span id="cb27-18"><a href="#cb27-18"></a>        <span class="co"># initialize LAMBDA</span></span>
<span id="cb27-19"><a href="#cb27-19"></a>        <span class="va">self</span>.LAMBDA <span class="op">=</span> LAMBDA</span>
<span id="cb27-20"><a href="#cb27-20"></a></span>
<span id="cb27-21"><a href="#cb27-21"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb27-22"><a href="#cb27-22"></a>        <span class="co"># linear combination</span></span>
<span id="cb27-23"><a href="#cb27-23"></a>        linear <span class="op">=</span> torch.add(torch.mm(x, <span class="va">self</span>.weights.t()), <span class="va">self</span>.bias).view(<span class="op">-</span><span class="dv">1</span>)</span>
<span id="cb27-24"><a href="#cb27-24"></a>        <span class="co"># activation = identity(x) = x</span></span>
<span id="cb27-25"><a href="#cb27-25"></a>        <span class="cf">pass</span>  <span class="co"># do nothing for activation</span></span>
<span id="cb27-26"><a href="#cb27-26"></a>        <span class="cf">return</span> linear</span>
<span id="cb27-27"><a href="#cb27-27"></a></span>
<span id="cb27-28"><a href="#cb27-28"></a>    <span class="kw">def</span> backward(<span class="va">self</span>, x, y, y_hat):</span>
<span id="cb27-29"><a href="#cb27-29"></a>        <span class="co"># find gradient loss</span></span>
<span id="cb27-30"><a href="#cb27-30"></a>        grad_loss_out <span class="op">=</span> y <span class="op">-</span> y_hat.view(<span class="op">-</span><span class="dv">1</span>)</span>
<span id="cb27-31"><a href="#cb27-31"></a>        <span class="co"># chain rule: find loss for weights</span></span>
<span id="cb27-32"><a href="#cb27-32"></a>        grad_loss_w <span class="op">=</span> <span class="dv">2</span> <span class="op">*</span> <span class="op">-</span>torch.mm(x.t(), grad_loss_out.view(<span class="op">-</span><span class="dv">1</span>, <span class="dv">1</span>)) <span class="op">/</span> y.size(<span class="dv">0</span>)</span>
<span id="cb27-33"><a href="#cb27-33"></a>        <span class="co"># chain rule: find loss for bias</span></span>
<span id="cb27-34"><a href="#cb27-34"></a>        grad_loss_b <span class="op">=</span> <span class="dv">2</span> <span class="op">*</span> <span class="op">-</span>torch.<span class="bu">sum</span>(grad_loss_out) <span class="op">/</span> y.size(<span class="dv">0</span>)</span>
<span id="cb27-35"><a href="#cb27-35"></a>        <span class="cf">return</span> grad_loss_w, grad_loss_b</span>
<span id="cb27-36"><a href="#cb27-36"></a></span>
<span id="cb27-37"><a href="#cb27-37"></a>    <span class="kw">def</span> loss(<span class="va">self</span>, y_hat, y):</span>
<span id="cb27-38"><a href="#cb27-38"></a>        <span class="co"># mean squared error</span></span>
<span id="cb27-39"><a href="#cb27-39"></a>        <span class="cf">return</span> torch.mean((y_hat <span class="op">-</span> y)<span class="op">**</span><span class="dv">2</span>)</span></code></pre></div>
<div class="sourceCode" id="cb28"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb28-1"><a href="#cb28-1"></a><span class="co"># create DataLoader</span></span>
<span id="cb28-2"><a href="#cb28-2"></a>train_dataloader <span class="op">=</span> DataLoader(csv_dataset2,</span>
<span id="cb28-3"><a href="#cb28-3"></a>                              batch_size<span class="op">=</span><span class="dv">100</span>,</span>
<span id="cb28-4"><a href="#cb28-4"></a>                              sampler<span class="op">=</span><span class="va">None</span>,</span>
<span id="cb28-5"><a href="#cb28-5"></a>                              shuffle<span class="op">=</span><span class="va">True</span>)</span></code></pre></div>
<div class="sourceCode" id="cb29"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb29-1"><a href="#cb29-1"></a><span class="co"># set epochs</span></span>
<span id="cb29-2"><a href="#cb29-2"></a>epochs <span class="op">=</span> <span class="dv">40</span></span>
<span id="cb29-3"><a href="#cb29-3"></a></span>
<span id="cb29-4"><a href="#cb29-4"></a><span class="co"># set lr</span></span>
<span id="cb29-5"><a href="#cb29-5"></a>learning_rate <span class="op">=</span> <span class="fl">0.05</span></span>
<span id="cb29-6"><a href="#cb29-6"></a></span>
<span id="cb29-7"><a href="#cb29-7"></a><span class="co"># instantiate model</span></span>
<span id="cb29-8"><a href="#cb29-8"></a>model <span class="op">=</span> LM(num_features<span class="op">=</span><span class="dv">2</span>, LAMBDA<span class="op">=</span><span class="fl">10.0</span>)</span></code></pre></div>
<div class="sourceCode" id="cb30"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb30-1"><a href="#cb30-1"></a><span class="co"># prepare training function</span></span>
<span id="cb30-2"><a href="#cb30-2"></a><span class="kw">def</span> train(dataloader):</span>
<span id="cb30-3"><a href="#cb30-3"></a>    <span class="co">#pbar = ProgressBar(n_total=len(dataloader), desc=&#39;Training&#39;)</span></span>
<span id="cb30-4"><a href="#cb30-4"></a>    train_loss <span class="op">=</span> AverageMeter()</span>
<span id="cb30-5"><a href="#cb30-5"></a>    <span class="cf">for</span> batch_idx, batch <span class="kw">in</span> <span class="bu">enumerate</span>(dataloader):</span>
<span id="cb30-6"><a href="#cb30-6"></a>        <span class="co"># forward</span></span>
<span id="cb30-7"><a href="#cb30-7"></a>        y_hat <span class="op">=</span> model.forward(batch[<span class="st">&#39;features&#39;</span>].<span class="bu">float</span>())</span>
<span id="cb30-8"><a href="#cb30-8"></a>        <span class="co"># backward</span></span>
<span id="cb30-9"><a href="#cb30-9"></a>        grad_w, grad_b <span class="op">=</span> model.backward(batch[<span class="st">&#39;features&#39;</span>],</span>
<span id="cb30-10"><a href="#cb30-10"></a>                                        batch[<span class="st">&#39;target&#39;</span>],</span>
<span id="cb30-11"><a href="#cb30-11"></a>                                        y_hat)</span>
<span id="cb30-12"><a href="#cb30-12"></a>        <span class="co"># manual regularization</span></span>
<span id="cb30-13"><a href="#cb30-13"></a>        l2_reg <span class="op">=</span> (model.LAMBDA <span class="op">*</span> model.weights)</span>
<span id="cb30-14"><a href="#cb30-14"></a>        l2_reg <span class="op">=</span> l2_reg.reshape(<span class="dv">2</span>, <span class="dv">1</span>)</span>
<span id="cb30-15"><a href="#cb30-15"></a>        <span class="co"># update weights</span></span>
<span id="cb30-16"><a href="#cb30-16"></a>        model.weights <span class="op">-=</span> learning_rate <span class="op">*</span> (grad_w <span class="op">+</span> l2_reg).view(<span class="op">-</span><span class="dv">1</span>)</span>
<span id="cb30-17"><a href="#cb30-17"></a>        model.bias <span class="op">-=</span> (learning_rate <span class="op">*</span> grad_b).view(<span class="op">-</span><span class="dv">1</span>)</span>
<span id="cb30-18"><a href="#cb30-18"></a>        <span class="co"># record loss</span></span>
<span id="cb30-19"><a href="#cb30-19"></a>        loss <span class="op">=</span> model.loss(batch[<span class="st">&#39;target&#39;</span>], y_hat)</span>
<span id="cb30-20"><a href="#cb30-20"></a>        <span class="co"># update meter</span></span>
<span id="cb30-21"><a href="#cb30-21"></a>        train_loss.update(loss.item(), n<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb30-22"><a href="#cb30-22"></a>        <span class="co"># update progress bar</span></span>
<span id="cb30-23"><a href="#cb30-23"></a>        <span class="co">#pbar(step=batch_idx, info={&#39;batch_loss&#39;: loss.item()})</span></span>
<span id="cb30-24"><a href="#cb30-24"></a>    <span class="cf">return</span> {<span class="st">&#39;train_loss&#39;</span>: train_loss.avg}</span>
<span id="cb30-25"><a href="#cb30-25"></a></span>
<span id="cb30-26"><a href="#cb30-26"></a></span>
<span id="cb30-27"><a href="#cb30-27"></a><span class="co"># training</span></span>
<span id="cb30-28"><a href="#cb30-28"></a><span class="cf">for</span> epoch <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">1</span>, epochs <span class="op">+</span> <span class="dv">1</span>):</span>
<span id="cb30-29"><a href="#cb30-29"></a>    train_log <span class="op">=</span> train(train_dataloader)</span>
<span id="cb30-30"><a href="#cb30-30"></a>    logs <span class="op">=</span> <span class="bu">dict</span>(train_log)</span>
<span id="cb30-31"><a href="#cb30-31"></a>    train_logs <span class="op">=</span> <span class="ss">f&#39;</span><span class="ch">\n</span><span class="ss">Epoch: </span><span class="sc">{</span>epoch<span class="sc">}</span><span class="ss"> - &#39;</span> <span class="op">+</span> <span class="st">&quot;-&quot;</span>.join([<span class="ss">f&#39; </span><span class="sc">{</span>key<span class="sc">}</span><span class="ss">: </span><span class="sc">{</span>value<span class="sc">:.4f}</span><span class="ss"> &#39;</span> <span class="cf">for</span> key, value <span class="kw">in</span> logs.items()])</span>
<span id="cb30-32"><a href="#cb30-32"></a>    <span class="bu">print</span>(train_logs)</span></code></pre></div>
<pre><code>## 
## Epoch: 1 -  train_loss: 0.2927 
## 
## Epoch: 2 -  train_loss: 0.1986 
## 
## Epoch: 3 -  train_loss: 0.1871 
## 
## Epoch: 4 -  train_loss: 0.1845 
## 
## Epoch: 5 -  train_loss: 0.1851 
## 
## Epoch: 6 -  train_loss: 0.1849 
## 
## Epoch: 7 -  train_loss: 0.1848 
## 
## Epoch: 8 -  train_loss: 0.1849 
## 
## Epoch: 9 -  train_loss: 0.1854 
## 
## Epoch: 10 -  train_loss: 0.1844 
## 
## Epoch: 11 -  train_loss: 0.1851 
## 
## Epoch: 12 -  train_loss: 0.1844 
## 
## Epoch: 13 -  train_loss: 0.1848 
## 
## Epoch: 14 -  train_loss: 0.1845 
## 
## Epoch: 15 -  train_loss: 0.1851 
## 
## Epoch: 16 -  train_loss: 0.1850 
## 
## Epoch: 17 -  train_loss: 0.1844 
## 
## Epoch: 18 -  train_loss: 0.1854 
## 
## Epoch: 19 -  train_loss: 0.1849 
## 
## Epoch: 20 -  train_loss: 0.1849 
## 
## Epoch: 21 -  train_loss: 0.1843 
## 
## Epoch: 22 -  train_loss: 0.1845 
## 
## Epoch: 23 -  train_loss: 0.1853 
## 
## Epoch: 24 -  train_loss: 0.1853 
## 
## Epoch: 25 -  train_loss: 0.1845 
## 
## Epoch: 26 -  train_loss: 0.1847 
## 
## Epoch: 27 -  train_loss: 0.1851 
## 
## Epoch: 28 -  train_loss: 0.1854 
## 
## Epoch: 29 -  train_loss: 0.1843 
## 
## Epoch: 30 -  train_loss: 0.1846 
## 
## Epoch: 31 -  train_loss: 0.1854 
## 
## Epoch: 32 -  train_loss: 0.1839 
## 
## Epoch: 33 -  train_loss: 0.1856 
## 
## Epoch: 34 -  train_loss: 0.1843 
## 
## Epoch: 35 -  train_loss: 0.1855 
## 
## Epoch: 36 -  train_loss: 0.1848 
## 
## Epoch: 37 -  train_loss: 0.1849 
## 
## Epoch: 38 -  train_loss: 0.1848 
## 
## Epoch: 39 -  train_loss: 0.1849 
## 
## Epoch: 40 -  train_loss: 0.1848</code></pre>
</div>
<div id="check-output" class="section level2" number="8.3">
<h2 number="8.3"><span class="header-section-number">8.3</span> Check Output</h2>
<div class="sourceCode" id="cb32"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb32-1"><a href="#cb32-1"></a><span class="bu">print</span>(<span class="st">&#39;Weights&#39;</span>, model.weights)</span></code></pre></div>
<pre><code>## Weights tensor([[0.0011, 0.0749]], device=&#39;cuda:0&#39;)</code></pre>
<div class="sourceCode" id="cb34"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb34-1"><a href="#cb34-1"></a><span class="bu">print</span>(<span class="st">&#39;Bias&#39;</span>, model.bias)</span></code></pre></div>
<pre><code>## Bias tensor([0.4993], device=&#39;cuda:0&#39;)</code></pre>
</div>
<div id="compare-with-sklearn" class="section level2" number="8.4">
<h2 number="8.4"><span class="header-section-number">8.4</span> Compare with Sklearn</h2>
<div class="sourceCode" id="cb36"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb36-1"><a href="#cb36-1"></a><span class="co"># sklearn ridge</span></span>
<span id="cb36-2"><a href="#cb36-2"></a><span class="im">from</span> sklearn.linear_model <span class="im">import</span> Ridge</span>
<span id="cb36-3"><a href="#cb36-3"></a>LAMBDA <span class="op">=</span> <span class="fl">10.0</span></span>
<span id="cb36-4"><a href="#cb36-4"></a><span class="co"># C = inverse of lambda</span></span>
<span id="cb36-5"><a href="#cb36-5"></a>C <span class="op">=</span> <span class="dv">1</span><span class="op">/</span>LAMBDA</span>
<span id="cb36-6"><a href="#cb36-6"></a><span class="co"># alpha = 1 / (2C)</span></span>
<span id="cb36-7"><a href="#cb36-7"></a>alpha <span class="op">=</span> <span class="dv">1</span> <span class="op">/</span> (<span class="dv">2</span><span class="op">*</span>C)</span>
<span id="cb36-8"><a href="#cb36-8"></a>x <span class="op">=</span> np.ascontiguousarray(X)</span>
<span id="cb36-9"><a href="#cb36-9"></a><span class="co"># alpha * N obs for ridge</span></span>
<span id="cb36-10"><a href="#cb36-10"></a>ridge <span class="op">=</span> Ridge(alpha<span class="op">=</span>alpha<span class="op">*</span><span class="dv">1000</span>, solver<span class="op">=</span><span class="st">&#39;sag&#39;</span>).fit(x, y)</span>
<span id="cb36-11"><a href="#cb36-11"></a><span class="bu">print</span>(ridge.coef_)</span></code></pre></div>
<pre><code>## [0.00110112 0.07494251]</code></pre>
<div class="sourceCode" id="cb38"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb38-1"><a href="#cb38-1"></a>ridge.intercept_</span></code></pre></div>
<pre><code>## 0.49776617212765734</code></pre>
</div>
</div>



<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>
