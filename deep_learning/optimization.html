<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml">

<head>

<meta charset="utf-8" />
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="generator" content="pandoc" />

<meta name="viewport" content="width=device-width, initial-scale=1">

<meta name="author" content="Andrew Fogarty" />

<meta name="date" content="2020-10-14" />

<title>Optimization</title>



<style type="text/css">code{white-space: pre;}</style>
<style type="text/css" data-origin="pandoc">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */

</style>
<script>
// apply pandoc div.sourceCode style to pre.sourceCode instead
(function() {
  var sheets = document.styleSheets;
  for (var i = 0; i < sheets.length; i++) {
    if (sheets[i].ownerNode.dataset["origin"] !== "pandoc") continue;
    try { var rules = sheets[i].cssRules; } catch (e) { continue; }
    for (var j = 0; j < rules.length; j++) {
      var rule = rules[j];
      // check if there is a div.sourceCode rule
      if (rule.type !== rule.STYLE_RULE || rule.selectorText !== "div.sourceCode") continue;
      var style = rule.style.cssText;
      // check if color or background-color is set
      if (rule.style.color === '' || rule.style.backgroundColor === '') continue;
      // replace div.sourceCode by a pre.sourceCode rule
      sheets[i].deleteRule(j);
      sheets[i].insertRule('pre.sourceCode{' + style + '}', j);
    }
  }
})();
</script>



<style type="text/css">body {
background-color: #fff;
margin: 1em auto;
max-width: 700px;
overflow: visible;
padding-left: 2em;
padding-right: 2em;
font-family: "Open Sans", "Helvetica Neue", Helvetica, Arial, sans-serif;
font-size: 14px;
line-height: 1.35;
}
#header {
text-align: center;
}
#TOC {
clear: both;
margin: 0 0 10px 10px;
padding: 4px;
width: 400px;
border: 1px solid #CCCCCC;
border-radius: 5px;
background-color: #f6f6f6;
font-size: 13px;
line-height: 1.3;
}
#TOC .toctitle {
font-weight: bold;
font-size: 15px;
margin-left: 5px;
}
#TOC ul {
padding-left: 40px;
margin-left: -1.5em;
margin-top: 5px;
margin-bottom: 5px;
}
#TOC ul ul {
margin-left: -2em;
}
#TOC li {
line-height: 16px;
}
table {
margin: 1em auto;
border-width: 1px;
border-color: #DDDDDD;
border-style: outset;
border-collapse: collapse;
}
table th {
border-width: 2px;
padding: 5px;
border-style: inset;
}
table td {
border-width: 1px;
border-style: inset;
line-height: 18px;
padding: 5px 5px;
}
table, table th, table td {
border-left-style: none;
border-right-style: none;
}
table thead, table tr.even {
background-color: #f7f7f7;
}
p {
margin: 0.5em 0;
}
blockquote {
background-color: #f6f6f6;
padding: 0.25em 0.75em;
}
hr {
border-style: solid;
border: none;
border-top: 1px solid #777;
margin: 28px 0;
}
dl {
margin-left: 0;
}
dl dd {
margin-bottom: 13px;
margin-left: 13px;
}
dl dt {
font-weight: bold;
}
ul {
margin-top: 0;
}
ul li {
list-style: circle outside;
}
ul ul {
margin-bottom: 0;
}
pre, code {
background-color: #f7f7f7;
border-radius: 3px;
color: #333;
white-space: pre-wrap; 
}
pre {
border-radius: 3px;
margin: 5px 0px 10px 0px;
padding: 10px;
}
pre:not([class]) {
background-color: #f7f7f7;
}
code {
font-family: Consolas, Monaco, 'Courier New', monospace;
font-size: 85%;
}
p > code, li > code {
padding: 2px 0px;
}
div.figure {
text-align: center;
}
img {
background-color: #FFFFFF;
padding: 2px;
border: 1px solid #DDDDDD;
border-radius: 3px;
border: 1px solid #CCCCCC;
margin: 0 5px;
}
h1 {
margin-top: 0;
font-size: 35px;
line-height: 40px;
}
h2 {
border-bottom: 4px solid #f7f7f7;
padding-top: 10px;
padding-bottom: 2px;
font-size: 145%;
}
h3 {
border-bottom: 2px solid #f7f7f7;
padding-top: 10px;
font-size: 120%;
}
h4 {
border-bottom: 1px solid #f7f7f7;
margin-left: 8px;
font-size: 105%;
}
h5, h6 {
border-bottom: 1px solid #ccc;
font-size: 105%;
}
a {
color: #0033dd;
text-decoration: none;
}
a:hover {
color: #6666ff; }
a:visited {
color: #800080; }
a:visited:hover {
color: #BB00BB; }
a[href^="http:"] {
text-decoration: underline; }
a[href^="https:"] {
text-decoration: underline; }

code > span.kw { color: #555; font-weight: bold; } 
code > span.dt { color: #902000; } 
code > span.dv { color: #40a070; } 
code > span.bn { color: #d14; } 
code > span.fl { color: #d14; } 
code > span.ch { color: #d14; } 
code > span.st { color: #d14; } 
code > span.co { color: #888888; font-style: italic; } 
code > span.ot { color: #007020; } 
code > span.al { color: #ff0000; font-weight: bold; } 
code > span.fu { color: #900; font-weight: bold; }  code > span.er { color: #a61717; background-color: #e3d2d2; } 
</style>




</head>

<body>




<h1 class="title toc-ignore">Optimization</h1>
<h4 class="author">Andrew Fogarty</h4>
<h4 class="date">10/14/2020</h4>


<div id="TOC">
<ul>
<li><a href="#introduction"><span class="toc-section-number">1</span> Introduction</a></li>
<li><a href="#generating-data-and-loading-on-the-fly"><span class="toc-section-number">2</span> Generating Data and Loading On-The-Fly</a></li>
<li><a href="#by-hand-logistic-regression-with-l2-regularization"><span class="toc-section-number">3</span> By Hand: Logistic Regression with L2 Regularization</a></li>
<li><a href="#data-loaders"><span class="toc-section-number">4</span> Data Loaders</a></li>
<li><a href="#training"><span class="toc-section-number">5</span> Training</a>
<ul>
<li><a href="#training-coefficients"><span class="toc-section-number">5.1</span> Training Coefficients</a></li>
</ul></li>
<li><a href="#testing"><span class="toc-section-number">6</span> Testing</a>
<ul>
<li><a href="#testing-coefficients"><span class="toc-section-number">6.1</span> Testing Coefficients</a></li>
</ul></li>
<li><a href="#verification"><span class="toc-section-number">7</span> Verification</a></li>
</ul>
</div>

<div class="sourceCode" id="cb1"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1-1"><a href="#cb1-1"></a><span class="co"># load python</span></span>
<span id="cb1-2"><a href="#cb1-2"></a><span class="kw">library</span>(reticulate)</span>
<span id="cb1-3"><a href="#cb1-3"></a><span class="kw">use_condaenv</span>(<span class="st">&quot;my_ml&quot;</span>)</span></code></pre></div>
<div class="sourceCode" id="cb2"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1"></a><span class="co"># import packages</span></span>
<span id="cb2-2"><a href="#cb2-2"></a><span class="im">import</span> torch</span>
<span id="cb2-3"><a href="#cb2-3"></a><span class="im">from</span> torch.utils.data <span class="im">import</span> Dataset, random_split, DataLoader, RandomSampler</span>
<span id="cb2-4"><a href="#cb2-4"></a><span class="im">import</span> torch.nn.functional <span class="im">as</span> F</span>
<span id="cb2-5"><a href="#cb2-5"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb2-6"><a href="#cb2-6"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb2-7"><a href="#cb2-7"></a><span class="im">from</span> sklearn <span class="im">import</span> datasets</span>
<span id="cb2-8"><a href="#cb2-8"></a><span class="im">from</span> sklearn.linear_model <span class="im">import</span> LogisticRegression</span>
<span id="cb2-9"><a href="#cb2-9"></a><span class="im">import</span> sys</span>
<span id="cb2-10"><a href="#cb2-10"></a>sys.path.append(<span class="st">&quot;C:/Users/Andrew/Desktop/Projects/Deep Learning/utils&quot;</span>)</span>
<span id="cb2-11"><a href="#cb2-11"></a><span class="im">from</span> tools <span class="im">import</span> AverageMeter, ProgressBar</span>
<span id="cb2-12"><a href="#cb2-12"></a></span>
<span id="cb2-13"><a href="#cb2-13"></a></span>
<span id="cb2-14"><a href="#cb2-14"></a>SEED <span class="op">=</span> <span class="dv">15</span></span>
<span id="cb2-15"><a href="#cb2-15"></a>torch.manual_seed(SEED)</span></code></pre></div>
<pre><code>## &lt;torch._C.Generator object at 0x000000001FA9C070&gt;</code></pre>
<div class="sourceCode" id="cb4"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1"></a>torch.backends.cudnn.deterministic <span class="op">=</span> <span class="va">True</span></span>
<span id="cb4-2"><a href="#cb4-2"></a>torch.cuda.amp.autocast(enabled<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb4-3"><a href="#cb4-3"></a></span>
<span id="cb4-4"><a href="#cb4-4"></a><span class="co"># set torch device</span></span></code></pre></div>
<pre><code>## &lt;torch.cuda.amp.autocast_mode.autocast object at 0x00000000265C6C08&gt;</code></pre>
<div class="sourceCode" id="cb6"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1"></a>device <span class="op">=</span> torch.device(<span class="st">&quot;cuda:0&quot;</span> <span class="cf">if</span> torch.cuda.is_available() <span class="cf">else</span> <span class="st">&quot;cpu&quot;</span>)</span></code></pre></div>
<div id="introduction" class="section level1" number="1">
<h1 number="1"><span class="header-section-number">1</span> Introduction</h1>
<p>Numerical optimizers provide the means to estimate our parameters by finding the values that maximize the likelihood of generating our data. This guide helps us understand how optimization algorithms find the best estimates for our coefficients in a step-by-step process.</p>
<p>This guide uses Gradient Descent and regularization techniques in a completely manual approach to finding parameters that most likely generated our data. Another way of thinking about gradient descent is that we are inevitably asking the algorithm the following: What parameter values will push our error to zero?</p>
</div>
<div id="generating-data-and-loading-on-the-fly" class="section level1" number="2">
<h1 number="2"><span class="header-section-number">2</span> Generating Data and Loading On-The-Fly</h1>
<p>In this section, we will create a torch data set that reads a csv file and processes (normalizes) our data on the fly via the <code>standardize</code> class.</p>
<div class="sourceCode" id="cb7"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1"></a><span class="co"># create Dataset</span></span>
<span id="cb7-2"><a href="#cb7-2"></a><span class="kw">class</span> CSVDataset(Dataset):</span>
<span id="cb7-3"><a href="#cb7-3"></a>    <span class="co">&quot;&quot;&quot;LM dataset.&quot;&quot;&quot;</span></span>
<span id="cb7-4"><a href="#cb7-4"></a></span>
<span id="cb7-5"><a href="#cb7-5"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, csv_file, transform<span class="op">=</span><span class="va">None</span>):</span>
<span id="cb7-6"><a href="#cb7-6"></a>        <span class="co">&quot;&quot;&quot;</span></span>
<span id="cb7-7"><a href="#cb7-7"></a><span class="co">        Args:</span></span>
<span id="cb7-8"><a href="#cb7-8"></a><span class="co">            csv_file (string): Path to the csv file.</span></span>
<span id="cb7-9"><a href="#cb7-9"></a><span class="co">            transform (callable, optional): Optional transform to be applied</span></span>
<span id="cb7-10"><a href="#cb7-10"></a><span class="co">                on a sample.</span></span>
<span id="cb7-11"><a href="#cb7-11"></a><span class="co">        &quot;&quot;&quot;</span></span>
<span id="cb7-12"><a href="#cb7-12"></a>        <span class="co"># initialize</span></span>
<span id="cb7-13"><a href="#cb7-13"></a>        <span class="va">self</span>.data_frame <span class="op">=</span> pd.read_csv(csv_file)</span>
<span id="cb7-14"><a href="#cb7-14"></a>        <span class="co"># all columns but the last</span></span>
<span id="cb7-15"><a href="#cb7-15"></a>        <span class="va">self</span>.features <span class="op">=</span> <span class="va">self</span>.data_frame[<span class="va">self</span>.data_frame.columns[:<span class="op">-</span><span class="dv">1</span>]]</span>
<span id="cb7-16"><a href="#cb7-16"></a>        <span class="co"># the last column</span></span>
<span id="cb7-17"><a href="#cb7-17"></a>        <span class="va">self</span>.target <span class="op">=</span> <span class="va">self</span>.data_frame[<span class="va">self</span>.data_frame.columns[<span class="op">-</span><span class="dv">1</span>]]</span>
<span id="cb7-18"><a href="#cb7-18"></a>        <span class="co"># initialize the transform if specified</span></span>
<span id="cb7-19"><a href="#cb7-19"></a>        <span class="va">self</span>.transform <span class="op">=</span> transform</span>
<span id="cb7-20"><a href="#cb7-20"></a></span>
<span id="cb7-21"><a href="#cb7-21"></a>        <span class="co"># get length of df</span></span>
<span id="cb7-22"><a href="#cb7-22"></a>    <span class="kw">def</span> <span class="fu">__len__</span>(<span class="va">self</span>):</span>
<span id="cb7-23"><a href="#cb7-23"></a>        <span class="cf">return</span> <span class="bu">len</span>(<span class="va">self</span>.data_frame)</span>
<span id="cb7-24"><a href="#cb7-24"></a></span>
<span id="cb7-25"><a href="#cb7-25"></a>        <span class="co"># get df mean and std</span></span>
<span id="cb7-26"><a href="#cb7-26"></a>    <span class="kw">def</span> __get_norm__(<span class="va">self</span>):</span>
<span id="cb7-27"><a href="#cb7-27"></a>        <span class="va">self</span>.mu, <span class="va">self</span>.sigma <span class="op">=</span> np.mean(<span class="va">self</span>.features.values, axis<span class="op">=</span><span class="dv">0</span>), np.std(<span class="va">self</span>.features.values, axis<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb7-28"><a href="#cb7-28"></a>        <span class="cf">return</span> <span class="va">self</span>.mu, <span class="va">self</span>.sigma</span>
<span id="cb7-29"><a href="#cb7-29"></a></span>
<span id="cb7-30"><a href="#cb7-30"></a>        <span class="co"># get sample target</span></span>
<span id="cb7-31"><a href="#cb7-31"></a>    <span class="kw">def</span> __get_target__(<span class="va">self</span>):</span>
<span id="cb7-32"><a href="#cb7-32"></a>        <span class="cf">return</span> <span class="va">self</span>.target</span>
<span id="cb7-33"><a href="#cb7-33"></a></span>
<span id="cb7-34"><a href="#cb7-34"></a>        <span class="co"># get df filtered by indices</span></span>
<span id="cb7-35"><a href="#cb7-35"></a>    <span class="kw">def</span> __get_values__(<span class="va">self</span>, indices):</span>
<span id="cb7-36"><a href="#cb7-36"></a>        <span class="cf">return</span> <span class="va">self</span>.data_frame.iloc[indices]</span>
<span id="cb7-37"><a href="#cb7-37"></a></span>
<span id="cb7-38"><a href="#cb7-38"></a>    <span class="kw">def</span> <span class="fu">__getitem__</span>(<span class="va">self</span>, idx):</span>
<span id="cb7-39"><a href="#cb7-39"></a>        <span class="cf">if</span> torch.is_tensor(idx):</span>
<span id="cb7-40"><a href="#cb7-40"></a>            idx <span class="op">=</span> idx.tolist()</span>
<span id="cb7-41"><a href="#cb7-41"></a></span>
<span id="cb7-42"><a href="#cb7-42"></a>        <span class="co"># pull a sample in a dict</span></span>
<span id="cb7-43"><a href="#cb7-43"></a>        sample <span class="op">=</span> {<span class="st">&#39;features&#39;</span>: torch.tensor(<span class="va">self</span>.features.iloc[idx].values),</span>
<span id="cb7-44"><a href="#cb7-44"></a>                  <span class="st">&#39;target&#39;</span>: torch.tensor(<span class="va">self</span>.target.iloc[idx]),</span>
<span id="cb7-45"><a href="#cb7-45"></a>                  <span class="st">&#39;idx&#39;</span>: torch.tensor(idx)}</span>
<span id="cb7-46"><a href="#cb7-46"></a></span>
<span id="cb7-47"><a href="#cb7-47"></a>        <span class="cf">if</span> <span class="va">self</span>.transform:</span>
<span id="cb7-48"><a href="#cb7-48"></a>            sample <span class="op">=</span> <span class="va">self</span>.transform(sample)</span>
<span id="cb7-49"><a href="#cb7-49"></a></span>
<span id="cb7-50"><a href="#cb7-50"></a>        <span class="cf">return</span> sample</span>
<span id="cb7-51"><a href="#cb7-51"></a></span>
<span id="cb7-52"><a href="#cb7-52"></a></span>
<span id="cb7-53"><a href="#cb7-53"></a><span class="kw">class</span> Standardize():</span>
<span id="cb7-54"><a href="#cb7-54"></a>    <span class="co"># retrieve sample and unpack it</span></span>
<span id="cb7-55"><a href="#cb7-55"></a>    <span class="kw">def</span> <span class="fu">__call__</span>(<span class="va">self</span>, sample):</span>
<span id="cb7-56"><a href="#cb7-56"></a>        features, target, idx <span class="op">=</span> (sample[<span class="st">&#39;features&#39;</span>],</span>
<span id="cb7-57"><a href="#cb7-57"></a>                              sample[<span class="st">&#39;target&#39;</span>],</span>
<span id="cb7-58"><a href="#cb7-58"></a>                              sample[<span class="st">&#39;idx&#39;</span>])</span>
<span id="cb7-59"><a href="#cb7-59"></a></span>
<span id="cb7-60"><a href="#cb7-60"></a>        <span class="co"># normalize each value; zero mean, unit variance</span></span>
<span id="cb7-61"><a href="#cb7-61"></a>        normalized_features <span class="op">=</span> (features <span class="op">-</span> csv_dataset.__get_norm__()[<span class="dv">0</span>]) <span class="op">/</span> csv_dataset.__get_norm__()[<span class="dv">1</span>]</span>
<span id="cb7-62"><a href="#cb7-62"></a></span>
<span id="cb7-63"><a href="#cb7-63"></a>        <span class="co"># yield another dict</span></span>
<span id="cb7-64"><a href="#cb7-64"></a>        <span class="cf">return</span> {<span class="st">&#39;features&#39;</span>: torch.as_tensor(normalized_features,</span>
<span id="cb7-65"><a href="#cb7-65"></a>                                         dtype<span class="op">=</span>torch.float32,</span>
<span id="cb7-66"><a href="#cb7-66"></a>                                         device<span class="op">=</span>device),</span>
<span id="cb7-67"><a href="#cb7-67"></a>                <span class="st">&#39;target&#39;</span>: torch.as_tensor(target,</span>
<span id="cb7-68"><a href="#cb7-68"></a>                                          dtype<span class="op">=</span>torch.float32,</span>
<span id="cb7-69"><a href="#cb7-69"></a>                                          device<span class="op">=</span>device),</span>
<span id="cb7-70"><a href="#cb7-70"></a>                <span class="st">&#39;idx&#39;</span>: torch.as_tensor(idx,</span>
<span id="cb7-71"><a href="#cb7-71"></a>                                       dtype<span class="op">=</span>torch.<span class="bu">int</span>,</span>
<span id="cb7-72"><a href="#cb7-72"></a>                                       device<span class="op">=</span>device)}</span></code></pre></div>
<p>We will use <code>sklearn</code>’s <code>make_classification</code> method to generate some data useful for logistic regression.</p>
<div class="sourceCode" id="cb8"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1"></a>X, y <span class="op">=</span> datasets.make_classification(n_samples<span class="op">=</span><span class="dv">1000</span>,</span>
<span id="cb8-2"><a href="#cb8-2"></a>                                         n_features<span class="op">=</span><span class="dv">2</span>,</span>
<span id="cb8-3"><a href="#cb8-3"></a>                                         n_informative<span class="op">=</span><span class="dv">2</span>,</span>
<span id="cb8-4"><a href="#cb8-4"></a>                                         n_redundant<span class="op">=</span><span class="dv">0</span>,</span>
<span id="cb8-5"><a href="#cb8-5"></a>                                         n_classes<span class="op">=</span><span class="dv">2</span>,</span>
<span id="cb8-6"><a href="#cb8-6"></a>                                         random_state<span class="op">=</span><span class="dv">15</span>)</span>
<span id="cb8-7"><a href="#cb8-7"></a><span class="co"># place data into df</span></span>
<span id="cb8-8"><a href="#cb8-8"></a>df <span class="op">=</span> pd.DataFrame({<span class="st">&#39;x1&#39;</span>: X[:, <span class="dv">0</span>], <span class="st">&#39;x2&#39;</span>: X[:, <span class="dv">1</span>], <span class="st">&#39;y&#39;</span>: y})</span>
<span id="cb8-9"><a href="#cb8-9"></a>df.to_csv(<span class="st">&#39;classification_demo.csv&#39;</span>, index<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb8-10"><a href="#cb8-10"></a></span>
<span id="cb8-11"><a href="#cb8-11"></a><span class="co"># instantiate the lazy data set</span></span>
<span id="cb8-12"><a href="#cb8-12"></a>csv_dataset <span class="op">=</span> CSVDataset(csv_file<span class="op">=</span><span class="st">&#39;classification_demo.csv&#39;</span>, transform<span class="op">=</span>Standardize())</span>
<span id="cb8-13"><a href="#cb8-13"></a></span>
<span id="cb8-14"><a href="#cb8-14"></a><span class="co"># check normalization unit variance values</span></span>
<span id="cb8-15"><a href="#cb8-15"></a>csv_dataset.__get_norm__()[<span class="dv">1</span>]</span></code></pre></div>
<pre><code>## array([1.39204168, 1.26812651])</code></pre>
<div class="sourceCode" id="cb10"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb10-1"><a href="#cb10-1"></a>csv_dataset.__get_norm__()[<span class="dv">0</span>]</span></code></pre></div>
<pre><code>## array([-0.02077113,  0.01676885])</code></pre>
<p>Next, we will split the data into training and test sets.</p>
<div class="sourceCode" id="cb12"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb12-1"><a href="#cb12-1"></a><span class="co"># check some data</span></span>
<span id="cb12-2"><a href="#cb12-2"></a><span class="cf">for</span> i, batch <span class="kw">in</span> <span class="bu">enumerate</span>(csv_dataset):</span>
<span id="cb12-3"><a href="#cb12-3"></a>    <span class="cf">if</span> i <span class="op">==</span> <span class="dv">0</span>:</span>
<span id="cb12-4"><a href="#cb12-4"></a>        <span class="cf">break</span></span>
<span id="cb12-5"><a href="#cb12-5"></a></span>
<span id="cb12-6"><a href="#cb12-6"></a><span class="co"># set train and test size</span></span>
<span id="cb12-7"><a href="#cb12-7"></a>train_size <span class="op">=</span> <span class="bu">int</span>(<span class="fl">0.8</span> <span class="op">*</span> <span class="bu">len</span>(csv_dataset))</span>
<span id="cb12-8"><a href="#cb12-8"></a>test_size <span class="op">=</span> <span class="bu">int</span>(<span class="fl">0.2</span> <span class="op">*</span> <span class="bu">len</span>(csv_dataset))</span>
<span id="cb12-9"><a href="#cb12-9"></a></span>
<span id="cb12-10"><a href="#cb12-10"></a><span class="co"># split data sets</span></span>
<span id="cb12-11"><a href="#cb12-11"></a>train_ds, test_ds <span class="op">=</span> torch.utils.data.random_split(csv_dataset, [train_size, test_size])</span></code></pre></div>
</div>
<div id="by-hand-logistic-regression-with-l2-regularization" class="section level1" number="3">
<h1 number="3"><span class="header-section-number">3</span> By Hand: Logistic Regression with L2 Regularization</h1>
<p>The class below finds the weights and bias terms for a logistic regression model through stochastic gradient descent. Through numerical optimization, we find our coefficients via backward propagation by:</p>
<ol style="list-style-type: decimal">
<li><p>Instantiating guesses for our weights and intercept at 0. Alternatively, we could initialize our guesses based on a distribution of our choice.</p></li>
<li><p>Matrix multiplying our features against our errors to find our new weight increment and summing our errors to find our new intercept increment.</p></li>
<li><p>Update our weights and intercept by multiplying the learning rate (<span class="math inline">\(\eta\)</span>) against our respective increments and then finally subtracting these values from the instantiated guesses for our weights and bias.</p></li>
</ol>
<p>To avoid calculating and figuring out backward propagation, PyTorch allows us to skip this process by simplying wrapping class model with <code>torch.nn.Module</code> and then relying on <code>torch.optim</code> to handle backward propagation and to update our model’s weights.</p>
<div class="sourceCode" id="cb13"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb13-1"><a href="#cb13-1"></a><span class="co"># create manual logistic regression model</span></span>
<span id="cb13-2"><a href="#cb13-2"></a><span class="kw">class</span> LR():</span>
<span id="cb13-3"><a href="#cb13-3"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, num_features, LAMBDA<span class="op">=</span><span class="fl">0.0</span>):</span>
<span id="cb13-4"><a href="#cb13-4"></a>        <span class="va">self</span>.num_features <span class="op">=</span> num_features</span>
<span id="cb13-5"><a href="#cb13-5"></a>        <span class="va">self</span>.weights <span class="op">=</span> torch.zeros(<span class="dv">1</span>, num_features,</span>
<span id="cb13-6"><a href="#cb13-6"></a>                                   dtype<span class="op">=</span>torch.float32, device<span class="op">=</span>device)</span>
<span id="cb13-7"><a href="#cb13-7"></a>        <span class="va">self</span>.bias <span class="op">=</span> torch.zeros(<span class="dv">1</span>, dtype<span class="op">=</span>torch.float32, device<span class="op">=</span>device)</span>
<span id="cb13-8"><a href="#cb13-8"></a>        <span class="va">self</span>.LAMBDA <span class="op">=</span> LAMBDA</span>
<span id="cb13-9"><a href="#cb13-9"></a></span>
<span id="cb13-10"><a href="#cb13-10"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb13-11"><a href="#cb13-11"></a>        linear <span class="op">=</span> torch.add(torch.mm(x, <span class="va">self</span>.weights.t()), <span class="va">self</span>.bias).view(<span class="op">-</span><span class="dv">1</span>)</span>
<span id="cb13-12"><a href="#cb13-12"></a>        probas <span class="op">=</span> torch.sigmoid(linear)</span>
<span id="cb13-13"><a href="#cb13-13"></a>        <span class="cf">return</span> probas</span>
<span id="cb13-14"><a href="#cb13-14"></a></span>
<span id="cb13-15"><a href="#cb13-15"></a>    <span class="kw">def</span> backward(<span class="va">self</span>, x, y, probas):</span>
<span id="cb13-16"><a href="#cb13-16"></a>        grad_loss_out <span class="op">=</span> y <span class="op">-</span> probas.view(<span class="op">-</span><span class="dv">1</span>)</span>
<span id="cb13-17"><a href="#cb13-17"></a>        grad_loss_w <span class="op">=</span> <span class="op">-</span>torch.mm(x.t(), grad_loss_out.view(<span class="op">-</span><span class="dv">1</span>, <span class="dv">1</span>)).t()</span>
<span id="cb13-18"><a href="#cb13-18"></a>        grad_loss_b <span class="op">=</span> <span class="op">-</span>torch.<span class="bu">sum</span>(grad_loss_out)</span>
<span id="cb13-19"><a href="#cb13-19"></a>        <span class="cf">return</span> grad_loss_w, grad_loss_b</span>
<span id="cb13-20"><a href="#cb13-20"></a></span>
<span id="cb13-21"><a href="#cb13-21"></a>    <span class="kw">def</span> _logit_cost(<span class="va">self</span>, y, probas):</span>
<span id="cb13-22"><a href="#cb13-22"></a>        tmp1 <span class="op">=</span> torch.mm(<span class="op">-</span>y.view(<span class="dv">1</span>, <span class="op">-</span><span class="dv">1</span>), torch.log(probas.view(<span class="op">-</span><span class="dv">1</span>, <span class="dv">1</span>)))</span>
<span id="cb13-23"><a href="#cb13-23"></a>        tmp2 <span class="op">=</span> torch.mm((<span class="dv">1</span> <span class="op">-</span> y).view(<span class="dv">1</span>, <span class="op">-</span><span class="dv">1</span>), torch.log(<span class="dv">1</span> <span class="op">-</span> probas.view(<span class="op">-</span><span class="dv">1</span>, <span class="dv">1</span>)))</span>
<span id="cb13-24"><a href="#cb13-24"></a>        l2 <span class="op">=</span> <span class="va">self</span>.LAMBDA <span class="op">/</span> <span class="fl">2.0</span> <span class="op">*</span> torch.<span class="bu">sum</span>(<span class="va">self</span>.weights<span class="op">**</span><span class="dv">2</span>)</span>
<span id="cb13-25"><a href="#cb13-25"></a>        <span class="cf">return</span> (tmp1 <span class="op">-</span> tmp2) <span class="op">+</span> l2</span></code></pre></div>
</div>
<div id="data-loaders" class="section level1" number="4">
<h1 number="4"><span class="header-section-number">4</span> Data Loaders</h1>
<p>Next we prepare our data loaders and check our data again.</p>
<div class="sourceCode" id="cb14"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb14-1"><a href="#cb14-1"></a><span class="co"># create DataLoaders</span></span>
<span id="cb14-2"><a href="#cb14-2"></a>train_dataloader <span class="op">=</span> DataLoader(train_ds,</span>
<span id="cb14-3"><a href="#cb14-3"></a>                              batch_size<span class="op">=</span><span class="dv">100</span>,</span>
<span id="cb14-4"><a href="#cb14-4"></a>                              sampler<span class="op">=</span><span class="va">None</span>,</span>
<span id="cb14-5"><a href="#cb14-5"></a>                              shuffle<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb14-6"><a href="#cb14-6"></a></span>
<span id="cb14-7"><a href="#cb14-7"></a>test_dataloader <span class="op">=</span> DataLoader(test_ds,</span>
<span id="cb14-8"><a href="#cb14-8"></a>                              batch_size<span class="op">=</span><span class="dv">100</span>,</span>
<span id="cb14-9"><a href="#cb14-9"></a>                              sampler<span class="op">=</span><span class="va">None</span>,</span>
<span id="cb14-10"><a href="#cb14-10"></a>                              shuffle<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb14-11"><a href="#cb14-11"></a></span>
<span id="cb14-12"><a href="#cb14-12"></a><span class="co"># check data</span></span>
<span id="cb14-13"><a href="#cb14-13"></a><span class="cf">for</span> i, batch <span class="kw">in</span> <span class="bu">enumerate</span>(train_dataloader):</span>
<span id="cb14-14"><a href="#cb14-14"></a>    <span class="cf">if</span> i <span class="op">==</span> <span class="dv">0</span>:</span>
<span id="cb14-15"><a href="#cb14-15"></a>        <span class="cf">break</span></span></code></pre></div>
</div>
<div id="training" class="section level1" number="5">
<h1 number="5"><span class="header-section-number">5</span> Training</h1>
<div class="sourceCode" id="cb15"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb15-1"><a href="#cb15-1"></a><span class="co"># set epochs</span></span>
<span id="cb15-2"><a href="#cb15-2"></a>epochs <span class="op">=</span> <span class="dv">20</span></span>
<span id="cb15-3"><a href="#cb15-3"></a></span>
<span id="cb15-4"><a href="#cb15-4"></a><span class="co"># set lr</span></span>
<span id="cb15-5"><a href="#cb15-5"></a>learning_rate <span class="op">=</span> <span class="fl">0.01</span></span>
<span id="cb15-6"><a href="#cb15-6"></a></span>
<span id="cb15-7"><a href="#cb15-7"></a><span class="co"># instantiate model</span></span>
<span id="cb15-8"><a href="#cb15-8"></a>model <span class="op">=</span> LR(num_features<span class="op">=</span><span class="dv">2</span>, LAMBDA<span class="op">=</span><span class="fl">25.0</span>)</span>
<span id="cb15-9"><a href="#cb15-9"></a></span>
<span id="cb15-10"><a href="#cb15-10"></a></span>
<span id="cb15-11"><a href="#cb15-11"></a><span class="co"># prepare training function</span></span>
<span id="cb15-12"><a href="#cb15-12"></a><span class="kw">def</span> train(dataloader):</span>
<span id="cb15-13"><a href="#cb15-13"></a>    <span class="co">#pbar = ProgressBar(n_total=len(dataloader), desc=&#39;Training&#39;)</span></span>
<span id="cb15-14"><a href="#cb15-14"></a>    train_loss <span class="op">=</span> AverageMeter()</span>
<span id="cb15-15"><a href="#cb15-15"></a>    <span class="cf">for</span> batch_idx, batch <span class="kw">in</span> <span class="bu">enumerate</span>(dataloader):</span>
<span id="cb15-16"><a href="#cb15-16"></a>        <span class="co"># forward</span></span>
<span id="cb15-17"><a href="#cb15-17"></a>        probas <span class="op">=</span> model.forward(batch[<span class="st">&#39;features&#39;</span>])</span>
<span id="cb15-18"><a href="#cb15-18"></a>        <span class="co"># backward</span></span>
<span id="cb15-19"><a href="#cb15-19"></a>        grad_w, grad_b <span class="op">=</span> model.backward(batch[<span class="st">&#39;features&#39;</span>],</span>
<span id="cb15-20"><a href="#cb15-20"></a>                                        batch[<span class="st">&#39;target&#39;</span>],</span>
<span id="cb15-21"><a href="#cb15-21"></a>                                        probas)</span>
<span id="cb15-22"><a href="#cb15-22"></a>        <span class="co"># manual regularization -- account for mini-batches</span></span>
<span id="cb15-23"><a href="#cb15-23"></a>        l2_reg <span class="op">=</span> model.LAMBDA <span class="op">*</span> model.weights <span class="op">/</span> <span class="bu">len</span>(dataloader)</span>
<span id="cb15-24"><a href="#cb15-24"></a>        <span class="co"># update weights</span></span>
<span id="cb15-25"><a href="#cb15-25"></a>        model.weights <span class="op">-=</span> learning_rate <span class="op">*</span> (grad_w <span class="op">+</span> l2_reg)</span>
<span id="cb15-26"><a href="#cb15-26"></a>        model.bias <span class="op">-=</span> learning_rate <span class="op">*</span> grad_b</span>
<span id="cb15-27"><a href="#cb15-27"></a>        <span class="co"># record loss</span></span>
<span id="cb15-28"><a href="#cb15-28"></a>        loss <span class="op">=</span> model._logit_cost(batch[<span class="st">&#39;target&#39;</span>], probas)</span>
<span id="cb15-29"><a href="#cb15-29"></a>        <span class="co"># update meter</span></span>
<span id="cb15-30"><a href="#cb15-30"></a>        train_loss.update(loss.item(), n<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb15-31"><a href="#cb15-31"></a>        <span class="co"># update progress bar</span></span>
<span id="cb15-32"><a href="#cb15-32"></a>        <span class="co">#pbar(step=batch_idx, info={&#39;batch_loss&#39;: loss.item()})</span></span>
<span id="cb15-33"><a href="#cb15-33"></a>    <span class="cf">return</span> {<span class="st">&#39;train_loss&#39;</span>: train_loss.avg}</span>
<span id="cb15-34"><a href="#cb15-34"></a></span>
<span id="cb15-35"><a href="#cb15-35"></a></span>
<span id="cb15-36"><a href="#cb15-36"></a><span class="co"># training</span></span>
<span id="cb15-37"><a href="#cb15-37"></a><span class="cf">for</span> epoch <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">1</span>, epochs <span class="op">+</span> <span class="dv">1</span>):</span>
<span id="cb15-38"><a href="#cb15-38"></a>    train_log <span class="op">=</span> train(train_dataloader)</span>
<span id="cb15-39"><a href="#cb15-39"></a>    logs <span class="op">=</span> <span class="bu">dict</span>(train_log)</span>
<span id="cb15-40"><a href="#cb15-40"></a>    train_logs <span class="op">=</span> <span class="ss">f&#39;</span><span class="ch">\n</span><span class="ss">Epoch: </span><span class="sc">{</span>epoch<span class="sc">}</span><span class="ss"> - &#39;</span> <span class="op">+</span> <span class="st">&quot;-&quot;</span>.join([<span class="ss">f&#39; </span><span class="sc">{</span>key<span class="sc">}</span><span class="ss">: </span><span class="sc">{</span>value<span class="sc">:.4f}</span><span class="ss"> &#39;</span> <span class="cf">for</span> key, value <span class="kw">in</span> logs.items()])</span>
<span id="cb15-41"><a href="#cb15-41"></a>    <span class="bu">print</span>(train_logs)</span></code></pre></div>
<pre><code>## 
## Epoch: 1 -  train_loss: 61.6227 
## 
## Epoch: 2 -  train_loss: 68.7676 
## 
## Epoch: 3 -  train_loss: 76.8934 
## 
## Epoch: 4 -  train_loss: 80.5848 
## 
## Epoch: 5 -  train_loss: 81.5844 
## 
## Epoch: 6 -  train_loss: 82.5682 
## 
## Epoch: 7 -  train_loss: 82.3940 
## 
## Epoch: 8 -  train_loss: 83.0089 
## 
## Epoch: 9 -  train_loss: 82.7335 
## 
## Epoch: 10 -  train_loss: 83.3378 
## 
## Epoch: 11 -  train_loss: 83.1370 
## 
## Epoch: 12 -  train_loss: 83.2270 
## 
## Epoch: 13 -  train_loss: 82.8907 
## 
## Epoch: 14 -  train_loss: 83.9467 
## 
## Epoch: 15 -  train_loss: 83.5733 
## 
## Epoch: 16 -  train_loss: 82.8066 
## 
## Epoch: 17 -  train_loss: 83.3528 
## 
## Epoch: 18 -  train_loss: 83.0623 
## 
## Epoch: 19 -  train_loss: 83.3338 
## 
## Epoch: 20 -  train_loss: 83.3090</code></pre>
<div id="training-coefficients" class="section level2" number="5.1">
<h2 number="5.1"><span class="header-section-number">5.1</span> Training Coefficients</h2>
<div class="sourceCode" id="cb17"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb17-1"><a href="#cb17-1"></a><span class="bu">print</span>(<span class="st">&#39;Weights&#39;</span>, model.weights)</span></code></pre></div>
<pre><code>## Weights tensor([[-0.0477,  2.0780]], device=&#39;cuda:0&#39;)</code></pre>
<div class="sourceCode" id="cb19"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb19-1"><a href="#cb19-1"></a><span class="bu">print</span>(<span class="st">&#39;Bias&#39;</span>, model.bias)</span></code></pre></div>
<pre><code>## Bias tensor([0.1234], device=&#39;cuda:0&#39;)</code></pre>
</div>
</div>
<div id="testing" class="section level1" number="6">
<h1 number="6"><span class="header-section-number">6</span> Testing</h1>
<div class="sourceCode" id="cb21"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb21-1"><a href="#cb21-1"></a><span class="co"># valid/test function</span></span>
<span id="cb21-2"><a href="#cb21-2"></a><span class="kw">def</span> test(dataloader):</span>
<span id="cb21-3"><a href="#cb21-3"></a>    <span class="co">#pbar = ProgressBar(n_total=len(dataloader), desc=&#39;Testing&#39;)</span></span>
<span id="cb21-4"><a href="#cb21-4"></a>    valid_loss <span class="op">=</span> AverageMeter()</span>
<span id="cb21-5"><a href="#cb21-5"></a>    valid_acc <span class="op">=</span> AverageMeter()</span>
<span id="cb21-6"><a href="#cb21-6"></a>    count <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb21-7"><a href="#cb21-7"></a>    <span class="cf">for</span> batch_idx, batch <span class="kw">in</span> <span class="bu">enumerate</span>(dataloader):</span>
<span id="cb21-8"><a href="#cb21-8"></a>        <span class="co"># forward -- skip backward prop</span></span>
<span id="cb21-9"><a href="#cb21-9"></a>        probas <span class="op">=</span> model.forward(batch[<span class="st">&#39;features&#39;</span>])</span>
<span id="cb21-10"><a href="#cb21-10"></a>        <span class="co"># record loss</span></span>
<span id="cb21-11"><a href="#cb21-11"></a>        loss <span class="op">=</span> model._logit_cost(batch[<span class="st">&#39;target&#39;</span>], probas)</span>
<span id="cb21-12"><a href="#cb21-12"></a>        <span class="co"># get predictions</span></span>
<span id="cb21-13"><a href="#cb21-13"></a>        prediction <span class="op">=</span> torch.where(probas <span class="op">&gt;</span> <span class="fl">0.5</span>, torch.tensor(<span class="dv">1</span>, device<span class="op">=</span>device), torch.tensor(<span class="dv">0</span>, device<span class="op">=</span>device)).view(<span class="op">-</span><span class="dv">1</span>)</span>
<span id="cb21-14"><a href="#cb21-14"></a>        <span class="co"># compare</span></span>
<span id="cb21-15"><a href="#cb21-15"></a>        correct <span class="op">=</span> prediction.eq(batch[<span class="st">&#39;target&#39;</span>]).<span class="bu">sum</span>().item()</span>
<span id="cb21-16"><a href="#cb21-16"></a>        valid_loss.update(loss, n<span class="op">=</span>batch[<span class="st">&#39;features&#39;</span>].size(<span class="dv">0</span>))</span>
<span id="cb21-17"><a href="#cb21-17"></a>        valid_acc.update(correct, n<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb21-18"><a href="#cb21-18"></a>        count <span class="op">+=</span> batch[<span class="st">&#39;features&#39;</span>].size(<span class="dv">0</span>)</span>
<span id="cb21-19"><a href="#cb21-19"></a>        <span class="co">#pbar(step=batch_idx)</span></span>
<span id="cb21-20"><a href="#cb21-20"></a>    <span class="cf">return</span> {<span class="st">&#39;valid_loss&#39;</span>: valid_loss.avg,</span>
<span id="cb21-21"><a href="#cb21-21"></a>            <span class="st">&#39;valid_acc&#39;</span>: valid_acc.<span class="bu">sum</span> <span class="op">/</span> count}</span></code></pre></div>
<div id="testing-coefficients" class="section level2" number="6.1">
<h2 number="6.1"><span class="header-section-number">6.1</span> Testing Coefficients</h2>
<div class="sourceCode" id="cb22"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb22-1"><a href="#cb22-1"></a><span class="co"># testing</span></span>
<span id="cb22-2"><a href="#cb22-2"></a>test_log <span class="op">=</span> test(test_dataloader)</span>
<span id="cb22-3"><a href="#cb22-3"></a><span class="bu">print</span>(test_log)</span></code></pre></div>
<pre><code>## {&#39;valid_loss&#39;: tensor([[86.0372]], device=&#39;cuda:0&#39;), &#39;valid_acc&#39;: 0.89}</code></pre>
</div>
</div>
<div id="verification" class="section level1" number="7">
<h1 number="7"><span class="header-section-number">7</span> Verification</h1>
<p>We can see that our work above maps closely an sklearn implementation that uses all of the data for training.</p>
<div class="sourceCode" id="cb24"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb24-1"><a href="#cb24-1"></a><span class="co">## sklearn verify</span></span>
<span id="cb24-2"><a href="#cb24-2"></a><span class="co"># C = inverse of lambda</span></span>
<span id="cb24-3"><a href="#cb24-3"></a>LAMBDA <span class="op">=</span> <span class="fl">25.0</span></span>
<span id="cb24-4"><a href="#cb24-4"></a>C <span class="op">=</span> <span class="dv">1</span> <span class="op">/</span> LAMBDA</span>
<span id="cb24-5"><a href="#cb24-5"></a>clf <span class="op">=</span> LogisticRegression(solver<span class="op">=</span><span class="st">&#39;lbfgs&#39;</span>, penalty<span class="op">=</span><span class="st">&#39;l2&#39;</span>, C<span class="op">=</span>C).fit(X, y)</span>
<span id="cb24-6"><a href="#cb24-6"></a><span class="bu">print</span>(clf.coef_)</span></code></pre></div>
<pre><code>## [[-0.03849518  2.00255362]]</code></pre>
</div>



<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>
