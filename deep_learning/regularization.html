<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml">

<head>

<meta charset="utf-8" />
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="generator" content="pandoc" />

<meta name="viewport" content="width=device-width, initial-scale=1">

<meta name="author" content="Andrew Fogarty" />

<meta name="date" content="2020-10-06" />

<title>Regularization</title>



<style type="text/css">code{white-space: pre;}</style>
<style type="text/css" data-origin="pandoc">
a.sourceLine { display: inline-block; line-height: 1.25; }
a.sourceLine { pointer-events: none; color: inherit; text-decoration: inherit; }
a.sourceLine:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
a.sourceLine { text-indent: -1em; padding-left: 1em; }
}
pre.numberSource a.sourceLine
  { position: relative; left: -4em; }
pre.numberSource a.sourceLine::before
  { content: attr(title);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; pointer-events: all; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {  }
@media screen {
a.sourceLine::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */

</style>
<script>
// apply pandoc div.sourceCode style to pre.sourceCode instead
(function() {
  var sheets = document.styleSheets;
  for (var i = 0; i < sheets.length; i++) {
    if (sheets[i].ownerNode.dataset["origin"] !== "pandoc") continue;
    try { var rules = sheets[i].cssRules; } catch (e) { continue; }
    for (var j = 0; j < rules.length; j++) {
      var rule = rules[j];
      // check if there is a div.sourceCode rule
      if (rule.type !== rule.STYLE_RULE || rule.selectorText !== "div.sourceCode") continue;
      var style = rule.style.cssText;
      // check if color or background-color is set
      if (rule.style.color === '' || rule.style.backgroundColor === '') continue;
      // replace div.sourceCode by a pre.sourceCode rule
      sheets[i].deleteRule(j);
      sheets[i].insertRule('pre.sourceCode{' + style + '}', j);
    }
  }
})();
</script>



<style type="text/css">body {
background-color: #fff;
margin: 1em auto;
max-width: 700px;
overflow: visible;
padding-left: 2em;
padding-right: 2em;
font-family: "Open Sans", "Helvetica Neue", Helvetica, Arial, sans-serif;
font-size: 14px;
line-height: 1.35;
}
#header {
text-align: center;
}
#TOC {
clear: both;
margin: 0 0 10px 10px;
padding: 4px;
width: 400px;
border: 1px solid #CCCCCC;
border-radius: 5px;
background-color: #f6f6f6;
font-size: 13px;
line-height: 1.3;
}
#TOC .toctitle {
font-weight: bold;
font-size: 15px;
margin-left: 5px;
}
#TOC ul {
padding-left: 40px;
margin-left: -1.5em;
margin-top: 5px;
margin-bottom: 5px;
}
#TOC ul ul {
margin-left: -2em;
}
#TOC li {
line-height: 16px;
}
table {
margin: 1em auto;
border-width: 1px;
border-color: #DDDDDD;
border-style: outset;
border-collapse: collapse;
}
table th {
border-width: 2px;
padding: 5px;
border-style: inset;
}
table td {
border-width: 1px;
border-style: inset;
line-height: 18px;
padding: 5px 5px;
}
table, table th, table td {
border-left-style: none;
border-right-style: none;
}
table thead, table tr.even {
background-color: #f7f7f7;
}
p {
margin: 0.5em 0;
}
blockquote {
background-color: #f6f6f6;
padding: 0.25em 0.75em;
}
hr {
border-style: solid;
border: none;
border-top: 1px solid #777;
margin: 28px 0;
}
dl {
margin-left: 0;
}
dl dd {
margin-bottom: 13px;
margin-left: 13px;
}
dl dt {
font-weight: bold;
}
ul {
margin-top: 0;
}
ul li {
list-style: circle outside;
}
ul ul {
margin-bottom: 0;
}
pre, code {
background-color: #f7f7f7;
border-radius: 3px;
color: #333;
white-space: pre-wrap; 
}
pre {
border-radius: 3px;
margin: 5px 0px 10px 0px;
padding: 10px;
}
pre:not([class]) {
background-color: #f7f7f7;
}
code {
font-family: Consolas, Monaco, 'Courier New', monospace;
font-size: 85%;
}
p > code, li > code {
padding: 2px 0px;
}
div.figure {
text-align: center;
}
img {
background-color: #FFFFFF;
padding: 2px;
border: 1px solid #DDDDDD;
border-radius: 3px;
border: 1px solid #CCCCCC;
margin: 0 5px;
}
h1 {
margin-top: 0;
font-size: 35px;
line-height: 40px;
}
h2 {
border-bottom: 4px solid #f7f7f7;
padding-top: 10px;
padding-bottom: 2px;
font-size: 145%;
}
h3 {
border-bottom: 2px solid #f7f7f7;
padding-top: 10px;
font-size: 120%;
}
h4 {
border-bottom: 1px solid #f7f7f7;
margin-left: 8px;
font-size: 105%;
}
h5, h6 {
border-bottom: 1px solid #ccc;
font-size: 105%;
}
a {
color: #0033dd;
text-decoration: none;
}
a:hover {
color: #6666ff; }
a:visited {
color: #800080; }
a:visited:hover {
color: #BB00BB; }
a[href^="http:"] {
text-decoration: underline; }
a[href^="https:"] {
text-decoration: underline; }

code > span.kw { color: #555; font-weight: bold; } 
code > span.dt { color: #902000; } 
code > span.dv { color: #40a070; } 
code > span.bn { color: #d14; } 
code > span.fl { color: #d14; } 
code > span.ch { color: #d14; } 
code > span.st { color: #d14; } 
code > span.co { color: #888888; font-style: italic; } 
code > span.ot { color: #007020; } 
code > span.al { color: #ff0000; font-weight: bold; } 
code > span.fu { color: #900; font-weight: bold; }  code > span.er { color: #a61717; background-color: #e3d2d2; } 
</style>




</head>

<body>




<h1 class="title toc-ignore">Regularization</h1>
<h4 class="author">Andrew Fogarty</h4>
<h4 class="date">10/06/2020</h4>


<div id="TOC">
<ul>
<li><a href="#introduction"><span class="toc-section-number">1</span> Introduction</a><ul>
<li><a href="#reducing-model-capacity"><span class="toc-section-number">1.1</span> Reducing Model Capacity</a><ul>
<li><a href="#l1l2-regularization"><span class="toc-section-number">1.1.1</span> L1/L2 Regularization</a></li>
<li><a href="#l2-regularization-in-pytorch"><span class="toc-section-number">1.1.2</span> L2 Regularization in PyTorch</a></li>
<li><a href="#manual-l2-regularization-in-pytorch"><span class="toc-section-number">1.1.3</span> Manual: L2 Regularization in PyTorch</a></li>
<li><a href="#automatic-l2-regularization-in-pytorch"><span class="toc-section-number">1.1.4</span> Automatic: L2 Regularization in PyTorch</a></li>
<li><a href="#dropout"><span class="toc-section-number">1.1.5</span> Dropout</a></li>
<li><a href="#dropout-tips"><span class="toc-section-number">1.1.6</span> Dropout Tips</a></li>
<li><a href="#dropout-in-pytorch"><span class="toc-section-number">1.1.7</span> Dropout in PyTorch</a></li>
</ul></li>
<li><a href="#regularized-models-in-practice"><span class="toc-section-number">1.2</span> Regularized Models in Practice</a><ul>
<li><a href="#l2-norm"><span class="toc-section-number">1.2.1</span> L2 Norm</a></li>
<li><a href="#dropout-1"><span class="toc-section-number">1.2.2</span> Dropout</a></li>
</ul></li>
</ul></li>
<li><a href="#sources"><span class="toc-section-number">2</span> Sources</a></li>
</ul>
</div>

<div class="sourceCode" id="cb1"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1-1" title="1"><span class="co"># load python</span></a>
<a class="sourceLine" id="cb1-2" title="2"><span class="kw">library</span>(reticulate)</a>
<a class="sourceLine" id="cb1-3" title="3"><span class="kw">use_condaenv</span>(<span class="st">&quot;my_ml&quot;</span>)</a></code></pre></div>
<div class="sourceCode" id="cb2"><pre class="sourceCode python"><code class="sourceCode python"><a class="sourceLine" id="cb2-1" title="1"><span class="co"># load packages</span></a>
<a class="sourceLine" id="cb2-2" title="2"><span class="im">import</span> sys</a>
<a class="sourceLine" id="cb2-3" title="3">sys.path.append(<span class="st">&quot;C:/Users/Andrew/Desktop/Projects/Deep Learning/utils&quot;</span>)  <span class="co"># this is the folder with py files</span></a>
<a class="sourceLine" id="cb2-4" title="4"><span class="im">from</span> tools <span class="im">import</span> AverageMeter, ProgressBar <span class="co">#scriptName without .py extension; import each class</span></a>
<a class="sourceLine" id="cb2-5" title="5"><span class="im">from</span> radam <span class="im">import</span> RAdam</a>
<a class="sourceLine" id="cb2-6" title="6"><span class="im">import</span> torch</a>
<a class="sourceLine" id="cb2-7" title="7"><span class="im">import</span> torch.nn <span class="im">as</span> nn</a>
<a class="sourceLine" id="cb2-8" title="8"><span class="im">import</span> torch.nn.functional <span class="im">as</span> F</a>
<a class="sourceLine" id="cb2-9" title="9"><span class="im">from</span> transformers <span class="im">import</span> get_linear_schedule_with_warmup, AdamW</a>
<a class="sourceLine" id="cb2-10" title="10"><span class="im">from</span> torch.utils.data <span class="im">import</span> TensorDataset, random_split, DataLoader, RandomSampler, SequentialSampler</a>
<a class="sourceLine" id="cb2-11" title="11"><span class="im">import</span> time, datetime, random, re, os</a>
<a class="sourceLine" id="cb2-12" title="12"><span class="im">import</span> pandas <span class="im">as</span> pd</a>
<a class="sourceLine" id="cb2-13" title="13"><span class="im">import</span> numpy <span class="im">as</span> np</a>
<a class="sourceLine" id="cb2-14" title="14"><span class="im">from</span> sklearn.metrics <span class="im">import</span> accuracy_score, f1_score, precision_score, recall_score, confusion_matrix</a>
<a class="sourceLine" id="cb2-15" title="15"><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</a>
<a class="sourceLine" id="cb2-16" title="16"><span class="im">import</span> seaborn <span class="im">as</span> sns</a>
<a class="sourceLine" id="cb2-17" title="17"><span class="im">from</span> torch.cuda.amp <span class="im">import</span> autocast, GradScaler</a>
<a class="sourceLine" id="cb2-18" title="18"><span class="im">from</span> sklearn.model_selection <span class="im">import</span> train_test_split</a>
<a class="sourceLine" id="cb2-19" title="19"><span class="im">from</span> torch.utils.data <span class="im">import</span> Dataset, Subset</a>
<a class="sourceLine" id="cb2-20" title="20"><span class="im">from</span> sklearn.preprocessing <span class="im">import</span> LabelEncoder</a>
<a class="sourceLine" id="cb2-21" title="21"><span class="im">from</span> torchvision <span class="im">import</span> transforms</a>
<a class="sourceLine" id="cb2-22" title="22"></a>
<a class="sourceLine" id="cb2-23" title="23"><span class="co"># set seed and gpu requirements</span></a>
<a class="sourceLine" id="cb2-24" title="24">SEED <span class="op">=</span> <span class="dv">15</span></a>
<a class="sourceLine" id="cb2-25" title="25">random.seed(SEED)</a>
<a class="sourceLine" id="cb2-26" title="26">np.random.seed(SEED)</a>
<a class="sourceLine" id="cb2-27" title="27">torch.manual_seed(SEED)</a></code></pre></div>
<pre><code>## &lt;torch._C.Generator object at 0x0000000021E70370&gt;</code></pre>
<div class="sourceCode" id="cb4"><pre class="sourceCode python"><code class="sourceCode python"><a class="sourceLine" id="cb4-1" title="1">torch.backends.cudnn.deterministic <span class="op">=</span> <span class="va">True</span></a>
<a class="sourceLine" id="cb4-2" title="2">torch.cuda.amp.autocast(enabled<span class="op">=</span><span class="va">True</span>)</a>
<a class="sourceLine" id="cb4-3" title="3"></a>
<a class="sourceLine" id="cb4-4" title="4"><span class="co"># set gpu/cpu</span></a></code></pre></div>
<pre><code>## &lt;torch.cuda.amp.autocast_mode.autocast object at 0x0000000033ACEBC8&gt;</code></pre>
<div class="sourceCode" id="cb6"><pre class="sourceCode python"><code class="sourceCode python"><a class="sourceLine" id="cb6-1" title="1">DEVICE <span class="op">=</span> torch.device(<span class="st">&#39;cuda:0&#39;</span> <span class="cf">if</span> torch.cuda.is_available() <span class="cf">else</span> <span class="st">&#39;cpu&#39;</span>)</a></code></pre></div>
<div id="introduction" class="section level1">
<h1><span class="header-section-number">1</span> Introduction</h1>
<p>We can think of regularization as a general means to reduce overfitting. In practice, this means that regularizing effects aim to reduce the model capacity and/or the variance of the predictions. While the best way to reduce overfitting is to acquire more data, data augmentation (e.g., for images: random rotation, crop, etc) is the next best thing. Barring those possibilities, we can reduce the capacity of the model through regularization methods.</p>
<div id="reducing-model-capacity" class="section level2">
<h2><span class="header-section-number">1.1</span> Reducing Model Capacity</h2>
<p>We can reduce the capacity of model in the following ways:</p>
<ol style="list-style-type: decimal">
<li><p>Smaller architecture: fewer hidden layers, neurons, and dropout</p></li>
<li><p>Smaller weights: early stopping (not very common anymore) and L1/L2 norm penalties</p></li>
<li><p>Adding Noise: dropout</p></li>
</ol>
<div id="l1l2-regularization" class="section level3">
<h3><span class="header-section-number">1.1.1</span> L1/L2 Regularization</h3>
<p>These methods shrink the weights and can be thought of as penalties against model complexity.</p>
<ul>
<li><p>L1 Regularization – encourages sparsity but does not work well in practice because its not smooth and is hard to optimize.</p></li>
<li><p>L2 Regularization – shrinks weights. Large regularization yields high bias while low regularization yields high variance. We want to aim for something in between.</p></li>
</ul>
</div>
<div id="l2-regularization-in-pytorch" class="section level3">
<h3><span class="header-section-number">1.1.2</span> L2 Regularization in PyTorch</h3>
<p>In this section, we will apply L2 regularization both manually and through PyTorch. To apply L2 regularization through PyTorch, we simply manipulate <code>weight_decay</code> in the optimizer instance. To apply L2 regularization manually, we manipulate our cost function before sending it through backward propagation and our optimizer.</p>
<p>It is important to note that when you use <code>weight_decay</code>, PyTorch will also regularize the intercept. I do not think we should regularize the bias term because if we consider linear regression, we have an intercept term because we want to consider all possible lines. Without an intercept, a line can only be fit through the origin which is of course limiting to the hypothesis space of our model.</p>
</div>
<div id="manual-l2-regularization-in-pytorch" class="section level3">
<h3><span class="header-section-number">1.1.3</span> Manual: L2 Regularization in PyTorch</h3>
<p>To see how we would apply L2 regularization manually, consider the following familiar training function from the multilayer perceptron demonstration.</p>
<div class="sourceCode" id="cb7"><pre class="sourceCode python"><code class="sourceCode python"><a class="sourceLine" id="cb7-1" title="1"><span class="co"># train function</span></a>
<a class="sourceLine" id="cb7-2" title="2"><span class="kw">def</span> train(dataloader):</a>
<a class="sourceLine" id="cb7-3" title="3">    pbar <span class="op">=</span> ProgressBar(n_total<span class="op">=</span><span class="bu">len</span>(dataloader), desc<span class="op">=</span><span class="st">&#39;Training&#39;</span>)</a>
<a class="sourceLine" id="cb7-4" title="4">    train_loss <span class="op">=</span> AverageMeter()</a>
<a class="sourceLine" id="cb7-5" title="5">    model.train()</a>
<a class="sourceLine" id="cb7-6" title="6">    <span class="cf">for</span> batch_idx, batch <span class="kw">in</span> <span class="bu">enumerate</span>(dataloader):</a>
<a class="sourceLine" id="cb7-7" title="7">        b_features, b_target, b_idx <span class="op">=</span> batch[<span class="st">&#39;features&#39;</span>].to(DEVICE),  batch[<span class="st">&#39;target&#39;</span>].to(DEVICE), batch[<span class="st">&#39;idx&#39;</span>].to(DEVICE)</a>
<a class="sourceLine" id="cb7-8" title="8">        optimizer.zero_grad()</a>
<a class="sourceLine" id="cb7-9" title="9">        <span class="cf">with</span> autocast():</a>
<a class="sourceLine" id="cb7-10" title="10">            logits, probs <span class="op">=</span> model(b_features)</a>
<a class="sourceLine" id="cb7-11" title="11">            loss <span class="op">=</span> F.cross_entropy(logits, b_target)</a>
<a class="sourceLine" id="cb7-12" title="12">            <span class="co"># regularize loss -- but not the intercept</span></a>
<a class="sourceLine" id="cb7-13" title="13">            LAMBDA, L2 <span class="op">=</span> <span class="dv">2</span>, <span class="fl">0.</span></a>
<a class="sourceLine" id="cb7-14" title="14">            <span class="cf">for</span> name, p <span class="kw">in</span> model.named_parameters():</a>
<a class="sourceLine" id="cb7-15" title="15">                <span class="cf">if</span> <span class="st">&#39;weight&#39;</span> <span class="kw">in</span> name:</a>
<a class="sourceLine" id="cb7-16" title="16">                    L2 <span class="op">=</span> L2 <span class="op">+</span> (p<span class="op">**</span><span class="dv">2</span>).<span class="bu">sum</span>()</a>
<a class="sourceLine" id="cb7-17" title="17">            loss <span class="op">=</span> loss <span class="op">+</span> <span class="fl">2.</span><span class="op">/</span>b_target.size(<span class="dv">0</span>) <span class="op">*</span> LAMBDA <span class="op">*</span> L2</a>
<a class="sourceLine" id="cb7-18" title="18">        scaler.scale(loss).backward()</a>
<a class="sourceLine" id="cb7-19" title="19">        scaler.step(optimizer)</a>
<a class="sourceLine" id="cb7-20" title="20">        scaler.update()</a>
<a class="sourceLine" id="cb7-21" title="21">        scheduler.step()</a>
<a class="sourceLine" id="cb7-22" title="22">        pbar(step<span class="op">=</span>batch_idx, info<span class="op">=</span>{<span class="st">&#39;loss&#39;</span>: loss.item()})</a>
<a class="sourceLine" id="cb7-23" title="23">        train_loss.update(loss.item(), n<span class="op">=</span><span class="dv">1</span>)</a>
<a class="sourceLine" id="cb7-24" title="24">    <span class="cf">return</span> {<span class="st">&#39;loss&#39;</span>: train_loss.avg}</a></code></pre></div>
</div>
<div id="automatic-l2-regularization-in-pytorch" class="section level3">
<h3><span class="header-section-number">1.1.4</span> Automatic: L2 Regularization in PyTorch</h3>
<div class="sourceCode" id="cb8"><pre class="sourceCode python"><code class="sourceCode python"><a class="sourceLine" id="cb8-1" title="1">LAMBDA <span class="op">=</span> <span class="dv">2</span></a>
<a class="sourceLine" id="cb8-2" title="2">optimizer <span class="op">=</span> torch.optim.SGD(model.parameters(), lr<span class="op">=</span><span class="fl">0.1</span>, weight_decay<span class="op">=</span>LAMBDA)</a></code></pre></div>
</div>
<div id="dropout" class="section level3">
<h3><span class="header-section-number">1.1.5</span> Dropout</h3>
<p>Dropout effectively drops neurons by relying on Bernoulli sampling to choose neurons to drop. Dropout works very well because:</p>
<ol style="list-style-type: decimal">
<li><p>Our model will learn to not rely on particular connections heavily</p></li>
<li><p>Our model will consider more connections</p></li>
<li><p>The weight values will be more spread out and perhaps some smaller weights like L2 norm</p></li>
</ol>
<p>In practice, can use different dropout probabilities for different layers and we should probably assign dropout probabilities proportional to the number of neurons in a layer. PyTorch uses inverted dropout which scales the activation values by the factor <span class="math inline">\(\frac{1}{1-p}\)</span> during training instead of inference. Thus, we need to set <code>model.train()</code> and <code>model.eval()</code> appropriately in our training and validation functions.</p>
</div>
<div id="dropout-tips" class="section level3">
<h3><span class="header-section-number">1.1.6</span> Dropout Tips</h3>
<p>In practice, do not use dropout if the model does not overfit. However, in the case that your model does not overfit, it is then recommended that you increase the model’s capacity to make it overfit so that you may then use dropout to retain the benefits of a larger capacity model while constraining its ability to overfit.</p>
</div>
<div id="dropout-in-pytorch" class="section level3">
<h3><span class="header-section-number">1.1.7</span> Dropout in PyTorch</h3>
<div class="sourceCode" id="cb9"><pre class="sourceCode python"><code class="sourceCode python"><a class="sourceLine" id="cb9-1" title="1"><span class="co"># create NN</span></a>
<a class="sourceLine" id="cb9-2" title="2"><span class="co"># nn.Module tells PyTorch to do backward propagation</span></a>
<a class="sourceLine" id="cb9-3" title="3"><span class="kw">class</span> FF_NN(torch.nn.Module):</a>
<a class="sourceLine" id="cb9-4" title="4">    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, num_features, num_classes, drop_prob):</a>
<a class="sourceLine" id="cb9-5" title="5">        <span class="bu">super</span>(FF_NN, <span class="va">self</span>).<span class="fu">__init__</span>()</a>
<a class="sourceLine" id="cb9-6" title="6"></a>
<a class="sourceLine" id="cb9-7" title="7">        <span class="co"># first hidden layer</span></a>
<a class="sourceLine" id="cb9-8" title="8">        <span class="va">self</span>.linear_1 <span class="op">=</span> torch.nn.Linear(num_features, num_hidden_1)</a>
<a class="sourceLine" id="cb9-9" title="9">        <span class="co"># second hidden layer</span></a>
<a class="sourceLine" id="cb9-10" title="10">        <span class="va">self</span>.linear_2 <span class="op">=</span> torch.nn.Linear(num_hidden_1, num_hidden_2)</a>
<a class="sourceLine" id="cb9-11" title="11">        <span class="co"># output layer</span></a>
<a class="sourceLine" id="cb9-12" title="12">        <span class="va">self</span>.linear_out <span class="op">=</span> torch.nn.Linear(num_hidden_2, num_classes)</a>
<a class="sourceLine" id="cb9-13" title="13">        <span class="co"># dropout</span></a>
<a class="sourceLine" id="cb9-14" title="14">        <span class="va">self</span>.drop_prob <span class="op">=</span> drop_prob</a>
<a class="sourceLine" id="cb9-15" title="15"></a>
<a class="sourceLine" id="cb9-16" title="16">    <span class="co"># define how and what order model parameters should be used in forward prop.</span></a>
<a class="sourceLine" id="cb9-17" title="17">    <span class="kw">def</span> forward(<span class="va">self</span>, x):</a>
<a class="sourceLine" id="cb9-18" title="18">        <span class="co"># run inputs through first layer</span></a>
<a class="sourceLine" id="cb9-19" title="19">        out <span class="op">=</span> <span class="va">self</span>.linear_1(x)</a>
<a class="sourceLine" id="cb9-20" title="20">        <span class="co"># apply relu</span></a>
<a class="sourceLine" id="cb9-21" title="21">        out <span class="op">=</span> F.relu(out)</a>
<a class="sourceLine" id="cb9-22" title="22">        <span class="co"># apply dropout</span></a>
<a class="sourceLine" id="cb9-23" title="23">        out <span class="op">=</span> F.dropout(p<span class="op">=</span><span class="va">self</span>.drop_prob)</a>
<a class="sourceLine" id="cb9-24" title="24">        <span class="co"># run inputs through second layer</span></a>
<a class="sourceLine" id="cb9-25" title="25">        out <span class="op">=</span> <span class="va">self</span>.linear_2(out)</a>
<a class="sourceLine" id="cb9-26" title="26">        <span class="co"># apply relu</span></a>
<a class="sourceLine" id="cb9-27" title="27">        out <span class="op">=</span> F.relu(out)</a>
<a class="sourceLine" id="cb9-28" title="28">        <span class="co"># apply dropout</span></a>
<a class="sourceLine" id="cb9-29" title="29">        out <span class="op">=</span> F.dropout(p<span class="op">=</span><span class="va">self</span>.drop_prob)</a>
<a class="sourceLine" id="cb9-30" title="30">        <span class="co"># run inputs through final classification layer</span></a>
<a class="sourceLine" id="cb9-31" title="31">        logits <span class="op">=</span> <span class="va">self</span>.linear_out(out)</a>
<a class="sourceLine" id="cb9-32" title="32">        probs <span class="op">=</span> F.log_softmax(logits, dim<span class="op">=</span><span class="dv">1</span>)</a>
<a class="sourceLine" id="cb9-33" title="33">        <span class="cf">return</span> logits, probs</a></code></pre></div>
</div>
</div>
<div id="regularized-models-in-practice" class="section level2">
<h2><span class="header-section-number">1.2</span> Regularized Models in Practice</h2>
<p>In this section, we will apply the regularized functions on the MNIST data set.</p>
<div id="l2-norm" class="section level3">
<h3><span class="header-section-number">1.2.1</span> L2 Norm</h3>
<div class="sourceCode" id="cb10"><pre class="sourceCode python"><code class="sourceCode python"><a class="sourceLine" id="cb10-1" title="1"></a>
<a class="sourceLine" id="cb10-2" title="2"><span class="co"># create Dataset</span></a>
<a class="sourceLine" id="cb10-3" title="3"><span class="kw">class</span> CSVDataset(Dataset):</a>
<a class="sourceLine" id="cb10-4" title="4">    <span class="co">&quot;&quot;&quot;MNIST dataset.&quot;&quot;&quot;</span></a>
<a class="sourceLine" id="cb10-5" title="5"></a>
<a class="sourceLine" id="cb10-6" title="6">    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, csv_file, transform<span class="op">=</span><span class="va">None</span>):</a>
<a class="sourceLine" id="cb10-7" title="7">        <span class="co">&quot;&quot;&quot;</span></a>
<a class="sourceLine" id="cb10-8" title="8"><span class="co">        Args:</span></a>
<a class="sourceLine" id="cb10-9" title="9"><span class="co">            csv_file (string): Path to the csv file.</span></a>
<a class="sourceLine" id="cb10-10" title="10"><span class="co">            transform (callable, optional): Optional transform to be applied</span></a>
<a class="sourceLine" id="cb10-11" title="11"><span class="co">                on a sample.</span></a>
<a class="sourceLine" id="cb10-12" title="12"><span class="co">        &quot;&quot;&quot;</span></a>
<a class="sourceLine" id="cb10-13" title="13">        <span class="co"># initialize</span></a>
<a class="sourceLine" id="cb10-14" title="14">        <span class="va">self</span>.data_frame <span class="op">=</span> pd.read_csv(csv_file)</a>
<a class="sourceLine" id="cb10-15" title="15">        <span class="co"># all columns but the last</span></a>
<a class="sourceLine" id="cb10-16" title="16">        <span class="va">self</span>.features <span class="op">=</span> <span class="va">self</span>.data_frame[<span class="va">self</span>.data_frame.columns[:<span class="op">-</span><span class="dv">1</span>]]</a>
<a class="sourceLine" id="cb10-17" title="17">        <span class="co"># the last column</span></a>
<a class="sourceLine" id="cb10-18" title="18">        <span class="va">self</span>.target <span class="op">=</span> <span class="va">self</span>.data_frame[<span class="va">self</span>.data_frame.columns[<span class="op">-</span><span class="dv">1</span>]]</a>
<a class="sourceLine" id="cb10-19" title="19">        <span class="co"># initialize the transform if specified</span></a>
<a class="sourceLine" id="cb10-20" title="20">        <span class="va">self</span>.transform <span class="op">=</span> transform</a>
<a class="sourceLine" id="cb10-21" title="21"></a>
<a class="sourceLine" id="cb10-22" title="22">        <span class="co"># get length of df</span></a>
<a class="sourceLine" id="cb10-23" title="23">    <span class="kw">def</span> <span class="fu">__len__</span>(<span class="va">self</span>):</a>
<a class="sourceLine" id="cb10-24" title="24">        <span class="cf">return</span> <span class="bu">len</span>(<span class="va">self</span>.data_frame)</a>
<a class="sourceLine" id="cb10-25" title="25"></a>
<a class="sourceLine" id="cb10-26" title="26">        <span class="co"># get sample target</span></a>
<a class="sourceLine" id="cb10-27" title="27">    <span class="kw">def</span> __get_target__(<span class="va">self</span>):</a>
<a class="sourceLine" id="cb10-28" title="28">        <span class="cf">return</span> <span class="va">self</span>.target</a>
<a class="sourceLine" id="cb10-29" title="29"></a>
<a class="sourceLine" id="cb10-30" title="30">        <span class="co"># get df filtered by indices</span></a>
<a class="sourceLine" id="cb10-31" title="31">    <span class="kw">def</span> __get_values__(<span class="va">self</span>, indices):</a>
<a class="sourceLine" id="cb10-32" title="32">        <span class="cf">return</span> <span class="va">self</span>.data_frame.iloc[indices]</a>
<a class="sourceLine" id="cb10-33" title="33"></a>
<a class="sourceLine" id="cb10-34" title="34">    <span class="kw">def</span> <span class="fu">__getitem__</span>(<span class="va">self</span>, idx):</a>
<a class="sourceLine" id="cb10-35" title="35">        <span class="cf">if</span> torch.is_tensor(idx):</a>
<a class="sourceLine" id="cb10-36" title="36">            idx <span class="op">=</span> idx.tolist()</a>
<a class="sourceLine" id="cb10-37" title="37"></a>
<a class="sourceLine" id="cb10-38" title="38">        <span class="co"># pull a sample in a dict</span></a>
<a class="sourceLine" id="cb10-39" title="39">        sample <span class="op">=</span> {<span class="st">&#39;features&#39;</span>: torch.tensor(<span class="va">self</span>.features.iloc[idx].values),</a>
<a class="sourceLine" id="cb10-40" title="40">                  <span class="st">&#39;target&#39;</span>: torch.tensor(<span class="va">self</span>.target.iloc[idx]),</a>
<a class="sourceLine" id="cb10-41" title="41">                  <span class="st">&#39;idx&#39;</span>: torch.tensor(idx)}</a>
<a class="sourceLine" id="cb10-42" title="42"></a>
<a class="sourceLine" id="cb10-43" title="43">        <span class="cf">if</span> <span class="va">self</span>.transform:</a>
<a class="sourceLine" id="cb10-44" title="44">            sample <span class="op">=</span> <span class="va">self</span>.transform(sample)</a>
<a class="sourceLine" id="cb10-45" title="45"></a>
<a class="sourceLine" id="cb10-46" title="46">        <span class="cf">return</span> sample</a>
<a class="sourceLine" id="cb10-47" title="47"></a>
<a class="sourceLine" id="cb10-48" title="48"></a>
<a class="sourceLine" id="cb10-49" title="49"><span class="kw">class</span> Pixel_Normalize():</a>
<a class="sourceLine" id="cb10-50" title="50"></a>
<a class="sourceLine" id="cb10-51" title="51">    <span class="co"># retrieve sample and unpack it</span></a>
<a class="sourceLine" id="cb10-52" title="52">    <span class="kw">def</span> <span class="fu">__call__</span>(<span class="va">self</span>, sample):</a>
<a class="sourceLine" id="cb10-53" title="53">        features, target, idx <span class="op">=</span> (sample[<span class="st">&#39;features&#39;</span>],</a>
<a class="sourceLine" id="cb10-54" title="54">                              sample[<span class="st">&#39;target&#39;</span>],</a>
<a class="sourceLine" id="cb10-55" title="55">                              sample[<span class="st">&#39;idx&#39;</span>])</a>
<a class="sourceLine" id="cb10-56" title="56"></a>
<a class="sourceLine" id="cb10-57" title="57">        <span class="co"># normalize each pixel</span></a>
<a class="sourceLine" id="cb10-58" title="58">        normalized_pixels <span class="op">=</span> torch.true_divide(sample[<span class="st">&#39;features&#39;</span>], <span class="dv">255</span>)</a>
<a class="sourceLine" id="cb10-59" title="59"></a>
<a class="sourceLine" id="cb10-60" title="60">        <span class="co"># yield another dict</span></a>
<a class="sourceLine" id="cb10-61" title="61">        <span class="cf">return</span> {<span class="st">&#39;features&#39;</span>: normalized_pixels,</a>
<a class="sourceLine" id="cb10-62" title="62">                <span class="st">&#39;target&#39;</span>: target,</a>
<a class="sourceLine" id="cb10-63" title="63">                <span class="st">&#39;idx&#39;</span>: idx}</a>
<a class="sourceLine" id="cb10-64" title="64"></a>
<a class="sourceLine" id="cb10-65" title="65"></a>
<a class="sourceLine" id="cb10-66" title="66"><span class="co"># instantiate the lazy data set</span></a>
<a class="sourceLine" id="cb10-67" title="67">csv_dataset <span class="op">=</span> CSVDataset(csv_file<span class="op">=</span><span class="st">&#39;https://datahub.io/machine-learning/mnist_784/r/mnist_784.csv&#39;</span>,</a>
<a class="sourceLine" id="cb10-68" title="68">                         transform<span class="op">=</span>Pixel_Normalize())</a>
<a class="sourceLine" id="cb10-69" title="69"></a>
<a class="sourceLine" id="cb10-70" title="70"><span class="co"># set train, valid, and test size</span></a>
<a class="sourceLine" id="cb10-71" title="71">train_size <span class="op">=</span> <span class="bu">int</span>(<span class="fl">0.8</span> <span class="op">*</span> <span class="bu">len</span>(csv_dataset))</a>
<a class="sourceLine" id="cb10-72" title="72">valid_size <span class="op">=</span> <span class="bu">int</span>(<span class="fl">0.1</span> <span class="op">*</span> <span class="bu">len</span>(csv_dataset))</a>
<a class="sourceLine" id="cb10-73" title="73"></a>
<a class="sourceLine" id="cb10-74" title="74"><span class="co"># use random split to create three data sets;</span></a>
<a class="sourceLine" id="cb10-75" title="75">train_ds, valid_ds, test_ds <span class="op">=</span> torch.utils.data.random_split(csv_dataset, [train_size, valid_size, valid_size])</a>
<a class="sourceLine" id="cb10-76" title="76"></a>
<a class="sourceLine" id="cb10-77" title="77"><span class="co"># check the output</span></a>
<a class="sourceLine" id="cb10-78" title="78"><span class="cf">for</span> i, batch <span class="kw">in</span> <span class="bu">enumerate</span>(train_ds):</a>
<a class="sourceLine" id="cb10-79" title="79">    <span class="cf">if</span> i <span class="op">==</span> <span class="dv">0</span>:</a>
<a class="sourceLine" id="cb10-80" title="80">        <span class="cf">break</span></a>
<a class="sourceLine" id="cb10-81" title="81"></a>
<a class="sourceLine" id="cb10-82" title="82"></a>
<a class="sourceLine" id="cb10-83" title="83"><span class="co"># check the distribution of dependent variable; some imbalance</span></a>
<a class="sourceLine" id="cb10-84" title="84">csv_dataset.__get_target__().value_counts()</a>
<a class="sourceLine" id="cb10-85" title="85"></a>
<a class="sourceLine" id="cb10-86" title="86"></a>
<a class="sourceLine" id="cb10-87" title="87"><span class="co"># prepare weighted sampling for imbalanced classification</span></a></code></pre></div>
<pre><code>## 1    7877
## 7    7293
## 3    7141
## 2    6990
## 9    6958
## 0    6903
## 6    6876
## 8    6825
## 4    6824
## 5    6313
## Name: class, dtype: int64</code></pre>
<div class="sourceCode" id="cb12"><pre class="sourceCode python"><code class="sourceCode python"><a class="sourceLine" id="cb12-1" title="1"><span class="kw">def</span> create_sampler(train_ds, csv_dataset):</a>
<a class="sourceLine" id="cb12-2" title="2">    <span class="co"># get indicies from train split</span></a>
<a class="sourceLine" id="cb12-3" title="3">    train_indices <span class="op">=</span> train_ds.indices</a>
<a class="sourceLine" id="cb12-4" title="4">    <span class="co"># generate class distributions [y1, y2, etc...]</span></a>
<a class="sourceLine" id="cb12-5" title="5">    bin_count <span class="op">=</span> np.bincount(csv_dataset.__get_target__()[train_indices])</a>
<a class="sourceLine" id="cb12-6" title="6">    <span class="co"># weight gen</span></a>
<a class="sourceLine" id="cb12-7" title="7">    weight <span class="op">=</span> <span class="fl">1.</span> <span class="op">/</span> bin_count.astype(np.float32)</a>
<a class="sourceLine" id="cb12-8" title="8">    <span class="co"># produce weights for each observation in the data set</span></a>
<a class="sourceLine" id="cb12-9" title="9">    samples_weight <span class="op">=</span> torch.tensor([weight[t] <span class="cf">for</span> t <span class="kw">in</span> csv_dataset.__get_target__()[train_indices]])</a>
<a class="sourceLine" id="cb12-10" title="10">    <span class="co"># prepare sampler</span></a>
<a class="sourceLine" id="cb12-11" title="11">    sampler <span class="op">=</span> torch.utils.data.WeightedRandomSampler(weights<span class="op">=</span>samples_weight,</a>
<a class="sourceLine" id="cb12-12" title="12">                                                     num_samples<span class="op">=</span><span class="bu">len</span>(samples_weight),</a>
<a class="sourceLine" id="cb12-13" title="13">                                                     replacement<span class="op">=</span><span class="va">True</span>)</a>
<a class="sourceLine" id="cb12-14" title="14">    <span class="cf">return</span> sampler</a>
<a class="sourceLine" id="cb12-15" title="15"></a>
<a class="sourceLine" id="cb12-16" title="16"></a>
<a class="sourceLine" id="cb12-17" title="17"><span class="co"># create sampler for the training ds</span></a>
<a class="sourceLine" id="cb12-18" title="18">train_sampler <span class="op">=</span> create_sampler(train_ds, csv_dataset)</a>
<a class="sourceLine" id="cb12-19" title="19"></a>
<a class="sourceLine" id="cb12-20" title="20"><span class="co"># create NN</span></a>
<a class="sourceLine" id="cb12-21" title="21"><span class="co"># nn.Module tells PyTorch to do backward propagation</span></a>
<a class="sourceLine" id="cb12-22" title="22"><span class="kw">class</span> FF_NN(torch.nn.Module):</a>
<a class="sourceLine" id="cb12-23" title="23">    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, num_features, num_classes):</a>
<a class="sourceLine" id="cb12-24" title="24">        <span class="bu">super</span>(FF_NN, <span class="va">self</span>).<span class="fu">__init__</span>()</a>
<a class="sourceLine" id="cb12-25" title="25">        <span class="co"># initialize 3 layers</span></a>
<a class="sourceLine" id="cb12-26" title="26">        <span class="co"># first hidden layer</span></a>
<a class="sourceLine" id="cb12-27" title="27">        <span class="va">self</span>.linear_1 <span class="op">=</span> torch.nn.Linear(num_features, num_hidden_1)</a>
<a class="sourceLine" id="cb12-28" title="28">        <span class="co"># second hidden layer</span></a>
<a class="sourceLine" id="cb12-29" title="29">        <span class="va">self</span>.linear_2 <span class="op">=</span> torch.nn.Linear(num_hidden_1, num_hidden_2)</a>
<a class="sourceLine" id="cb12-30" title="30">        <span class="co"># output layer</span></a>
<a class="sourceLine" id="cb12-31" title="31">        <span class="va">self</span>.linear_out <span class="op">=</span> torch.nn.Linear(num_hidden_2, num_classes)</a>
<a class="sourceLine" id="cb12-32" title="32"></a>
<a class="sourceLine" id="cb12-33" title="33">    <span class="co"># define how and what order model parameters should be used in forward prop.</span></a>
<a class="sourceLine" id="cb12-34" title="34">    <span class="kw">def</span> forward(<span class="va">self</span>, x):</a>
<a class="sourceLine" id="cb12-35" title="35">        <span class="co"># run inputs through first layer</span></a>
<a class="sourceLine" id="cb12-36" title="36">        out <span class="op">=</span> <span class="va">self</span>.linear_1(x)</a>
<a class="sourceLine" id="cb12-37" title="37">        <span class="co"># apply relu</span></a>
<a class="sourceLine" id="cb12-38" title="38">        out <span class="op">=</span> F.relu(out)</a>
<a class="sourceLine" id="cb12-39" title="39">        <span class="co"># run inputs through second layer</span></a>
<a class="sourceLine" id="cb12-40" title="40">        out <span class="op">=</span> <span class="va">self</span>.linear_2(out)</a>
<a class="sourceLine" id="cb12-41" title="41">        <span class="co"># apply relu</span></a>
<a class="sourceLine" id="cb12-42" title="42">        out <span class="op">=</span> F.relu(out)</a>
<a class="sourceLine" id="cb12-43" title="43">        <span class="co"># run inputs through final classification layer</span></a>
<a class="sourceLine" id="cb12-44" title="44">        logits <span class="op">=</span> <span class="va">self</span>.linear_out(out)</a>
<a class="sourceLine" id="cb12-45" title="45">        probs <span class="op">=</span> F.log_softmax(logits, dim<span class="op">=</span><span class="dv">1</span>)</a>
<a class="sourceLine" id="cb12-46" title="46">        <span class="cf">return</span> logits, probs</a>
<a class="sourceLine" id="cb12-47" title="47"></a>
<a class="sourceLine" id="cb12-48" title="48"></a>
<a class="sourceLine" id="cb12-49" title="49"><span class="co"># load the NN model</span></a>
<a class="sourceLine" id="cb12-50" title="50">num_features <span class="op">=</span> <span class="dv">784</span></a>
<a class="sourceLine" id="cb12-51" title="51">num_hidden_1 <span class="op">=</span> <span class="dv">128</span></a>
<a class="sourceLine" id="cb12-52" title="52">num_hidden_2 <span class="op">=</span> <span class="dv">256</span></a>
<a class="sourceLine" id="cb12-53" title="53">num_classes <span class="op">=</span> <span class="dv">10</span></a>
<a class="sourceLine" id="cb12-54" title="54">model <span class="op">=</span> FF_NN(num_features<span class="op">=</span>num_features, num_classes<span class="op">=</span>num_classes).to(DEVICE)</a>
<a class="sourceLine" id="cb12-55" title="55"></a>
<a class="sourceLine" id="cb12-56" title="56"></a>
<a class="sourceLine" id="cb12-57" title="57"><span class="co"># optimizer</span></a>
<a class="sourceLine" id="cb12-58" title="58">optimizer <span class="op">=</span> RAdam(model.parameters(), lr<span class="op">=</span><span class="fl">0.1</span>)</a>
<a class="sourceLine" id="cb12-59" title="59"></a>
<a class="sourceLine" id="cb12-60" title="60"><span class="co"># set number of epochs</span></a>
<a class="sourceLine" id="cb12-61" title="61">epochs <span class="op">=</span> <span class="dv">4</span></a>
<a class="sourceLine" id="cb12-62" title="62"></a>
<a class="sourceLine" id="cb12-63" title="63"></a>
<a class="sourceLine" id="cb12-64" title="64"><span class="co"># create DataLoaders with samplers</span></a>
<a class="sourceLine" id="cb12-65" title="65">train_dataloader <span class="op">=</span> DataLoader(train_ds,</a>
<a class="sourceLine" id="cb12-66" title="66">                              batch_size<span class="op">=</span><span class="dv">100</span>,</a>
<a class="sourceLine" id="cb12-67" title="67">                              sampler<span class="op">=</span>train_sampler,</a>
<a class="sourceLine" id="cb12-68" title="68">                              shuffle<span class="op">=</span><span class="va">False</span>)</a>
<a class="sourceLine" id="cb12-69" title="69"></a>
<a class="sourceLine" id="cb12-70" title="70">valid_dataloader <span class="op">=</span> DataLoader(valid_ds,</a>
<a class="sourceLine" id="cb12-71" title="71">                              batch_size<span class="op">=</span><span class="dv">100</span>,</a>
<a class="sourceLine" id="cb12-72" title="72">                              shuffle<span class="op">=</span><span class="va">True</span>)</a>
<a class="sourceLine" id="cb12-73" title="73"></a>
<a class="sourceLine" id="cb12-74" title="74">test_dataloader <span class="op">=</span> DataLoader(test_ds,</a>
<a class="sourceLine" id="cb12-75" title="75">                              batch_size<span class="op">=</span><span class="dv">100</span>,</a>
<a class="sourceLine" id="cb12-76" title="76">                              shuffle<span class="op">=</span><span class="va">True</span>)</a>
<a class="sourceLine" id="cb12-77" title="77"></a>
<a class="sourceLine" id="cb12-78" title="78"><span class="co"># set LR scheduler</span></a>
<a class="sourceLine" id="cb12-79" title="79">scheduler <span class="op">=</span> torch.optim.lr_scheduler.OneCycleLR(optimizer,</a>
<a class="sourceLine" id="cb12-80" title="80">                                                max_lr<span class="op">=</span><span class="fl">0.01</span>,</a>
<a class="sourceLine" id="cb12-81" title="81">                                                total_steps<span class="op">=</span><span class="bu">len</span>(train_dataloader)<span class="op">*</span>epochs)</a>
<a class="sourceLine" id="cb12-82" title="82"></a>
<a class="sourceLine" id="cb12-83" title="83"><span class="co"># create gradient scaler for mixed precision</span></a>
<a class="sourceLine" id="cb12-84" title="84">scaler <span class="op">=</span> GradScaler()</a>
<a class="sourceLine" id="cb12-85" title="85"></a>
<a class="sourceLine" id="cb12-86" title="86"><span class="co"># train function</span></a>
<a class="sourceLine" id="cb12-87" title="87"><span class="kw">def</span> train(dataloader):</a>
<a class="sourceLine" id="cb12-88" title="88">    <span class="co">#pbar = ProgressBar(n_total=len(dataloader), desc=&#39;Training&#39;)</span></a>
<a class="sourceLine" id="cb12-89" title="89">    train_loss <span class="op">=</span> AverageMeter()</a>
<a class="sourceLine" id="cb12-90" title="90">    model.train()</a>
<a class="sourceLine" id="cb12-91" title="91">    <span class="cf">for</span> batch_idx, batch <span class="kw">in</span> <span class="bu">enumerate</span>(dataloader):</a>
<a class="sourceLine" id="cb12-92" title="92">        b_features, b_target, b_idx <span class="op">=</span> batch[<span class="st">&#39;features&#39;</span>].to(DEVICE),  batch[<span class="st">&#39;target&#39;</span>].to(DEVICE), batch[<span class="st">&#39;idx&#39;</span>].to(DEVICE)</a>
<a class="sourceLine" id="cb12-93" title="93">        optimizer.zero_grad()</a>
<a class="sourceLine" id="cb12-94" title="94">        <span class="cf">with</span> autocast():</a>
<a class="sourceLine" id="cb12-95" title="95">            logits, probs <span class="op">=</span> model(b_features)</a>
<a class="sourceLine" id="cb12-96" title="96">            loss <span class="op">=</span> F.cross_entropy(logits, b_target)</a>
<a class="sourceLine" id="cb12-97" title="97">            <span class="co"># regularize loss -- but not the intercept</span></a>
<a class="sourceLine" id="cb12-98" title="98">            LAMBDA, L2 <span class="op">=</span> <span class="fl">0.5</span>, <span class="fl">0.</span></a>
<a class="sourceLine" id="cb12-99" title="99">            <span class="cf">for</span> name, p <span class="kw">in</span> model.named_parameters():</a>
<a class="sourceLine" id="cb12-100" title="100">                <span class="cf">if</span> <span class="st">&#39;weight&#39;</span> <span class="kw">in</span> name:</a>
<a class="sourceLine" id="cb12-101" title="101">                    L2 <span class="op">=</span> L2 <span class="op">+</span> (p<span class="op">**</span><span class="dv">2</span>).<span class="bu">sum</span>()</a>
<a class="sourceLine" id="cb12-102" title="102">            loss <span class="op">=</span> loss <span class="op">+</span> <span class="fl">2.</span><span class="op">/</span>b_target.size(<span class="dv">0</span>) <span class="op">*</span> LAMBDA <span class="op">*</span> L2</a>
<a class="sourceLine" id="cb12-103" title="103">        scaler.scale(loss).backward()</a>
<a class="sourceLine" id="cb12-104" title="104">        scaler.step(optimizer)</a>
<a class="sourceLine" id="cb12-105" title="105">        scaler.update()</a>
<a class="sourceLine" id="cb12-106" title="106">        scheduler.step()</a>
<a class="sourceLine" id="cb12-107" title="107">        <span class="co">#pbar(step=batch_idx, info={&#39;loss&#39;: loss.item()})</span></a>
<a class="sourceLine" id="cb12-108" title="108">        train_loss.update(loss.item(), n<span class="op">=</span><span class="dv">1</span>)</a>
<a class="sourceLine" id="cb12-109" title="109">    <span class="cf">return</span> {<span class="st">&#39;loss&#39;</span>: train_loss.avg}</a>
<a class="sourceLine" id="cb12-110" title="110"></a>
<a class="sourceLine" id="cb12-111" title="111"></a>
<a class="sourceLine" id="cb12-112" title="112"><span class="co"># valid/test function</span></a>
<a class="sourceLine" id="cb12-113" title="113"><span class="kw">def</span> test(dataloader):</a>
<a class="sourceLine" id="cb12-114" title="114">    <span class="co">#pbar = ProgressBar(n_total=len(dataloader), desc=&#39;Testing&#39;)</span></a>
<a class="sourceLine" id="cb12-115" title="115">    valid_loss <span class="op">=</span> AverageMeter()</a>
<a class="sourceLine" id="cb12-116" title="116">    valid_acc <span class="op">=</span> AverageMeter()</a>
<a class="sourceLine" id="cb12-117" title="117">    valid_f1 <span class="op">=</span> AverageMeter()</a>
<a class="sourceLine" id="cb12-118" title="118">    model.<span class="bu">eval</span>()</a>
<a class="sourceLine" id="cb12-119" title="119">    count <span class="op">=</span> <span class="dv">0</span></a>
<a class="sourceLine" id="cb12-120" title="120">    <span class="cf">with</span> torch.no_grad():</a>
<a class="sourceLine" id="cb12-121" title="121">        <span class="cf">for</span> batch_idx, batch <span class="kw">in</span> <span class="bu">enumerate</span>(dataloader):</a>
<a class="sourceLine" id="cb12-122" title="122">            b_features, b_target, b_idx <span class="op">=</span> batch[<span class="st">&#39;features&#39;</span>].to(DEVICE),  batch[<span class="st">&#39;target&#39;</span>].to(DEVICE), batch[<span class="st">&#39;idx&#39;</span>].to(DEVICE)</a>
<a class="sourceLine" id="cb12-123" title="123">            logits, probs <span class="op">=</span> model(b_features)</a>
<a class="sourceLine" id="cb12-124" title="124">            loss <span class="op">=</span> F.cross_entropy(logits, b_target).item()</a>
<a class="sourceLine" id="cb12-125" title="125">            pred <span class="op">=</span> probs.argmax(dim<span class="op">=</span><span class="dv">1</span>, keepdim<span class="op">=</span><span class="va">True</span>)  <span class="co"># get the index of the max log-probability</span></a>
<a class="sourceLine" id="cb12-126" title="126">            correct <span class="op">=</span> pred.eq(b_target.view_as(pred)).<span class="bu">sum</span>().item()</a>
<a class="sourceLine" id="cb12-127" title="127">            f1 <span class="op">=</span> f1_score(pred.to(<span class="st">&quot;cpu&quot;</span>).numpy(), b_target.to(<span class="st">&quot;cpu&quot;</span>).numpy(), average<span class="op">=</span><span class="st">&#39;macro&#39;</span>)</a>
<a class="sourceLine" id="cb12-128" title="128">            valid_f1.update(f1, n<span class="op">=</span>b_features.size(<span class="dv">0</span>))</a>
<a class="sourceLine" id="cb12-129" title="129">            valid_loss.update(loss, n<span class="op">=</span>b_features.size(<span class="dv">0</span>))</a>
<a class="sourceLine" id="cb12-130" title="130">            valid_acc.update(correct, n<span class="op">=</span><span class="dv">1</span>)</a>
<a class="sourceLine" id="cb12-131" title="131">            count <span class="op">+=</span> b_features.size(<span class="dv">0</span>)</a>
<a class="sourceLine" id="cb12-132" title="132">            <span class="co">#pbar(step=batch_idx)</span></a>
<a class="sourceLine" id="cb12-133" title="133">    <span class="cf">return</span> {<span class="st">&#39;valid_loss&#39;</span>: valid_loss.avg,</a>
<a class="sourceLine" id="cb12-134" title="134">            <span class="st">&#39;valid_acc&#39;</span>: valid_acc.<span class="bu">sum</span> <span class="op">/</span>count,</a>
<a class="sourceLine" id="cb12-135" title="135">            <span class="st">&#39;valid_f1&#39;</span>: valid_f1.avg}</a>
<a class="sourceLine" id="cb12-136" title="136"></a>
<a class="sourceLine" id="cb12-137" title="137"><span class="co"># training</span></a>
<a class="sourceLine" id="cb12-138" title="138"><span class="cf">for</span> epoch <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">1</span>, epochs <span class="op">+</span> <span class="dv">1</span>):</a>
<a class="sourceLine" id="cb12-139" title="139">    train_log <span class="op">=</span> train(train_dataloader)</a>
<a class="sourceLine" id="cb12-140" title="140">    valid_log <span class="op">=</span> test(valid_dataloader)</a>
<a class="sourceLine" id="cb12-141" title="141">    logs <span class="op">=</span> <span class="bu">dict</span>(train_log, <span class="op">**</span>valid_log)</a>
<a class="sourceLine" id="cb12-142" title="142">    show_info <span class="op">=</span> <span class="ss">f&#39;</span><span class="ch">\n</span><span class="ss">Epoch: </span><span class="sc">{</span>epoch<span class="sc">}</span><span class="ss"> - &#39;</span> <span class="op">+</span> <span class="st">&quot;-&quot;</span>.join([<span class="ss">f&#39; </span><span class="sc">{</span>key<span class="sc">}</span><span class="ss">: </span><span class="sc">{</span>value<span class="sc">:.4f}</span><span class="ss"> &#39;</span> <span class="cf">for</span> key, value <span class="kw">in</span> logs.items()])</a>
<a class="sourceLine" id="cb12-143" title="143">    <span class="bu">print</span>(show_info)</a></code></pre></div>
<pre><code>## 
## Epoch: 1 -  loss: 1.5542 - valid_loss: 0.4248 - valid_acc: 0.8790 - valid_f1: 0.8705 
## 
## Epoch: 2 -  loss: 0.8890 - valid_loss: 0.3804 - valid_acc: 0.8974 - valid_f1: 0.8915 
## 
## Epoch: 3 -  loss: 0.8092 - valid_loss: 0.3693 - valid_acc: 0.9080 - valid_f1: 0.9015 
## 
## Epoch: 4 -  loss: 0.7494 - valid_loss: 0.3302 - valid_acc: 0.9173 - valid_f1: 0.9116 
## 
## C:/Users/Andrew/Desktop/Projects/Deep Learning/utils\radam.py:60: UserWarning: This overload of add_ is deprecated:
##  add_(Number alpha, Tensor other)
## Consider using one of the following signatures instead:
##  add_(Tensor other, *, Number alpha) (Triggered internally at  ..\torch\csrc\utils\python_arg_parser.cpp:766.)
##   exp_avg.mul_(beta1).add_(1 - beta1, grad)</code></pre>
<div class="sourceCode" id="cb14"><pre class="sourceCode python"><code class="sourceCode python"><a class="sourceLine" id="cb14-1" title="1"><span class="co"># testing</span></a>
<a class="sourceLine" id="cb14-2" title="2">test_log <span class="op">=</span> test(test_dataloader)</a>
<a class="sourceLine" id="cb14-3" title="3"><span class="bu">print</span>(test_log)</a></code></pre></div>
<pre><code>## {&#39;valid_loss&#39;: 0.3280821489436286, &#39;valid_acc&#39;: 0.918, &#39;valid_f1&#39;: 0.9128352114054326}</code></pre>
</div>
<div id="dropout-1" class="section level3">
<h3><span class="header-section-number">1.2.2</span> Dropout</h3>
<div class="sourceCode" id="cb16"><pre class="sourceCode python"><code class="sourceCode python"><a class="sourceLine" id="cb16-1" title="1"><span class="kw">class</span> FF_NN(torch.nn.Module):</a>
<a class="sourceLine" id="cb16-2" title="2">    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, num_features, num_classes, drop_prob):</a>
<a class="sourceLine" id="cb16-3" title="3">        <span class="bu">super</span>(FF_NN, <span class="va">self</span>).<span class="fu">__init__</span>()</a>
<a class="sourceLine" id="cb16-4" title="4"></a>
<a class="sourceLine" id="cb16-5" title="5">        <span class="co"># first hidden layer</span></a>
<a class="sourceLine" id="cb16-6" title="6">        <span class="va">self</span>.linear_1 <span class="op">=</span> torch.nn.Linear(num_features, num_hidden_1)</a>
<a class="sourceLine" id="cb16-7" title="7">        <span class="co"># second hidden layer</span></a>
<a class="sourceLine" id="cb16-8" title="8">        <span class="va">self</span>.linear_2 <span class="op">=</span> torch.nn.Linear(num_hidden_1, num_hidden_2)</a>
<a class="sourceLine" id="cb16-9" title="9">        <span class="co"># output layer</span></a>
<a class="sourceLine" id="cb16-10" title="10">        <span class="va">self</span>.linear_out <span class="op">=</span> torch.nn.Linear(num_hidden_2, num_classes)</a>
<a class="sourceLine" id="cb16-11" title="11">        <span class="co"># dropout</span></a>
<a class="sourceLine" id="cb16-12" title="12">        <span class="va">self</span>.drop_prob <span class="op">=</span> drop_prob</a>
<a class="sourceLine" id="cb16-13" title="13"></a>
<a class="sourceLine" id="cb16-14" title="14">    <span class="co"># define how and what order model parameters should be used in forward prop.</span></a>
<a class="sourceLine" id="cb16-15" title="15">    <span class="kw">def</span> forward(<span class="va">self</span>, x):</a>
<a class="sourceLine" id="cb16-16" title="16">        <span class="co"># run inputs through first layer</span></a>
<a class="sourceLine" id="cb16-17" title="17">        out <span class="op">=</span> <span class="va">self</span>.linear_1(x)</a>
<a class="sourceLine" id="cb16-18" title="18">        <span class="co"># apply relu</span></a>
<a class="sourceLine" id="cb16-19" title="19">        out <span class="op">=</span> F.relu(out)</a>
<a class="sourceLine" id="cb16-20" title="20">        <span class="co"># apply dropout</span></a>
<a class="sourceLine" id="cb16-21" title="21">        out <span class="op">=</span> F.dropout(out, p<span class="op">=</span><span class="va">self</span>.drop_prob)</a>
<a class="sourceLine" id="cb16-22" title="22">        <span class="co"># run inputs through second layer</span></a>
<a class="sourceLine" id="cb16-23" title="23">        out <span class="op">=</span> <span class="va">self</span>.linear_2(out)</a>
<a class="sourceLine" id="cb16-24" title="24">        <span class="co"># apply relu</span></a>
<a class="sourceLine" id="cb16-25" title="25">        out <span class="op">=</span> F.relu(out)</a>
<a class="sourceLine" id="cb16-26" title="26">        <span class="co"># apply dropout</span></a>
<a class="sourceLine" id="cb16-27" title="27">        out <span class="op">=</span> F.dropout(out, p<span class="op">=</span><span class="va">self</span>.drop_prob)</a>
<a class="sourceLine" id="cb16-28" title="28">        <span class="co"># run inputs through final classification layer</span></a>
<a class="sourceLine" id="cb16-29" title="29">        logits <span class="op">=</span> <span class="va">self</span>.linear_out(out)</a>
<a class="sourceLine" id="cb16-30" title="30">        probs <span class="op">=</span> F.log_softmax(logits, dim<span class="op">=</span><span class="dv">1</span>)</a>
<a class="sourceLine" id="cb16-31" title="31">        <span class="cf">return</span> logits, probs</a></code></pre></div>
<div class="sourceCode" id="cb17"><pre class="sourceCode python"><code class="sourceCode python"><a class="sourceLine" id="cb17-1" title="1"><span class="co"># load the NN model</span></a>
<a class="sourceLine" id="cb17-2" title="2">num_features <span class="op">=</span> <span class="dv">784</span></a>
<a class="sourceLine" id="cb17-3" title="3">num_hidden_1 <span class="op">=</span> <span class="dv">128</span></a>
<a class="sourceLine" id="cb17-4" title="4">num_hidden_2 <span class="op">=</span> <span class="dv">256</span></a>
<a class="sourceLine" id="cb17-5" title="5">num_classes <span class="op">=</span> <span class="dv">10</span></a>
<a class="sourceLine" id="cb17-6" title="6">dropout_prob <span class="op">=</span> <span class="fl">0.2</span></a>
<a class="sourceLine" id="cb17-7" title="7">model <span class="op">=</span> FF_NN(num_features<span class="op">=</span>num_features, num_classes<span class="op">=</span>num_classes, drop_prob<span class="op">=</span>dropout_prob).to(DEVICE)</a>
<a class="sourceLine" id="cb17-8" title="8"></a>
<a class="sourceLine" id="cb17-9" title="9"></a>
<a class="sourceLine" id="cb17-10" title="10"><span class="co"># optimizer</span></a>
<a class="sourceLine" id="cb17-11" title="11">optimizer <span class="op">=</span> RAdam(model.parameters(), lr<span class="op">=</span><span class="fl">0.1</span>)</a>
<a class="sourceLine" id="cb17-12" title="12"></a>
<a class="sourceLine" id="cb17-13" title="13"><span class="co"># set number of epochs</span></a>
<a class="sourceLine" id="cb17-14" title="14">epochs <span class="op">=</span> <span class="dv">4</span></a>
<a class="sourceLine" id="cb17-15" title="15"></a>
<a class="sourceLine" id="cb17-16" title="16"></a>
<a class="sourceLine" id="cb17-17" title="17"><span class="co"># create DataLoaders with samplers</span></a>
<a class="sourceLine" id="cb17-18" title="18">train_dataloader <span class="op">=</span> DataLoader(train_ds,</a>
<a class="sourceLine" id="cb17-19" title="19">                              batch_size<span class="op">=</span><span class="dv">100</span>,</a>
<a class="sourceLine" id="cb17-20" title="20">                              sampler<span class="op">=</span>train_sampler,</a>
<a class="sourceLine" id="cb17-21" title="21">                              shuffle<span class="op">=</span><span class="va">False</span>)</a>
<a class="sourceLine" id="cb17-22" title="22"></a>
<a class="sourceLine" id="cb17-23" title="23">valid_dataloader <span class="op">=</span> DataLoader(valid_ds,</a>
<a class="sourceLine" id="cb17-24" title="24">                              batch_size<span class="op">=</span><span class="dv">100</span>,</a>
<a class="sourceLine" id="cb17-25" title="25">                              shuffle<span class="op">=</span><span class="va">True</span>)</a>
<a class="sourceLine" id="cb17-26" title="26"></a>
<a class="sourceLine" id="cb17-27" title="27">test_dataloader <span class="op">=</span> DataLoader(test_ds,</a>
<a class="sourceLine" id="cb17-28" title="28">                              batch_size<span class="op">=</span><span class="dv">100</span>,</a>
<a class="sourceLine" id="cb17-29" title="29">                              shuffle<span class="op">=</span><span class="va">True</span>)</a>
<a class="sourceLine" id="cb17-30" title="30"></a>
<a class="sourceLine" id="cb17-31" title="31"><span class="co"># set LR scheduler</span></a>
<a class="sourceLine" id="cb17-32" title="32">scheduler <span class="op">=</span> torch.optim.lr_scheduler.OneCycleLR(optimizer,</a>
<a class="sourceLine" id="cb17-33" title="33">                                                max_lr<span class="op">=</span><span class="fl">0.01</span>,</a>
<a class="sourceLine" id="cb17-34" title="34">                                                total_steps<span class="op">=</span><span class="bu">len</span>(train_dataloader)<span class="op">*</span>epochs)</a>
<a class="sourceLine" id="cb17-35" title="35"></a>
<a class="sourceLine" id="cb17-36" title="36"><span class="co"># create gradient scaler for mixed precision</span></a>
<a class="sourceLine" id="cb17-37" title="37">scaler <span class="op">=</span> GradScaler()</a>
<a class="sourceLine" id="cb17-38" title="38"></a>
<a class="sourceLine" id="cb17-39" title="39"><span class="co"># train function</span></a>
<a class="sourceLine" id="cb17-40" title="40"><span class="kw">def</span> train(dataloader):</a>
<a class="sourceLine" id="cb17-41" title="41">    <span class="co">#pbar = ProgressBar(n_total=len(dataloader), desc=&#39;Training&#39;)</span></a>
<a class="sourceLine" id="cb17-42" title="42">    train_loss <span class="op">=</span> AverageMeter()</a>
<a class="sourceLine" id="cb17-43" title="43">    model.train()</a>
<a class="sourceLine" id="cb17-44" title="44">    <span class="cf">for</span> batch_idx, batch <span class="kw">in</span> <span class="bu">enumerate</span>(dataloader):</a>
<a class="sourceLine" id="cb17-45" title="45">        b_features, b_target, b_idx <span class="op">=</span> batch[<span class="st">&#39;features&#39;</span>].to(DEVICE),  batch[<span class="st">&#39;target&#39;</span>].to(DEVICE), batch[<span class="st">&#39;idx&#39;</span>].to(DEVICE)</a>
<a class="sourceLine" id="cb17-46" title="46">        optimizer.zero_grad()</a>
<a class="sourceLine" id="cb17-47" title="47">        <span class="cf">with</span> autocast():</a>
<a class="sourceLine" id="cb17-48" title="48">            logits, probs <span class="op">=</span> model(b_features)</a>
<a class="sourceLine" id="cb17-49" title="49">            loss <span class="op">=</span> F.cross_entropy(logits, b_target)</a>
<a class="sourceLine" id="cb17-50" title="50">        scaler.scale(loss).backward()</a>
<a class="sourceLine" id="cb17-51" title="51">        scaler.step(optimizer)</a>
<a class="sourceLine" id="cb17-52" title="52">        scaler.update()</a>
<a class="sourceLine" id="cb17-53" title="53">        scheduler.step()</a>
<a class="sourceLine" id="cb17-54" title="54">        <span class="co">#pbar(step=batch_idx, info={&#39;loss&#39;: loss.item()})</span></a>
<a class="sourceLine" id="cb17-55" title="55">        train_loss.update(loss.item(), n<span class="op">=</span><span class="dv">1</span>)</a>
<a class="sourceLine" id="cb17-56" title="56">    <span class="cf">return</span> {<span class="st">&#39;loss&#39;</span>: train_loss.avg}</a>
<a class="sourceLine" id="cb17-57" title="57"></a>
<a class="sourceLine" id="cb17-58" title="58"></a>
<a class="sourceLine" id="cb17-59" title="59"><span class="co"># valid/test function</span></a>
<a class="sourceLine" id="cb17-60" title="60"><span class="kw">def</span> test(dataloader):</a>
<a class="sourceLine" id="cb17-61" title="61">    <span class="co">#pbar = ProgressBar(n_total=len(dataloader), desc=&#39;Testing&#39;)</span></a>
<a class="sourceLine" id="cb17-62" title="62">    valid_loss <span class="op">=</span> AverageMeter()</a>
<a class="sourceLine" id="cb17-63" title="63">    valid_acc <span class="op">=</span> AverageMeter()</a>
<a class="sourceLine" id="cb17-64" title="64">    valid_f1 <span class="op">=</span> AverageMeter()</a>
<a class="sourceLine" id="cb17-65" title="65">    model.<span class="bu">eval</span>()</a>
<a class="sourceLine" id="cb17-66" title="66">    count <span class="op">=</span> <span class="dv">0</span></a>
<a class="sourceLine" id="cb17-67" title="67">    <span class="cf">with</span> torch.no_grad():</a>
<a class="sourceLine" id="cb17-68" title="68">        <span class="cf">for</span> batch_idx, batch <span class="kw">in</span> <span class="bu">enumerate</span>(dataloader):</a>
<a class="sourceLine" id="cb17-69" title="69">            b_features, b_target, b_idx <span class="op">=</span> batch[<span class="st">&#39;features&#39;</span>].to(DEVICE),  batch[<span class="st">&#39;target&#39;</span>].to(DEVICE), batch[<span class="st">&#39;idx&#39;</span>].to(DEVICE)</a>
<a class="sourceLine" id="cb17-70" title="70">            logits, probs <span class="op">=</span> model(b_features)</a>
<a class="sourceLine" id="cb17-71" title="71">            loss <span class="op">=</span> F.cross_entropy(logits, b_target).item()</a>
<a class="sourceLine" id="cb17-72" title="72">            pred <span class="op">=</span> probs.argmax(dim<span class="op">=</span><span class="dv">1</span>, keepdim<span class="op">=</span><span class="va">True</span>)  <span class="co"># get the index of the max log-probability</span></a>
<a class="sourceLine" id="cb17-73" title="73">            correct <span class="op">=</span> pred.eq(b_target.view_as(pred)).<span class="bu">sum</span>().item()</a>
<a class="sourceLine" id="cb17-74" title="74">            f1 <span class="op">=</span> f1_score(pred.to(<span class="st">&quot;cpu&quot;</span>).numpy(), b_target.to(<span class="st">&quot;cpu&quot;</span>).numpy(), average<span class="op">=</span><span class="st">&#39;macro&#39;</span>)</a>
<a class="sourceLine" id="cb17-75" title="75">            valid_f1.update(f1, n<span class="op">=</span>b_features.size(<span class="dv">0</span>))</a>
<a class="sourceLine" id="cb17-76" title="76">            valid_loss.update(loss, n<span class="op">=</span>b_features.size(<span class="dv">0</span>))</a>
<a class="sourceLine" id="cb17-77" title="77">            valid_acc.update(correct, n<span class="op">=</span><span class="dv">1</span>)</a>
<a class="sourceLine" id="cb17-78" title="78">            count <span class="op">+=</span> b_features.size(<span class="dv">0</span>)</a>
<a class="sourceLine" id="cb17-79" title="79">            <span class="co">#pbar(step=batch_idx)</span></a>
<a class="sourceLine" id="cb17-80" title="80">    <span class="cf">return</span> {<span class="st">&#39;valid_loss&#39;</span>: valid_loss.avg,</a>
<a class="sourceLine" id="cb17-81" title="81">            <span class="st">&#39;valid_acc&#39;</span>: valid_acc.<span class="bu">sum</span> <span class="op">/</span>count,</a>
<a class="sourceLine" id="cb17-82" title="82">            <span class="st">&#39;valid_f1&#39;</span>: valid_f1.avg}</a>
<a class="sourceLine" id="cb17-83" title="83"></a>
<a class="sourceLine" id="cb17-84" title="84"><span class="co"># training</span></a>
<a class="sourceLine" id="cb17-85" title="85"><span class="cf">for</span> epoch <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">1</span>, epochs <span class="op">+</span> <span class="dv">1</span>):</a>
<a class="sourceLine" id="cb17-86" title="86">    train_log <span class="op">=</span> train(train_dataloader)</a>
<a class="sourceLine" id="cb17-87" title="87">    valid_log <span class="op">=</span> test(valid_dataloader)</a>
<a class="sourceLine" id="cb17-88" title="88">    logs <span class="op">=</span> <span class="bu">dict</span>(train_log, <span class="op">**</span>valid_log)</a>
<a class="sourceLine" id="cb17-89" title="89">    show_info <span class="op">=</span> <span class="ss">f&#39;</span><span class="ch">\n</span><span class="ss">Epoch: </span><span class="sc">{</span>epoch<span class="sc">}</span><span class="ss"> - &#39;</span> <span class="op">+</span> <span class="st">&quot;-&quot;</span>.join([<span class="ss">f&#39; </span><span class="sc">{</span>key<span class="sc">}</span><span class="ss">: </span><span class="sc">{</span>value<span class="sc">:.4f}</span><span class="ss"> &#39;</span> <span class="cf">for</span> key, value <span class="kw">in</span> logs.items()])</a>
<a class="sourceLine" id="cb17-90" title="90">    <span class="bu">print</span>(show_info)</a></code></pre></div>
<pre><code>## 
## Epoch: 1 -  loss: 0.7804 - valid_loss: 0.2648 - valid_acc: 0.9176 - valid_f1: 0.9117 
## 
## Epoch: 2 -  loss: 0.1970 - valid_loss: 0.1802 - valid_acc: 0.9483 - valid_f1: 0.9461 
## 
## Epoch: 3 -  loss: 0.1286 - valid_loss: 0.1242 - valid_acc: 0.9643 - valid_f1: 0.9607 
## 
## Epoch: 4 -  loss: 0.0768 - valid_loss: 0.1024 - valid_acc: 0.9706 - valid_f1: 0.9691</code></pre>
<div class="sourceCode" id="cb19"><pre class="sourceCode python"><code class="sourceCode python"><a class="sourceLine" id="cb19-1" title="1"><span class="co"># testing</span></a>
<a class="sourceLine" id="cb19-2" title="2">test_log <span class="op">=</span> test(test_dataloader)</a>
<a class="sourceLine" id="cb19-3" title="3"><span class="bu">print</span>(test_log)</a></code></pre></div>
<pre><code>## {&#39;valid_loss&#39;: 0.10206319407692978, &#39;valid_acc&#39;: 0.9702857142857143, &#39;valid_f1&#39;: 0.9684667952447602}</code></pre>
</div>
</div>
</div>
<div id="sources" class="section level1">
<h1><span class="header-section-number">2</span> Sources</h1>
<ul>
<li><a href="https://github.com/rasbt/stat479-deep-learning-ss19/blob/master/L11_weight-init/L11_weight-init_slides.pdf" class="uri">https://github.com/rasbt/stat479-deep-learning-ss19/blob/master/L11_weight-init/L11_weight-init_slides.pdf</a></li>
</ul>
</div>



<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>
